{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">Predict Car Traffic Injury</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "In this document we follow the CRISP-DM process and methodolgies to develop our project.\n",
    "Full details are described in the report document of the project.\n",
    "\n",
    "In the file, we will do:\n",
    "* Coding + Analysis for results.\n",
    "* Code insights.\n",
    "* Apply the practices learned for the business part, including understanding domain concepts and details about each feature.\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to read this document**\n",
    "\n",
    "1. Text in black (or White, depends on reader theme background) - Describes section or sub-section titles and details.\n",
    "\n",
    "<span style=\"color:cyan\">\n",
    "\n",
    "2. Text in \"cyan\" is analysis realted to previous code section.\n",
    "</span>\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CRISP-DM: Business Understanding\n",
    "\n",
    "1.1. Background and Objective<br>\n",
    "1.2. Descriptive Features and Domain Concepts<br>\n",
    "1.3. Domain Concepts Explained<br>\n",
    "1.4. Relevant Resources<br>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<span style=\"color:cyan\">\n",
    "\n",
    "# 1.1. Background and Objective<br>\n",
    "Traffic Accidents globally rank high in causing deaths and injuries. Traffic accidents have significant health care and economic impact.<br>\n",
    "According to World Health Organization traffic accidents is one of the leading death causes worldwide.<br>\n",
    "<br>\n",
    "In this project, we are utilizing advanced machine learning models and data analytics to forecast accident severity for strategic planning and response, aiming to reduce traffic-related injuries, fatalities, and economic costs with targeted preventive measures and efficient resource allocation.<br>\n",
    "<br>\n",
    "We addressing the gap in predictive knowledge of traffic accident severity factors in Chicago's urban landscape to enhancing public health and safety by identifying risk patterns and contributing factors through historical data analysis.<br>\n",
    "\n",
    "</span>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:cyan\">\n",
    "\n",
    "## 1.2. Descriptive Features and Domain Concepts\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "| Column Name                    | Type           | Short Description                                                                                                                         | Domain Concept           |\n",
    "|--------------------------------|----------------|-------------------------------------------------------------------------------------------------------------------------------------------|-------------------------|\n",
    "| CRASH_RECORD_ID                | Text           | Unique identifier for the crash record, linking to the same crash in the Vehicles and People datasets.                                    | Identification          |\n",
    "| CRASH_DATE_EST_I               | Text           | Indicates if the crash date was estimated by a desk officer or reporting party.                                                           | Crash Details           |\n",
    "| CRASH_DATE                     | DateTime       | Date and time of the crash as entered by the reporting officer.                                                                           | Crash Details           |\n",
    "| POSTED_SPEED_LIMIT             | Number         | Posted speed limit as determined by the reporting officer.                                                                                | Environmental Factors   |\n",
    "| TRAFFIC_CONTROL_DEVICE         | Text           | Traffic control device present at the crash location as determined by the reporting officer.                                              | Environmental Factors   |\n",
    "| DEVICE_CONDITION               | Text           | Condition of the traffic control device as determined by the reporting officer.                                                           | Environmental Factors   |\n",
    "| WEATHER_CONDITION              | Text           | Weather condition at the time of the crash as determined by the reporting officer.                                                        | Environmental Factors   |\n",
    "| LIGHTING_CONDITION             | Text           | Lighting condition at the time of the crash as determined by the reporting officer.                                                       | Environmental Factors   |\n",
    "| FIRST_CRASH_TYPE               | Text           | Type of first collision in the crash.                                                                                                     | Crash Details           |\n",
    "| TRAFFICWAY_TYPE                | Text           | Trafficway type as determined by the reporting officer.                                                                                   | Environmental Factors   |\n",
    "| LANE_CNT                       | Number         | Total number of through lanes in either direction, excluding turn lanes, at the crash location as determined by the reporting officer.     | Environmental Factors   |\n",
    "| ALIGNMENT                      | Text           | Street alignment at the crash location as determined by the reporting officer.                                                            | Environmental Factors   |\n",
    "| ROADWAY_SURFACE_COND           | Text           | Road surface condition at the crash location as determined by the reporting officer.                                                      | Environmental Factors   |\n",
    "| ROAD_DEFECT                    | Text           | Road defects present at the crash location as determined by the reporting officer.                                                        | Environmental Factors   |\n",
    "| REPORT_TYPE                    | Text           | Administrative report type of the crash.                                                                                                   | Documentation          |\n",
    "| CRASH_TYPE                     | Text           | General severity classification for the crash.                                                                                             | Crash Details           |\n",
    "| INTERSECTION_RELATED_I         | Text           | Indicates if an intersection played a role in the crash as observed by the police officer.                                                | Environmental Factors   |\n",
    "| NOT_RIGHT_OF_WAY_I             | Text           | Indicates if the crash began or first contact was made outside of the public right-of-way.                                                | Legal                   |\n",
    "| HIT_AND_RUN_I                  | Text           | Indicates if the crash involved a driver who fled the scene without exchanging information and/or rendering aid.                          | Legal                   |\n",
    "| DAMAGE                         | Text           | Estimated damage from the crash as observed in the field.                                                                                 | Crash Outcome           |\n",
    "| DATE_POLICE_NOTIFIED           | DateTime       | Calendar date on which the police were notified of the crash.                                                                             | Documentation          |\n",
    "| PRIM_CONTRIBUTORY_CAUSE        | Text           | Primary factor contributing to the cause of the crash as determined by officer judgment.                                                  | Crash Analysis         |\n",
    "| SEC_CONTRIBUTORY_CAUSE         | Text           | Secondary factor contributing to the cause of the crash as determined by officer judgment.                                                | Crash Analysis         |\n",
    "| STREET_NO                      | Number         | Street address number of the crash location as determined by the reporting officer.                                                       | Location Details       |\n",
    "| STREET_DIRECTION               | Text           | Street address direction (N, E, S, W) of the crash location as determined by the reporting officer.                                       | Location Details       |\n",
    "| STREET_NAME                    | Text           | Street name of the crash location as determined by the reporting officer.                                                                 | Location Details       |\n",
    "| BEAT_OF_OCCURRENCE             | Number         | Chicago Police Department beat ID where the crash occurred.                                                                               | Administrative         |\n",
    "| PHOTOS_TAKEN_I                 | Text           | Indicates if photos were taken at the crash location by the Chicago Police Department.                                                    | Documentation          |\n",
    "| STATEMENTS_TAKEN_I             | Text           | Indicates if statements were taken from units involved in the crash.                                                                      | Documentation          |\n",
    "| DOORING_I                      | Text           | Indicates if the crash involved a vehicle occupant opening a door into the path of a bicyclist.                                           | Crash Type             |\n",
    "| WORK_ZONE_I                    | Text           | Indicates if the crash occurred in an active work zone.                                                                                   | Environmental Factors   |\n",
    "| WORK_ZONE_TYPE                 | Text           | Type of work zone, if any, where the crash occurred.                                                                                      | Environmental Factors   |\n",
    "| WORKERS_PRESENT_I              | Text           | Indicates if construction workers were present in the work zone at the crash location.                                                    | Environmental Factors   |\n",
    "| NUM_UNITS                      | Number         | Number of units involved in the crash, representing different modes of traffic with independent trajectories.                             | Crash Details           |\n",
    "| MOST_SEVERE_INJURY             | Text           | Most severe injury sustained by any person involved in the crash.                                                                         | Injury Analysis        |\n",
    "| INJURIES_TOTAL                 | Number         | Total number of people sustaining fatal, incapacitating, non-incapacitating, and possible injuries as determined by the reporting officer. | Injury Analysis        |\n",
    "| INJURIES_FATAL                 | Number         | Total number of people sustaining fatal injuries in the crash.                                                                            | Injury Analysis        |\n",
    "| INJURIES_INCAPACITATING        | Number         | Total number of people sustaining incapacitating/serious injuries in the crash.                                                           | Injury Analysis        |\n",
    "| INJURIES_NON_INCAPACITATING    | Number         | Total number of people sustaining non-incapacitating injuries in the crash.                                                               | Injury Analysis        |\n",
    "| INJURIES_REPORTED_NOT_EVIDENT  | Number         | Total number of people sustaining possible injuries in the crash as determined by the reporting officer.                                  | Injury Analysis        |\n",
    "| INJURIES_NO_INDICATION         | Number         | Total number of people with no injuries in the crash as determined by the reporting officer.                                              | Injury Analysis        |\n",
    "| INJURIES_UNKNOWN               | Number         | Total number of people for whom the injury status, if any, is unknown.                                                                    | Injury Analysis        |\n",
    "| CRASH_HOUR                     | Number         | Hour of the day when the crash occurred, derived from CRASH_DATE.                                                                         | Time Details           |\n",
    "| CRASH_DAY_OF_WEEK              | Number         | Day of the week when the crash occurred, derived from CRASH_DATE. Sunday=1                                                                | Time Details           |\n",
    "| CRASH_MONTH                    | Number         | Month when the crash occurred, derived from CRASH_DATE.                                                                                   | Time Details           |\n",
    "| LATITUDE                       | Number         | Latitude of the crash location as determined by the reporting officer.                                                                    | Geographic Information |\n",
    "| LONGITUDE                      | Number         | Longitude of the crash location as determined by the reporting officer.                                                                   | Geographic Information |\n",
    "| LOCATION                       | Point          | Geographic location of the crash as determined by the reporting officer, allowing for mapping and geographic analysis.                    | Geographic Information |\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:cyan\">\n",
    "\n",
    "\n",
    "\n",
    "## 1.3. Domain Concepts Explained<br>\n",
    "\n",
    "\n",
    "##### **1. Identification**\n",
    "- **Description**: Features that uniquely identify the crash records or events, often used for linking data across different datasets.\n",
    "- **Example**: \n",
    "  - **CRASH_RECORD_ID**: Serves as a unique identifier for each crash event, enabling connections with related records in vehicles and people datasets.\n",
    "\n",
    "##### **2. Crash Details**\n",
    "- **Description**: Specific details about the crash event, including the type of crash, date, and time, providing basic information on how and when the crash occurred.\n",
    "- **Example**: \n",
    "  - **CRASH_DATE**: Provides the exact date and time the crash happened.\n",
    "  - **FIRST_CRASH_TYPE**: Describes the initial type of collision.\n",
    "\n",
    "##### **3. Environmental Factors**\n",
    "- **Description**: Conditions surrounding the crash, such as weather, lighting, and road conditions, which can influence the occurrence and severity of crashes.\n",
    "- **Example**: \n",
    "  - **WEATHER_CONDITION** and **LIGHTING_CONDITION**: Detail the environmental state at the time of the crash, affecting driving conditions and visibility.\n",
    "\n",
    "##### **4. Legal**\n",
    "- **Description**: Features related to legal aspects of the crash, including compliance with right-of-way rules and hit-and-run incidents.\n",
    "- **Example**: \n",
    "  - **HIT_AND_RUN_I**: Indicates whether a driver involved in the crash fled the scene without providing information or rendering aid.\n",
    "\n",
    "##### **5. Crash Outcome**\n",
    "- **Description**: Descriptions of the immediate consequences of the crash, primarily in terms of physical damage.\n",
    "- **Example**: \n",
    "  - **DAMAGE**: Estimates the monetary damage to property resulting from the crash.\n",
    "\n",
    "##### **6. Documentation**\n",
    "- **Description**: Administrative details about the crash reporting and documentation process, including whether photos or statements were taken.\n",
    "- **Example**: \n",
    "  - **DATE_POLICE_NOTIFIED**: Records when the police were officially notified about the crash.\n",
    "\n",
    "##### **7. Crash Analysis**\n",
    "- **Description**: Factors identified as contributing to the crash, including primary and secondary causes as determined by officer judgment.\n",
    "- **Example**: \n",
    "  - **PRIM_CONTRIBUTORY_CAUSE** and **SEC_CONTRIBUTORY_CAUSE**: Identify the main reasons behind the crash according to the reporting officer.\n",
    "\n",
    "##### **8. Location Details**\n",
    "- **Description**: Specifics about where the crash occurred, including street names and numbers, helping in pinpointing the exact crash location.\n",
    "- **Example**: \n",
    "  - **STREET_NAME**, **STREET_NO**, and **STREET_DIRECTION**: Provide a detailed description of the crash site.\n",
    "\n",
    "##### **9. Administrative**\n",
    "- **Description**: Information related to police and emergency response jurisdictions, such as police beats.\n",
    "- **Example**: \n",
    "  - **BEAT_OF_OCCURRENCE**: Refers to the Chicago Police Department beat where the crash took place.\n",
    "\n",
    "##### **10. Crash Type**\n",
    "- **Description**: Categories that describe specific scenarios or types of crashes, including incidents involving bicycles or work zones.\n",
    "- **Example**: \n",
    "  - **DOORING_I**: Specifies if the crash involved a vehicle door being opened in the path of a bicyclist.\n",
    "\n",
    "##### **11. Injury Analysis**\n",
    "- **Description**: Detailed records of injuries resulting from the crash, categorizing the severity and type of injuries sustained.\n",
    "- **Example**: \n",
    "  - **INJURIES_TOTAL**: Counts the total number of injuries.\n",
    "  - **INJURIES_FATAL**: Counts the number of fatal injuries.\n",
    "\n",
    "##### **12. Time Details**\n",
    "- **Description**: Temporal information about the crash, including the hour, day of the week, and month when the crash occurred.\n",
    "- **Example**: \n",
    "  - **CRASH_HOUR**, **CRASH_DAY_OF_WEEK**, and **CRASH_MONTH**: Provide insights into the timing patterns of crashes.\n",
    "\n",
    "##### **13. Geographic Information**\n",
    "- **Description**: Geospatial data related to the crash location, enabling mapping and location-based analysis.\n",
    "- **Example**: \n",
    "  - **LATITUDE** and **LONGITUDE**: Give precise coordinates of the crash site.\n",
    "  - **LOCATION**: Offers a mappable point of the incident.\n",
    "\n",
    "Each domain concept helps categorize the dataset's features for easier analysis, highlighting different factors and details that contribute to a comprehensive understanding of traffic crashes.\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "## 1.4. Relevant Resources\n",
    "\n",
    "1. Dataset acquired from USA DATA GOV<br>\n",
    "URL: https://catalog.data.gov/dataset/traffic-crashes-crashes\n",
    "\n",
    "<br>\n",
    "\n",
    "2. Full details about this section, please refer to the report document.\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 2. CRISP-DM: Data Understanding\n",
    "\n",
    "In this section, the following will be addressed:<br><br>\n",
    "**2.1. Collect Initial Data**<br>\n",
    "2.1.1. Import the relevant Python packages that are going to be used.<br>\n",
    "2.1.2. Acquire the data.<br>\n",
    "2.1.3. Record of the data acquisition process.<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**2.2. Data Description**<br>\n",
    "2.2.1. Shape of the Dataset.<br>\n",
    "2.2.2. Head snippet.<br>\n",
    "2.2.3. Dataset info.<br>\n",
    "2.2.4. Numerical Feature Distribution.<br>\n",
    "2.2.5. Catagorical Feature Distribution.<br>\n",
    "2.2.6. HTML Reprot.<br>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**2.3. Explore the Data**<br>\n",
    "2.3.1. Continuous Features Tabular Report<br>\n",
    "2.3.2. Categorical Features Tabular Report <br>\n",
    "2.3.3. Correlations Matrix<br>\n",
    "2.3.4. HeatMap<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**2.4. Verify Data Quality**<br>\n",
    "2.4.1. Missing Values Summary<br>\n",
    "2.4.2. Irregular cardinality<br>\n",
    "2.4.3. Outliers<br>\n",
    "2.4.4. Uniqueness - Remove duplicated or irrelevant repetition in data.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 2.1. Collect Initial Data\n",
    "\n",
    "\n",
    "This a generic intial step, to start working on the data:<br>\n",
    "2.1.1. Import the relevant Python packages that are going to be used.<br>\n",
    "2.1.2. Acquire the data.<br>\n",
    "2.1.3. Record of the data acquisition process.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1.1. Import the relevant Python packages that are going to be used\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from ydata_profiling import ProfileReport\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1.2. Acquire the data\n",
    "\n",
    "# A generic utilies file with generic functionallity\n",
    "from misc.utilities import acquire_dataset\n",
    "\n",
    "# Download & load the dataset\n",
    "# If the file already downloaded, it will not be re-downloaded (to speed up work efficiency)\n",
    "df = acquire_dataset()\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "\n",
    "2.1.2. Acquire the data<br>\n",
    "\n",
    "The data acquired from: https://catalog.data.gov/dataset/traffic-crashes-crashes\n",
    "<br>\n",
    "The above website belongs to USA government free datasets advised by the course instructions.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1.3. Helper functions\n",
    "\n",
    "def get_continuous_feature_details(my_df, my_feature):\n",
    "    my_df[my_feature] = pd.to_numeric(my_df[my_feature], errors='coerce')\n",
    "    # Printing unique values\n",
    "    unique_values = my_df[my_feature].unique()\n",
    "    sorted_unique_values = np.sort(unique_values)\n",
    "    len_unique_values = len(unique_values)\n",
    "    unique_values_str = ', '.join(map(str, sorted_unique_values[:5])) + ', ..., ' + ', '.join(map(str, sorted_unique_values[-5:]))\n",
    "    max_value = my_df[my_feature].max()\n",
    "    min_value = my_df[my_feature].min()\n",
    "    # print(f\"{my_feature}: Unique Len={len_unique_values}, Unique={sorted_unique_values}, Max={max_value}, Min={min_value}\")\n",
    "    print(f\"{my_feature}:\\n  Unique Len={len_unique_values}\\n  Unique={unique_values_str}\\n  Max={max_value}\\n  Min={min_value}\")\n",
    "\n",
    "    return len_unique_values, sorted_unique_values, max_value, min_value\n",
    "    \n",
    "\n",
    "def dump_feature_frequency_to_a_file(my_df, my_feature):\n",
    "    value_counts = my_df[my_feature].value_counts()\n",
    "    # value_counts.to_csv(f\"{my_feature}_value_counts.csv\", index=True, header=['Frequency'])\n",
    "    # Open a file to write\n",
    "    with open(f\"{my_feature}_value_counts.txt\", 'w') as file:\n",
    "        # Write a header row\n",
    "        file.write(f\"{'Value':<50}       {'Frequency':<10}\\n\")\n",
    "        file.write(f\"{'-'*50}       {'-'*10}\\n\")\n",
    "        \n",
    "        # Iterate over the Series and write each value and its frequency\n",
    "        for value, count in value_counts.items():\n",
    "            file.write(f\"{value:<50}       {count:<10}\\n\")\n",
    "\n",
    "\n",
    "def dump_feature_classes_to_a_file(my_df, my_feature):\n",
    "    # Extract unique values from the feature column\n",
    "    unique_classes = my_df[my_feature].unique()\n",
    "    # Sort the unique values if needed\n",
    "    unique_classes_sorted = sorted(unique_classes)\n",
    "    # Write the unique classes to a file\n",
    "    with open(f\"{my_feature}_unique_classes.txt\", 'w') as file:\n",
    "        for cls in unique_classes_sorted:\n",
    "            file.write(cls + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "\n",
    "2.1.3. Helper functions<br>\n",
    "\n",
    "Helper functions used periodically.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 2.2. Data Description\n",
    "\n",
    "In this phase we investigate the following aspects:<br>\n",
    "2.2.1. Shape of the Dataset.<br>\n",
    "2.2.2. Head snippet.<br>\n",
    "2.2.3. Dataset info.<br>\n",
    "2.2.4. Numerical Feature Distribution.<br>\n",
    "2.2.5. Catagorical Feature Distribution.<br>\n",
    "2.2.6 HTML report<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2.1. Shape of the Dataset.\n",
    "\n",
    "def get_dataset_shape(df):\n",
    "    # Create a new DataFrame to display the shape information\n",
    "    shape_df = pd.DataFrame({\n",
    "        'Aspect': ['Records (Instances)', 'Features (Columns)'],\n",
    "        'Number': [df.shape[0], df.shape[1]]\n",
    "    })\n",
    "    return shape_df\n",
    "\n",
    "# Display the DataFrame\n",
    "get_dataset_shape(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2.2. Head snippet\n",
    "\n",
    "def get_dataset_head(df):\n",
    "    return df.head()\n",
    "\n",
    "get_dataset_head(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2.3. Dataset info\n",
    "\n",
    "def get_dataset_info(df):\n",
    "    df.info()\n",
    "\n",
    "get_dataset_info(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2.4. Numerical Feature Distribution\n",
    "\n",
    "# Function for visualizaiotn of categorical feature distribution\n",
    "def Numerical_feature_Dist(df):\n",
    "    # Select only numeric columns for distribution plots\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    # Determine the number of rows/columns for the subplot grid\n",
    "    num_features = numeric_df.shape[1]\n",
    "    num_rows = int(np.ceil(num_features / 3))  # Adjust the denominator to change the number of columns\n",
    "    # Create a figure and a grid of subplots\n",
    "    plt.figure(figsize=(15, num_rows * 5))\n",
    "    for i, column in enumerate(numeric_df.columns):\n",
    "        plt.subplot(num_rows, 3, i + 1)\n",
    "        sns.histplot(numeric_df[column], kde=True, stat = 'density')\n",
    "        plt.title(f'Distribution of {column}')\n",
    "        plt.xlabel(column)\n",
    "        plt.ylabel('Frequency')\n",
    "    # Adjust layout for better visualization\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Numerical_feature_Dist(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.2.5 Catagorical Feature Distribution\n",
    "\n",
    "\n",
    "# Function for visualizaiotn of categorical feature distribution\n",
    "def Categorical_feature_Dist(df):\n",
    "    # Select only categorical columns\n",
    "    categorical_df = df.select_dtypes(include=['object', 'category']) #exclude=['number']\n",
    "    # Delete some of the columns due ot cardinality issues (they have alot of levels like Id)\n",
    "    categorical_df = categorical_df.drop(['PRIM_CONTRIBUTORY_CAUSE','SEC_CONTRIBUTORY_CAUSE','STREET_NAME','DATE_POLICE_NOTIFIED','LOCATION','CRASH_RECORD_ID' , 'CRASH_DATE_EST_I','CRASH_DATE'] , axis = 1)\n",
    "    # Determine the number of rows/columns for the subplot grid\n",
    "    num_features = categorical_df.shape[1]\n",
    "    num_rows = int(np.ceil(num_features / 3))  # Adjust for desired number of columns per row\n",
    "    plt.figure(figsize=(15, num_rows * 5))\n",
    "    for i, column in enumerate(categorical_df.columns):\n",
    "        plt.subplot(num_rows, 3, i + 1)\n",
    "        ax = sns.countplot(y=categorical_df[column])\n",
    "        total = len(categorical_df[column])  # Total number of data points for the percentage calculation\n",
    "        for p in ax.patches:\n",
    "            percentage = '{:.1f}%'.format(100 * p.get_width()/total)\n",
    "            x = p.get_x() + p.get_width() + 0.02  # Shifts the text to the right side of the bars\n",
    "            y = p.get_y() + p.get_height()/2\n",
    "            ax.annotate(percentage, (x, y))\n",
    "        \n",
    "        plt.title(f'Distribution of {column}')\n",
    "        plt.xlabel('Count')\n",
    "        plt.ylabel(column)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Categorical_feature_Dist(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2.6. HTML report\n",
    "\n",
    "# Disable since it is taking high CPU/Memory resources\n",
    "# profile = ProfileReport(df, title=\"Pandas Profiling Report\",explorative=True)\n",
    "# profile.to_file(\"pandas_profiling_report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### 2.2. Data Description\n",
    "##### Analysis\n",
    "\n",
    "For all 2.2 section.\n",
    "\n",
    "1. **Determining the Size of the Dataset**:\n",
    "   - our dataset has a considerable size with **814,788 instances** and **51 features** that provide detailed attributes or properties of each instance.\n",
    "\n",
    "2. **Previewing the Data**:\n",
    "   - We have displayed the first few entries of the dataset to get an initial understanding of the data's structure and content. This is like reading the first page of a book to get a sense of the story.\n",
    "\n",
    "3. **Assessing Data Completeness**:\n",
    "   - The dataset's informational output revealed that certain features have missing values. For example, 'LANE_CNT' has many missing values, whereas 'CRASH_RECORD_ID' is complete. This insight is crucial for understanding which features are fully observed and which may require data imputation or cleaning.\n",
    "\n",
    "4. **Visualizing Feature Distributions**:\n",
    "   - We have created a series of plots to visualize the distribution of various features. These plots help us understand the frequency and pattern of different attributes. For example, the 'POSTED_SPEED_LIMIT' plot suggests most data points cluster around specific speed limits (30), while the 'LANE_CNT' plot indicates a right-skewed distribution, meaning lower lane counts are more common.\n",
    "\n",
    "5. **Exploring Categorical Features**:\n",
    "   - For categorical data, We have smartly chosen to remove features with high cardinality to simplify the analysis. Then, we have visualized the remaining categorical features in a grid of plots, displaying the relative frequency of each category within these features. These visualizations help to quickly grasp which categories are most or least common.\n",
    "\n",
    "there are some important thing I want to mention here: \n",
    "\n",
    "\n",
    "# Cloudiness in Illinois\n",
    "\n",
    "Illinois has never been considered the Sunshine State. However, cities in Illinois are considered in the middle ranks of U.S. cities in terms of cloudiness. They are neither as sunny as Las Vegas nor as cloudy as Seattle.\n",
    "\n",
    "Here is a breakdown of the number of days with clear, partly cloudy, and cloudy skies for select cities in Illinois with long-term observations. The daily average sky cover, based on hourly weather observations of cloud types and coverage (in tenths), is classified into three classes:\n",
    "- Clear: daily average sky cover ranging from 0 to 3 tenths\n",
    "- Partly cloudy: daily average sky cover from 4 to 6 tenths\n",
    "- Cloudy: daily average sky cover from 7 to 10 tenths\n",
    "\n",
    "## Annual Average Number of Days by City\n",
    "\n",
    "| City       | Clear | Partly Cloudy | Cloudy |\n",
    "|------------|-------|----------------|--------|\n",
    "| Cairo      | 113   | 104            | 149    |\n",
    "| Chicago    | 84    | 105            | 176    |\n",
    "| Moline     | 101   | 100            | 164    |\n",
    "| Peoria     | 95    | 97             | 172    |\n",
    "| Rockford   | 93    | 98             | 174    |\n",
    "| Springfield| 104   | 94             | 167    |\n",
    "\n",
    "You can also visit the [National Centers for Environmental Information](https://www.ncdc.noaa.gov/) for more information on cloudiness of U.S. cities.\n",
    "\n",
    "\n",
    "but as you can see in our dataset the Clear weather is about 78.5% of our data which means most of the accident happened in clear way and we can see from the report that there are only 84 days with clear weather condition in chicago, so this means that people maybe care more about driving in Cloudy day so there are less accident happening with this condition.\n",
    "<\\span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 2.3. Explore the Data\n",
    "\n",
    "2.3.1. Continuous Features Tabular Report<br>\n",
    "2.3.2. Categorical Features Tabular Report <br>\n",
    "2.3.3. Correlations Matrix<br>\n",
    "2.3.4. HeatMap<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3.1. Continuous Features Tabular Report\n",
    "\n",
    "def get_continuous_features_tabular_report(df):\n",
    "    # continuous_features_tabular_report_df = df.describe(include=['number'])\n",
    "    return df.describe(include=['number'])\n",
    "\n",
    "# Transpose - More friendly print\n",
    "continuous_features_transpose_tabular_report_df = get_continuous_features_tabular_report(df).transpose()\n",
    "\n",
    "continuous_features_transpose_tabular_report_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### 2.3.1. Continuous Features Tabular Report\n",
    "##### Analysis\n",
    "\n",
    "Using tabular report, we can see that several issues on the data quality, examples:\n",
    "* LANE_CNT max value is \"1.191625e+06\" - This not possible at all.\n",
    "* LATITUDE min value is \"0\" - This is not possible since this is out of the range of Chicago city.\n",
    "\n",
    "Deeper and more detailed analysis will follow in the next sections.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3.2. Categorical Features Tabular Report\n",
    "\n",
    "def get_categorical_features_tabular_report(df):\n",
    "    return df.describe(exclude=['number'])\n",
    "\n",
    "# Transpose - More friendly print\n",
    "categorical_features_transpose_tabular_report_df = get_categorical_features_tabular_report(df).transpose()\n",
    "\n",
    "categorical_features_transpose_tabular_report_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### 2.3.2. Categorical Features Tabular Report\n",
    "##### Analysis\n",
    "\n",
    "The categorical features tabular gives important insights especially for unique and count.\n",
    "We can immediately detect features with high dimensionality and features that mostly missing (need to be dropped).\n",
    "\n",
    "Deeper and more detailed analysis will follow in the next sections.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3.3. Correlation insights\n",
    "\n",
    "def get_correlation_insights(df):\n",
    "    # Get the relevant columns\n",
    "    subset_df = df[get_continuous_features_tabular_report(df).columns]\n",
    "    return subset_df.corr()\n",
    "\n",
    "correlation_matrix = get_correlation_insights(df)\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3.4. HeatMap\n",
    "\n",
    "def draw_heatmap(correlation_matrix):\n",
    "    # Create a heatmap using seaborn\n",
    "    plt.figure(figsize=(16, 14))\n",
    "    return sns.heatmap(correlation_matrix, cmap='coolwarm', annot=True, fmt=\".2f\", linewidths=0.8)\n",
    "\n",
    "draw_heatmap(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### 2.3.3. Correlation insights + 2.3.4. HeatMap\n",
    "##### Analysis\n",
    "\n",
    "The categorical features tabular gives important insights especially for unique and count.\n",
    "We can immediately detect features with high dimensionality and features that mostly missing (need to be dropped).\n",
    "\n",
    "Deeper and more detailed analysis will follow in the next sections.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 2.4. Verify Data Quality\n",
    "\n",
    "2.4.1. Missing Values Summary<br>\n",
    "2.4.2. Irregular cardinality<br>\n",
    "* Features with a cardinality of 1.<br>\n",
    "* Too high cardinality for categorical features.<br>\n",
    "* Too low cardinality for continous features.<br>\n",
    "\n",
    "2.4.3. Outliers<br>\n",
    "2.4.4. Uniqueness - Remove duplicated or irrelevant repetition in data.<br>\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.1. Missing Values Summary\n",
    "\n",
    "# Get missing values details\n",
    "# Retrun value per feature as a numeric value and a percentage\n",
    "def get_missing_values_details(my_df):\n",
    "    # Check for missing values in each column\n",
    "    missing_values = my_df.isnull().sum()\n",
    "    missing_values_sorted = missing_values.sort_values(ascending=False)\n",
    "    # Check the percentage of missing values for each column\n",
    "    missing_percentage_sorted = (missing_values_sorted / len(my_df)) * 100\n",
    "    return missing_values_sorted, missing_percentage_sorted\n",
    "\n",
    "\n",
    "def get_completeness_report(my_df):\n",
    "    missing_values_sorted, missing_percentage_sorted = get_missing_values_details(my_df)\n",
    "    # Create a DataFrame to summarize the completeness\n",
    "    completeness_report = pd.DataFrame({\n",
    "        'Missing Values': missing_values_sorted,\n",
    "        'Missing Percentage': missing_percentage_sorted\n",
    "    })\n",
    "    return completeness_report\n",
    "\n",
    "completeness_report = get_completeness_report(df)\n",
    "completeness_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### 2.4.1. Missing Values Summary\n",
    "##### Analysis\n",
    "\n",
    "As observed in the table above, the following values has very high missing values (more than 30%).\n",
    "\n",
    "* WORKERS_PRESENT_I\n",
    "* DOORING_I\n",
    "* WORK_ZONE_TYPE\n",
    "* WORK_ZONE_I\n",
    "* PHOTOS_TAKEN_I\n",
    "* STATEMENTS_TAKEN_I\n",
    "* NOT_RIGHT_OF_WAY_I\n",
    "* CRASH_DATE_EST_I\n",
    "* INTERSECTION_RELATED_I\n",
    "* LANE_CNT\n",
    "* HIT_AND_RUN_I\n",
    "\n",
    "<br>\n",
    "\n",
    "Other columns with missing values we will drop instances or do imputation. Details will follow in the preprocessing section.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.2. Irregular cardinality - Features with a cardinality of 1\n",
    "\n",
    "IRREGULAR_CARD_SINGLE = 1             # To detect features with cardinality of 1\n",
    "\n",
    "def get_irregular_cardinality_with_single_cardinality(my_df):\n",
    "    unique_counts = my_df.nunique()\n",
    "    unique_counts_single_threshold = unique_counts[unique_counts == IRREGULAR_CARD_SINGLE]\n",
    "    return pd.DataFrame(unique_counts_single_threshold)\n",
    "\n",
    "\n",
    "get_irregular_cardinality_with_single_cardinality(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.2. Irregular cardinality - Too high cardinality for categorical features\n",
    "\n",
    "IRREGULAR_CARD_MAX_THRESHOLD = 40     # To detect categorical features with high cardinality\n",
    "\n",
    "def get_unique_counts_exceed_max_threshold_categorical(my_df):\n",
    "    cardinality = my_df.select_dtypes(exclude=['number']).apply(lambda x: x.nunique())\n",
    "    high_cardinality_cols = cardinality[cardinality > IRREGULAR_CARD_MAX_THRESHOLD].index.tolist()\n",
    "    # print(\"High Cardinality Columns:\", high_cardinality_cols)\n",
    "    return high_cardinality_cols\n",
    "\n",
    "high_cardinality_cols = get_unique_counts_exceed_max_threshold_categorical(df)\n",
    "# Categorical Features with carinality higher than IRREGULAR_CARD_MAX_THRESHOLD\n",
    "print(f\"Categorical Features with carinality higher than {IRREGULAR_CARD_MAX_THRESHOLD} cardinality are: {high_cardinality_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.2. Irregular cardinality - Too low cardinality for continuous features\n",
    "\n",
    "IRREGULAR_CARD_MIN_THRESHOLD = 10      # To detect continuous features with low cardinality\n",
    "\n",
    "def get_unique_counts_exceed_min_threshold_continuous(my_df):\n",
    "    continuous = my_df.select_dtypes(include=['number']).apply(lambda x: x.nunique())\n",
    "    high_cardinality_cols = continuous[continuous < IRREGULAR_CARD_MIN_THRESHOLD].index.tolist()\n",
    "    # print(\"High Cardinality Columns:\", high_cardinality_cols)\n",
    "    return high_cardinality_cols\n",
    "\n",
    "get_unique_counts_exceed_min_threshold_continuous(df)\n",
    "# df['PRIM_CONTRIBUTORY_CAUSE'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### 2.4.2.  - Irregular Cardinality\n",
    "##### Analysis\n",
    "\n",
    "**Features with a cardinality of 1**<br>\n",
    "INJURIES_UNKNOWN feature has cardinality of 1. It is useless in the modeling phase, and overall in the prediction model. it will be dropped in the preprocessing phase.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Too high cardinality for categorical features**<br>\n",
    "\n",
    "For the following features - please find details and plan about each one.\n",
    "\n",
    "* CRASH_RECORD_ID - Will be dropped, this is just ID.\n",
    "* CRASH_DATE - It makes sense DateTime format highly varies, however, we do feature selection and store data in numeric values (e.g day of the week, month, year, ...).\n",
    "* DATE_POLICE_NOTIFIED - Will be dropped.\n",
    "* STREET_NAME - Will be dropped. To detect location we have longtiude and latitude.\n",
    "* LOCATION - This is duplicated feature. Longitude and Latitude provides same information but in numeric format. We will drop it.\n",
    "* PRIM_CONTRIBUTORY_CAUSE - Has cadrinality of 40, we will do feature engineering and map to 5 values (need to agree with the team).\n",
    "* SEC_CONTRIBUTORY_CAUSE - Has cadrinality of 40, we will do feature engineering and map to 5 values (need to agree with the team).\n",
    "<br>\n",
    "Please notice PRIM_CONTRIBUTORY_CAUSE and SEC_CONTRIBUTORY_CAUSE have exact same classes.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Too low cardinality for continuous features**<br>\n",
    "\n",
    "For the following features - please find details and plan about each one.\n",
    "* INJURIES_FATAL - Makes sense it is low number (which is good! Less people died)\n",
    "* INJURIES_UNKNOWN - Same here, this is indication for unkown injuries.\n",
    "* CRASH_DAY_OF_WEEK - Number vary between 1-7 (Mon, Tue, ... Sun). So value makes sense.\n",
    "\n",
    "Nothing to do in this section.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.3. Outliers\n",
    "\n",
    "def get_outliers_list_of_indices(df, ft):\n",
    "    Q1 = df[ft].quantile(0.25)\n",
    "    Q3 = df[ft].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    ls = df.index[ (df[ft] < lower_bound) | (df[ft] > upper_bound) ]\n",
    "    \n",
    "    return ls\n",
    "\n",
    "# To store the indicies\n",
    "outliers_index_list = []\n",
    "# To store the features (set, so no duplications)\n",
    "outliers_features_set = set()\n",
    "numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "for feature in numeric_columns:\n",
    "    indicies_list = get_outliers_list_of_indices(df, feature)\n",
    "    outliers_index_list.extend(indicies_list)\n",
    "    if len(indicies_list) != 0:\n",
    "        outliers_features_set.add(feature)\n",
    "\n",
    "print(f\"There are {len(outliers_index_list)} outliers\")\n",
    "print(f\"Features set are {outliers_features_set}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### 2.4.3.  - Outliers\n",
    "##### Analysis\n",
    "\n",
    "Outliers inital result shows the following features are outliers:\n",
    "* STREET_NO\n",
    "* POSTED_SPEED_LIMIT\n",
    "* INJURIES_REPORTED_NOT_EVIDENT\n",
    "* LONGITUDE\n",
    "* LANE_CNT\n",
    "* BEAT_OF_OCCURRENCE\n",
    "* INJURIES_TOTAL\n",
    "* INJURIES_INCAPACITATING\n",
    "* INJURIES_NON_INCAPACITATING\n",
    "* INJURIES_NO_INDICATION\n",
    "* LATITUDE\n",
    "* INJURIES_FATAL\n",
    "* NUM_UNITS\n",
    "\n",
    "Let's understand if they are really outliers, or the values are valid (e.g. street number is valid to be in a very big range).\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_continuous_feature_details(df, 'STREET_NO')\n",
    "get_continuous_feature_details(df, 'POSTED_SPEED_LIMIT')\n",
    "get_continuous_feature_details(df, 'INJURIES_REPORTED_NOT_EVIDENT')\n",
    "get_continuous_feature_details(df, 'LONGITUDE')\n",
    "get_continuous_feature_details(df, 'LANE_CNT')\n",
    "get_continuous_feature_details(df, 'BEAT_OF_OCCURRENCE')\n",
    "get_continuous_feature_details(df, 'INJURIES_TOTAL')\n",
    "get_continuous_feature_details(df, 'INJURIES_INCAPACITATING')\n",
    "get_continuous_feature_details(df, 'INJURIES_NON_INCAPACITATING')\n",
    "get_continuous_feature_details(df, 'INJURIES_NO_INDICATION')\n",
    "get_continuous_feature_details(df, 'LATITUDE')\n",
    "get_continuous_feature_details(df, 'INJURIES_FATAL')\n",
    "get_continuous_feature_details(df, 'NUM_UNITS')\n",
    "pass # To avoid printing of the previous line and keep only formatted-pretty prints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### 2.4.3.  - Outliers (Continued)\n",
    "##### Analysis\n",
    "\n",
    "| Feature                        | Unique Values Len | Unique Values Example                  | Max Value    | Min Value    | Comments                                                                 | Explanation                                        |\n",
    "|--------------------------------|-------------------|----------------------------------------|--------------|--------------|--------------------------------------------------------------------------|----------------------------------------------------|\n",
    "| STREET_NO                      | 11723             | 0, 1, 2, 3, 4, ..., 13787, 13795, 13799, 34453, 451100 | 451100       | 0            | Street numbers in Chicago, large range due to city size and data errors. | Street values can highly vary, it doesn't follow math logic      |\n",
    "| POSTED_SPEED_LIMIT             | 46                | 0, 1, 2, 3, 4, ..., 62, 63, 65, 70, 99 | 99           | 0            | Posted speed limits, including possibly erroneous values.                | Speed limits can highly vary. We checked all values count to verify no outliers in respect to allowed speed - e.g. no instance with value 500 (please refer to the function dump_feature_frequency_to_a_file)       |\n",
    "| INJURIES_REPORTED_NOT_EVIDENT  | 14                | 0.0, 1.0, 2.0, 3.0, 4.0, ..., 9.0, 10.0, 11.0, 15.0 | 15.0         | 0.0          | Number of non-evident injuries reported per crash.                       | Range is reasonable for severe accidents.           |\n",
    "| LONGITUDE                      | 299263            | -87.936192947, ..., -87.524587387, 0.0 | 0.0          | -87.936192947| Longitude values in Chicago, including erroneous 0.0 entries.            | 0.0 likely represents missing or incorrectly entered data. It will be handled in data preprocessing phase. |\n",
    "| LANE_CNT                       | 42                | 0.0, 1.0, 2.0, 3.0, 4.0, ..., 1191625.0| 1191625.0    | 0.0          | Number of lanes, with some unrealistic values due to errors.             | Extreme max value due to data entry errors.        |\n",
    "| BEAT_OF_OCCURRENCE             | 277               | 111.0, ..., 2535.0, 6100.0             | 6100.0       | 111.0        | Police beat codes, including possibly erroneous 6100.0.                 | This is police internal codes and can't be treated as outliers          |\n",
    "| INJURIES_TOTAL                 | 21                | 0.0, 1.0, ..., 19.0, 21.0              | 21.0         | 0.0          | Total number of injuries per crash.                                      | Range is within expected limits for traffic accidents. |\n",
    "| INJURIES_INCAPACITATING        | 11                | 0.0, 1.0, ..., 8.0, 10.0               | 10.0         | 0.0          | Number of incapacitating injuries per crash.                             | Within expected limits, considering severe cases.   |\n",
    "| INJURIES_NON_INCAPACITATING    | 20                | 0.0, 1.0, ..., 19.0, 21.0              | 21.0         | 0.0          | Number of non-incapacitating injuries per crash.                         | Range reflects possible severe accidents.           |\n",
    "| INJURIES_NO_INDICATION         | 49                | 0.0, 1.0, ..., 50.0, 61.0              | 61.0         | 0.0          | People involved in crash with no injuries reported.                      | High values for large accidents, not outliers.     |\n",
    "| LATITUDE                       | 299300            | 0.0, 41.644670132, ..., 42.022779861   | 42.022779861 | 0.0          | Latitude values in Chicago, including erroneous 0.0 entries.             | 0.0 likely represents missing or incorrectly entered data. It will be handled in data preprocessing phase. |\n",
    "| INJURIES_FATAL                 | 6                 | 0.0, 1.0, ..., 3.0, 4.0                | 4.0          | 0.0          | Number of fatal injuries per crash.                                      | Fatalities are rare, but values are within expected limits. |\n",
    "| NUM_UNITS                      | 17                | 1, 2, ..., 16, 18                      | 18           | 1            | Number of units involved in a crash (e.g. cars, motocycles, ...) | Not an outlier, the highest values sounds reasonable |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "From the table above  we can notice that all features don't have outliers, except the following features:\n",
    "* LANE_CNT\n",
    "* LONGITUDE\n",
    "* LATITUDE\n",
    "\n",
    "\n",
    "Doing research in Google Maps and Google for Chicago city LATITUDE and LONGITUDE range (also NaN, but will be addressing in missing values section), here is our findings:<br>\n",
    "LATITUDE valid range: [41.640, 42.023]<br>\n",
    "LONGITUDE valid range: [-87.940, -87.524]<br>\n",
    "\n",
    "We will address that again in the Data Preprocessing section to clean up the out of range values.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.4. Uniqueness - Remove duplicated or irrelevant repetition in data\n",
    "\n",
    "def consistency_check_duplicated_instances(df):\n",
    "    duplicate_rows = df.duplicated().sum()\n",
    "    print(f\"Number of duplicate rows: {duplicate_rows}\")\n",
    "    # If you want to actually see the duplicate rows, you can use:\n",
    "    if duplicate_rows > 0:\n",
    "        print(df[df.duplicated(keep=False)].sort_values(by=df.columns.tolist()))\n",
    "\n",
    "consistency_check_duplicated_instances(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### 2.4.4. Uniqueness - Remove duplicated or irrelevant repetition in data\n",
    "##### Analysis\n",
    "\n",
    "No issues found.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 3. CRISP-DM: Data Preparation\n",
    "\n",
    "3.1. Missing Values<br>\n",
    "3.2. Duplicated Columns - LOCATION is the pair LATITUDE and LONGITUDE<br>\n",
    "3.3. Drop CRASH_ID - ID column<br>\n",
    "3.4. Irregular Cardinality<br>\n",
    "3.5. Outliers<br>\n",
    "3.6. Feature Egnineering - Extract Date Infromation<br>\n",
    "\n",
    "More details in each section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a copy from the original dataset to start preprocessing phase\n",
    "\n",
    "df_cleaned = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1. Missing Values (Part-1) - Drop\n",
    "\n",
    "# Set the threshold for dropping columns\n",
    "MISSING_THRESHOLD = 30.0\n",
    "\n",
    "# Identify columns that have missing value percentage greater than the threshold\n",
    "columns_to_drop = completeness_report[completeness_report['Missing Percentage'] >= MISSING_THRESHOLD].index\n",
    "\n",
    "# Drop these columns from the DataFrame\n",
    "df_cleaned = df_cleaned.drop(columns=columns_to_drop)\n",
    "\n",
    "# Now, df has the columns dropped where the missing value percentage was higher than 30%\n",
    "print(\"Features with missing instances higher than 30% that has been dropped\")\n",
    "columns_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1. Missing Values (Part-2) - Special handle for LATITUDE and LONGITUDE\n",
    "\n",
    "def drop_missing_instances(my_df, my_feature):\n",
    "    df_cleaned = my_df.dropna(subset=[my_feature])\n",
    "    return df_cleaned\n",
    "df_cleaned = drop_missing_instances(df_cleaned, 'LATITUDE')\n",
    "df_cleaned = drop_missing_instances(df_cleaned, 'LONGITUDE')\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1. Missing Values (Part-3) - Imputation for values\n",
    "\n",
    "# Identify columns that have missing value percentage less than the imputation threshold\n",
    "columns_to_impute = completeness_report[completeness_report['Missing Percentage'] < MISSING_THRESHOLD].index\n",
    "\n",
    "# Loop through the columns and perform imputation\n",
    "for column in columns_to_impute:\n",
    "    if df_cleaned[column].dtype == 'numeric':\n",
    "        # Impute numerical columns with the mean value\n",
    "        df_cleaned[column].fillna(df_cleaned[column].mean(), inplace=True)\n",
    "    else:\n",
    "        # Impute categorical columns with the mode value (the most frequent value)\n",
    "        df_cleaned[column].fillna(df_cleaned[column].mode()[0], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1. Missing Values (Part-43) - Print for visibility of current situation\n",
    "\n",
    "completeness_report = get_completeness_report(df_cleaned)\n",
    "completeness_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### 3.1. Missing Values\n",
    "##### Analysis\n",
    "\n",
    "\n",
    "**Missing**\n",
    "\n",
    "For missing values higher than 30%, we drop them.\n",
    "\n",
    "* WORKERS_PRESENT_I\n",
    "* DOORING_I\n",
    "* WORK_ZONE_TYPE\n",
    "* WORK_ZONE_I\n",
    "* PHOTOS_TAKEN_I\n",
    "* STATEMENTS_TAKEN_I\n",
    "* NOT_RIGHT_OF_WAY_I\n",
    "* CRASH_DATE_EST_I\n",
    "* INTERSECTION_RELATED_I\n",
    "* LANE_CNT\n",
    "* HIT_AND_RUN_I\n",
    "\n",
    "Please refer to missing values report in the Data Understanding section for more details.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Special handle for LATITUDE and LONGITUDE**\n",
    "Missing values for LATITUDE and LONGITUDE is less than 1%, we will drop these instances.\n",
    "Imputation is bad for this case, we prefer to have high accuracy and valid data for the location related params.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Imputation for values**\n",
    "\n",
    "For missing values lower than 30% - we do imputation. To be more accurate, in the terms of our dataset, missing values under 30% ranges between 0.1%-3% missing values so imputation is a very reasonable choice in this case.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2. Duplicated Columns - LOCATION is the pair LATITUDE and LONGITUDE\n",
    "\n",
    "df_cleaned = df_cleaned.drop('LOCATION', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### 3.2. Duplicated Columns - LOCATION is the pair LATITUDE and LONGITUDE\n",
    "##### Analysis\n",
    "\n",
    "LOCATION is the pair of values of LONGITUDE and LATITUDE, we choose to drop it due to two reasons:\n",
    "1. It is duplication of other features (aggregation of LONGITUDE and LATITUDE in pairs will result in LOCATION).\n",
    "2. It is categorical feature, with high class dimensionality - meaning, in the encoding phase to prepare for modeling, it will generate very big number of derived features. In the other hand, LONGITUDE and LATITUDE is numeric and much easier to the handling in modeling phase.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3. Drop CRASH_ID - ID column\n",
    "\n",
    "df_cleaned = df_cleaned.drop('CRASH_RECORD_ID', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### 3.3. Drop CRASH_ID - ID column\n",
    "##### Analysis\n",
    "\n",
    "It is just ID of the crash, and has no contributation to the prediction model.\n",
    "<br>\n",
    "This is also could be dropped as part of irregular cardinality.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4. Irregular Cardinality\n",
    "\n",
    "# Drop feature with cardinality of 1\n",
    "df_cleaned = df_cleaned.drop('INJURIES_UNKNOWN', axis=1)\n",
    "\n",
    "# Too high cardinality for categorical features\n",
    "# CRASH_RECORD_ID - Already dropped.\n",
    "# df_cleaned = df_cleaned.drop('CRASH_DATE', axis=1) # Will be deleted later after feature engineering\n",
    "df_cleaned = df_cleaned.drop('DATE_POLICE_NOTIFIED', axis=1)\n",
    "df_cleaned = df_cleaned.drop('STREET_NAME', axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### 3.4. Irregular Cardinality\n",
    "##### Analysis\n",
    "\n",
    "<br>\n",
    "\n",
    "**Drop feature with cardinality of 1**\n",
    "\n",
    "\n",
    "INJURIES_UNKNOWN feature has cardinality of 1 --> Drop.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Too high cardinality for categorical features**<br>\n",
    "\n",
    "For the following features - please find details and plan about each one.\n",
    "\n",
    "In addition to what was dropped in 3.2. (LOCATION), 3.3. (MONTH), and 3.4 (CRASH_ID) - The following will be dropped.\n",
    "* CRASH_DATE - Will be handled later after feature selection\n",
    "* DATE_POLICE_NOTIFIED\n",
    "* STREET_NAME - Will be dropped. To detect location we have longtiude and latitude.\n",
    "* LOCATION - This is duplicated feature. Longitude and Latitude provides same information but in numeric format. We will drop it.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Too low cardinality for continuous features**<br>\n",
    "\n",
    "Nothing to do in this section.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3.5. - Outliers\n",
    "\n",
    "print(\"========================== Values Before ==========================\")\n",
    "get_continuous_feature_details(df_cleaned, 'LATITUDE')\n",
    "get_continuous_feature_details(df_cleaned, 'LONGITUDE')\n",
    "# get_continuous_feature_details(df_fix_range_issues, 'LANE_CNT') # Already dropped due to high missing values\n",
    "\n",
    "# To remove values for my_feature outside a given range range (including min_value and max_value)\n",
    "def remove_invalid_values_outside_given_range(my_df, my_feature, min_value, max_value):\n",
    "    # Create a mask for values within the range\n",
    "    valid_mask = (my_df[my_feature] >= min_value) & (my_df[my_feature] <= max_value)\n",
    "    # Apply the mask to the DataFrame\n",
    "    filtered_df = my_df[valid_mask]\n",
    "    return filtered_df\n",
    "\n",
    "# Define the valid range for LATITUDE\n",
    "min_latitude = 41.640\n",
    "max_latitude = 42.023\n",
    "df_cleaned = remove_invalid_values_outside_given_range(df_cleaned, 'LATITUDE', min_latitude, max_latitude)\n",
    "\n",
    "# Define the valid range for LONGITUDE\n",
    "min_longitude = -87.940\n",
    "max_longitude = -87.524\n",
    "df_cleaned = remove_invalid_values_outside_given_range(df_cleaned, 'LONGITUDE', min_longitude, max_longitude)\n",
    "\n",
    "\n",
    "# Already dropped due to high missing values count\n",
    "# min_lane_count = 0\n",
    "# max_lane_count = 10\n",
    "# df_fix_range_issues = remove_invalid_values_outside_given_range(df_fix_range_issues, 'LANE_CNT', min_lane_count, max_lane_count)\n",
    "\n",
    "\n",
    "print(\"========================== Values After ==========================\")\n",
    "get_continuous_feature_details(df_cleaned, 'LATITUDE')\n",
    "get_continuous_feature_details(df_cleaned, 'LONGITUDE')\n",
    "# get_continuous_feature_details(df_fix_range_issues, 'LANE_CNT')\n",
    "\n",
    "get_continuous_features_tabular_report(df_cleaned).transpose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### 3.5. Outliers\n",
    "##### Analysis\n",
    "\n",
    "This section addresses problems mostly in ranges for special features.\n",
    "For example, week can't be more than 7 days.\n",
    "Let's go over the descriptive features and provide insights.\n",
    "\n",
    "We already described that above, but we address it here again.\n",
    "\n",
    "| Feature            | Unique Values Len | Unique Values Example    | Max Value | Min Value | Comments                |\n",
    "|--------------------|-------------------|--------------------------|-----------|-----------|-------------------------|\n",
    "| CRASH_HOUR         | 24 |  [0 1 2 ... 23]  |  23 | 0 | Time of the crash (0-23)|\n",
    "| CRASH_DAY_OF_WEEK  | 7 | [1 2 3 4 5 6 7] | 7 | 1 | Day of the week (1-7)   |\n",
    "| CRASH_MONTH        | 12 | [1 2 3 ... 12] |  12  |  1   | Month of the crash (1-12)|\n",
    "| LATITUDE           |  299299 |  [ 0 41.64467013 41.64469152 ... 42.02273632 ]      |    42.022779861       |     0.0      | Geographical latitude   |\n",
    "| LONGITUDE          |  299262  | [-87.93619295 -87.93587692 ... 0] |   0.0        |     -87.936192947      | Geographical longitude  |\n",
    "\n",
    "<br>\n",
    "\n",
    "As the table illustrates, the following features have valid values:<br>\n",
    "* CRASH_HOUR\n",
    "* CRASH_DAY_OF_WEEK\n",
    "* CRASH_MONTH\n",
    "\n",
    "<br>\n",
    "\n",
    "But the following features has values out of range / outliers in respect to each one type:\n",
    "* LATITUDE - Min value is 0.0\n",
    "* LONGITUDE - Max value is 0.0\n",
    "\n",
    "<br>\n",
    "\n",
    "Doing research in Google Maps and Google for Chicago city LATITUDE and LONGITUDE range (also NaN, but will be addressing in missing values section), here is our findings:<br>\n",
    "LATITUDE valid range: [41.640, 42.023]<br>\n",
    "LONGITUDE valid range: [-87.940, -87.524]<br>\n",
    "\n",
    "So for LATITUDE and LONGITUDE we make sure we are in the valid range.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.6. Feature Egnineering - Extract Date Infromation\n",
    "\n",
    "# Convert CRASH_DATE to datetime\n",
    "df_cleaned['CRASH_DATE'] = pd.to_datetime(df_cleaned['CRASH_DATE'])\n",
    "\n",
    "# Extract components from CRASH_DATE\n",
    "df_cleaned['YEAR'] = df_cleaned['CRASH_DATE'].dt.year\n",
    "df_cleaned['DAY'] = df_cleaned['CRASH_DATE'].dt.day\n",
    "\n",
    "# # Drop the CRASH_DATE\n",
    "df_cleaned = df_cleaned.drop('CRASH_DATE', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### 3.6. Feature Egnineering - Extract Date Infromation\n",
    "##### Analysis\n",
    "\n",
    "Extraction of YEAR and DAY from DateTime format of CRASH_DATE.<br>\n",
    "Please notice for month, we already have CRASH_MONTH.<br>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug Only\n",
    "\n",
    "# dump_feature_frequency_to_a_file(df_cleaned ,'POSTED_SPEED_LIMIT')\n",
    "# dump_feature_frequency_to_a_file(df_cleaned ,'DAMAGE')\n",
    "# dump_feature_frequency_to_a_file(df_cleaned ,'CRASH_TYPE')\n",
    "# dump_feature_frequency_to_a_file(df_cleaned ,'REPORT_TYPE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.7. Feature Engineering - Contributing Causes\n",
    "\n",
    "# Commented - Need to discuss.\n",
    "\n",
    "# df_fe_contributing_cause = df_outliers.copy()\n",
    "\n",
    "\n",
    "# # Define the mapping of original causes to the 5 broad categories\n",
    "# category_mapping = {\n",
    "#     'Traffic Sign and Signal Violations': [\n",
    "#         'DISREGARDING STOP SIGN', 'DISREGARDING TRAFFIC SIGNALS', 'DISREGARDING YIELD SIGN',\n",
    "#         'DISREGARDING OTHER TRAFFIC SIGNS', 'DISREGARDING ROAD MARKINGS',\n",
    "#         'BICYCLE ADVANCING LEGALLY ON RED LIGHT', 'MOTORCYCLE ADVANCING LEGALLY ON RED LIGHT',\n",
    "#         'TURNING RIGHT ON RED'\n",
    "#     ],\n",
    "#     'Driver Behavior and Condition': [\n",
    "#         'DISTRACTION - FROM INSIDE VEHICLE', 'DISTRACTION - FROM OUTSIDE VEHICLE',\n",
    "#         'DISTRACTION - OTHER ELECTRONIC DEVICE (NAVIGATION DEVICE, DVD PLAYER, ETC.)',\n",
    "#         'DRIVING SKILLS/KNOWLEDGE/EXPERIENCE',\n",
    "#         'OPERATING VEHICLE IN ERRATIC, RECKLESS, CARELESS, NEGLIGENT OR AGGRESSIVE MANNER',\n",
    "#         'PHYSICAL CONDITION OF DRIVER', 'HAD BEEN DRINKING (USE WHEN ARREST IS NOT MADE)',\n",
    "#         'UNDER THE INFLUENCE OF ALCOHOL/DRUGS (USE WHEN ARREST IS EFFECTED)', 'TEXTING',\n",
    "#         'CELL PHONE USE OTHER THAN TEXTING'\n",
    "#     ],\n",
    "#     'Vehicle Operation and Conditions': [\n",
    "#         'FAILING TO REDUCE SPEED TO AVOID CRASH', 'FAILING TO YIELD RIGHT-OF-WAY',\n",
    "#         'FOLLOWING TOO CLOSELY', 'IMPROPER BACKING', 'IMPROPER LANE USAGE',\n",
    "#         'IMPROPER OVERTAKING/PASSING', 'IMPROPER TURNING/NO SIGNAL', 'DRIVING ON WRONG SIDE/WRONG WAY',\n",
    "#         'EQUIPMENT - VEHICLE CONDITION'\n",
    "#     ],\n",
    "#     'Environmental and External Conditions': [\n",
    "#         'ANIMAL', 'EVASIVE ACTION DUE TO ANIMAL, OBJECT, NONMOTORIST', 'OBSTRUCTED CROSSWALKS',\n",
    "#         'ROAD CONSTRUCTION/MAINTENANCE', 'ROAD ENGINEERING/SURFACE/MARKING DEFECTS',\n",
    "#         'VISION OBSCURED (SIGNS, TREE LIMBS, BUILDINGS, ETC.)', 'WEATHER', 'RELATED TO BUS STOP'\n",
    "#     ],\n",
    "#     'Speed and Compliance': [\n",
    "#         'EXCEEDING AUTHORIZED SPEED LIMIT', 'EXCEEDING SAFE SPEED FOR CONDITIONS',\n",
    "#         'PASSING STOPPED SCHOOL BUS'\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# # Reverse the mapping to make it easier to map each cause to its new category\n",
    "# reversed_mapping = {}\n",
    "# for category, causes in category_mapping.items():\n",
    "#     for cause in causes:\n",
    "#         reversed_mapping[cause] = category\n",
    "\n",
    "# # Default category for those not listed\n",
    "# default_category = 'Miscellaneous'\n",
    "\n",
    "# # Create a new columns with the mapped categories\n",
    "# df_fe_contributing_cause['PRIM_CONTRIBUING_CAUSE_UPDATED'] = df_fe_contributing_cause['PRIM_CONTRIBUTORY_CAUSE'].map(lambda x: reversed_mapping.get(x, default_category))\n",
    "# df_fe_contributing_cause['SEC_CONTRIBUING_CAUSE_UPDATED'] = df_fe_contributing_cause['SEC_CONTRIBUTORY_CAUSE'].map(lambda x: reversed_mapping.get(x, default_category))\n",
    "\n",
    "# # Drop the old columns\n",
    "# df_fe_contributing_cause = df_fe_contributing_cause.drop('PRIM_CONTRIBUTORY_CAUSE', axis=1)\n",
    "# df_fe_contributing_cause = df_fe_contributing_cause.drop('SEC_CONTRIBUTORY_CAUSE', axis=1)\n",
    "\n",
    "# df_fe_prim_contributing_cause['PRIM_CONTRIBUING_CAUSE_UPDATED'].head(5)\n",
    "# df_fe_prim_contributing_cause['SEC_CONTRIBUING_CAUSE_UPDATED'].tail(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### 3.7. Feature Engineering - Contributing Causes\n",
    "##### Analysis\n",
    "\n",
    "PRIM_CONTRIBUTORY_CAUSE and SEC_CONTRIBUTORY_CAUSE each one has excatly 40 possible classes which are the same.\n",
    "We map the to 5 catagories as illustrated in the code.\n",
    "\n",
    "Purpose of this is two reasons:\n",
    "1. To generalize the reason of contributing factors, and see their impact on traffic injury prediction.\n",
    "2. To reduce high dimensionality.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Shape Before Preprocessing: {df.shape}\")\n",
    "print(f\"Shape After  Preprocessing: {df_cleaned.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "# 4. CRISP-DM: Modeling\n",
    "\n",
    "**Section Overview**\n",
    "\n",
    "4.1. Import relevant ML libs for Modeling.<br>\n",
    "4.2. Formaluize a Numerical Measure for the Target Variable(s).<br>\n",
    "4.3. Undersampling - To fix over-representation in the dataset (unbalanced data).<br>\n",
    "4.4. Modeling with Random Forest.<br>\n",
    "4.5. Modeling with Logistic  Regression.<br>\n",
    "4.6. Modeling with DNN.<br>\n",
    "4.7. Modeling with KNN.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1. Import relevant ML libs for Modeling\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2. Formaluize a Numerical Measure for the Target Variable(s)\n",
    "\n",
    "df_to_model = df_cleaned.copy()\n",
    "\n",
    "# Building our numerical metrics for measuring the Severity of injuries based on the reported injuries in the dataset\n",
    "df_to_model['SEVERITY_OF_INJURIES'] = ((0.3 * df_to_model['INJURIES_NON_INCAPACITATING'] + \n",
    "                                 0.6 * df_to_model['INJURIES_INCAPACITATING'] + \n",
    "                                 # Assuming we might use 'INJURIES_INCAPACITATING' or another column for fatal injuries representation\n",
    "                                 0.1 * df_to_model['INJURIES_NO_INDICATION'] + df_to_model['INJURIES_FATAL']) / \n",
    "                                ((df_to_model['INJURIES_NON_INCAPACITATING'] + \n",
    "                                      df_to_model['INJURIES_INCAPACITATING'] + \n",
    "                                      # Again, assuming a placeholder for fatal injuries if needed\n",
    "                                      df_to_model['INJURIES_NO_INDICATION']+df_to_model['INJURIES_FATAL'])))\n",
    "\n",
    "# Code to create the \"INJURY_CLASS\" feature based on \"SEVERITY_OF_INJURIES\"\n",
    "df_to_model['INJURY_ClASS'] = df_to_model['SEVERITY_OF_INJURIES'].apply(lambda x: 'HIGH INJURY' if x > 0.2 else 'LIGHT INJURY')\n",
    "\n",
    "# Code to create the \"SEVERITY_CLASS\" feature based on \"INJURIES_FATAL\" and \"INJURIES_INCAPACITATING\"\n",
    "df_to_model['SEVERITY_CLASS'] = df_to_model.apply(lambda x: 'HIGH SEVERITY' if x['INJURIES_FATAL'] > 0 or x['INJURIES_INCAPACITATING'] > 0 else 'LOW SEVERITY', axis=1)\n",
    "\n",
    "# Display the first few rows to see the new feature\n",
    "df_to_model[['INJURIES_NON_INCAPACITATING','INJURIES_FATAL', 'INJURIES_INCAPACITATING', 'INJURIES_NO_INDICATION', 'SEVERITY_OF_INJURIES','INJURY_ClASS','SEVERITY_CLASS']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### 4.2. Formaluize a Numerical Measure for the Target Variable(s)\n",
    "##### Analysis\n",
    "\n",
    "Our target variable is represented via 4 variables were each one giving the following indications:\n",
    "* INJURIES_NO_INDICATION - No injury or light reported.\n",
    "* INJURIES_NON_INCAPACITATING - Medium injury.\n",
    "* INJURIES_INCAPACITATING - Heavy.\n",
    "* INJURIES_FATAL - Death.\n",
    "\n",
    "To build a mesurement to estimate the fatality of the crash injury crash, we define a weighted equation for each class and normalize it.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug only - print columns\n",
    "df_to_model.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preparing features and target variables \n",
    "# X = df_to_model.drop(['SEVERITY_OF_INJURIES','INJURIES_TOTAL' ,'INJURIES_INCAPACITATING', 'MOST_SEVERE_INJURY','INJURY_ClASS','SEVERITY_CLASS',\n",
    "#                   'INJURIES_NON_INCAPACITATING', 'SEVERITY_CLASS','INJURIES_NO_INDICATION','CRASH_DATE',\n",
    "#                   'INJURIES_REPORTED_NOT_EVIDENT','INJURIES_FATAL','STREET_NAME'],axis = 1)\n",
    "\n",
    "\n",
    "X = df_to_model.drop(['SEVERITY_OF_INJURIES', 'INJURIES_TOTAL', 'INJURIES_INCAPACITATING', 'MOST_SEVERE_INJURY', \n",
    "                      'INJURY_ClASS', 'SEVERITY_CLASS', 'INJURIES_NON_INCAPACITATING', 'INJURIES_NO_INDICATION',\n",
    "                      'INJURIES_REPORTED_NOT_EVIDENT', 'INJURIES_FATAL'], axis=1)\n",
    "y = df_to_model['SEVERITY_CLASS']\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# Initialize the random under-sampler due to unbalanced dataset\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "\n",
    "# Resample the dataset\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "1. **Drops Irrelevant Columns**: Removes columns related to the severity and specifics of injuries, among others, that are not used as features for the prediction model. This step focuses the model on relevant predictors and avoids data leakage from features too closely related to the target variable.\n",
    "\n",
    "2. **Target Variable Selection**: Sets the `SEVERITY_CLASS` column as the target variable (`y`), which likely represents the classification outcome we want to predict.\n",
    "\n",
    "3. **One-Hot Encoding**: Applies `pd.get_dummies()` to convert categorical variables into a format that can be provided to machine learning algorithms. This step is crucial for models that require numerical input.\n",
    "\n",
    "4. **Addressing Class Imbalance**: Initializes and applies a Random Under-Sampler (`RandomUnderSampler`) to the dataset. This technique mitigates the issue of a highly imbalanced dataset by reducing the size of the over-represented class, leading to a more balanced dataset that can improve model performance. Choosing under-sampling over over-sampling is a strategic decision to prevent potential data leakage that could occur by duplicating entries of the under-represented class, which could give the model an unfair advantage by \"seeing\" duplicated data during training.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def split_data(X, y, test_size=0.2, random_state=42):\n",
    "    \n",
    "    # First, split into train+validation and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # Then split train+validation into train and validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "# logestic_regression\n",
    "def implement_logistic_regression(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "\n",
    "    # hyperparameter tuning for logestic regression model\n",
    "    best_accuracy = 0\n",
    "    best_c = None\n",
    "    for C in np.logspace(-4, 4, 20):\n",
    "        log_reg = LogisticRegression(C=C, max_iter=10000, random_state=42)\n",
    "        log_reg.fit(X_train, y_train)\n",
    "        accuracy = log_reg.score(X_val, y_val)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_c = C\n",
    "\n",
    "    # Initialize the Logistic Regression model with balanced class weights\n",
    "    log_reg = LogisticRegression(C=best_c, max_iter=10000, random_state=42)\n",
    "\n",
    "    # Fitting the model to the training data\n",
    "    log_reg.fit(X_train, y_train)\n",
    "\n",
    "    # Predicting on the test set\n",
    "    y_pred = log_reg.predict(X_test)\n",
    "\n",
    "    # Evaluating the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    classification_report_results = classification_report(y_test, y_pred)\n",
    "\n",
    "    print(f\"Best C: {best_c}\")\n",
    "    print(\"Accuracy of linear regressor:\", accuracy)\n",
    "    print(\"\\nClassification Report:\\n\", classification_report_results)\n",
    "        \n",
    "    return log_reg\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "def implement_random_forest(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "\n",
    "\n",
    "    # Hypterparameter tuning for random forest model\n",
    "    best_accuracy = 0\n",
    "    best_n_estimators = None\n",
    "    for n_estimators in range(10, 100, 10):\n",
    "        rf_classifier = RandomForestClassifier(n_estimators=n_estimators, class_weight='balanced', random_state=42)\n",
    "        rf_classifier.fit(X_train, y_train)\n",
    "        accuracy = rf_classifier.score(X_val, y_val)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_n_estimators = n_estimators\n",
    "\n",
    "\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=best_n_estimators, random_state=42)\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "    # Predicting on the test set\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Evaluating the model\n",
    "    classification_report_results = classification_report(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Best n_estimators: {best_n_estimators}\")\n",
    "    print(\"Accuracy of Random Forest Classifier:\", accuracy)\n",
    "    print(\"\\nClassification Report:\\n\", classification_report_results)\n",
    "    \n",
    "    return rf_classifier\n",
    "\n",
    "# Deep Neural Network \n",
    "def implement_DNN(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "\n",
    "    # It's important to scale your input features for neural networks\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Define the parameter grid to search\n",
    "    param_grid = {\n",
    "        'hidden_layer_sizes': [(10,10), (10,20), (20, 20)],\n",
    "        'activation': ['tanh', 'relu'],\n",
    "    }\n",
    "    \n",
    "    # Initialize GridSearchCV - note that we're using the training data for fitting\n",
    "    grid_search = GridSearchCV(MLPClassifier(max_iter=100, random_state=42), param_grid, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # Perform the grid search on the training dataset\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # The best_estimator_ is now the trained classifier with the best parameters found\n",
    "    best_classifier = grid_search.best_estimator_\n",
    "\n",
    "    # Predict on the test set with the best model\n",
    "    y_pred = best_classifier.predict(X_test_scaled)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    classification_report_results = classification_report(y_test, y_pred)\n",
    "\n",
    "    print(\"Best parameters found:\", grid_search.best_params_)\n",
    "    print(\"Accuracy of DNN model:\", accuracy)\n",
    "    print(\"\\nClassification Report:\\n\", classification_report_results)\n",
    "\n",
    "\n",
    "# K Nearest Neighbor\n",
    "def implement_KNN(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "\n",
    "    # Initialize the KNN classifier\n",
    "    # It's important to scale your input features for KNN\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    # Hyperparameter tuning for KNN\n",
    "    best_accuracy = 0\n",
    "    best_n_neighbors = None\n",
    "    for n_neighbors in range(1, 21):\n",
    "        knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "        knn.fit(X_train_scaled, y_train)\n",
    "        accuracy = knn.score(X_val_scaled, y_val)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_n_neighbors = n_neighbors\n",
    "\n",
    "    # n_neighbors is set to 10 as an example, but you should tune this parameter\n",
    "    knn = KNeighborsClassifier(n_neighbors=best_n_neighbors)\n",
    "\n",
    "    # Train the model\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    classification_report_results = classification_report(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Best n_estimators: {best_n_neighbors}\")\n",
    "    print(\"Accuracy of KNN model:\", accuracy)\n",
    "    print(\"\\nClassification Report:\\n\", classification_report_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "### Data Splitting Function\n",
    "The `split_data` function is designed for preparing the dataset for model training and evaluation. It performs the following tasks:\n",
    "\n",
    "1. **Data Splitting**: Initially, it splits the provided dataset into two parts: a combined training and validation set, and a separate test set. The test set size is determined by the `test_size` parameter, allowing for a portion of the data to be reserved for final evaluation.\n",
    "2. **Further Splitting**: Subsequently, it further splits the combined training and validation set into distinct training and validation sets. This allows for the evaluation of model performance and hyperparameter tuning before the final test set evaluation.\n",
    "\n",
    "### Logistic Regression Model Implementation\n",
    "The `implement_logistic_regression` function handles the logistic regression model. It includes:\n",
    "\n",
    "1. **Hyperparameter Tuning**: It iterates over different values of the regularization strength (`C`) to find the best setting based on validation set accuracy.\n",
    "2. **Model Training and Evaluation**: After identifying the best hyperparameter value, it retrains the model using the entire training set and evaluates its performance on the unseen test set.\n",
    "\n",
    "### Random Forest Model Implementation\n",
    "The `implement_random_forest` function focuses on the Random Forest model. Similar to logistic regression, it involves:\n",
    "\n",
    "1. **Hyperparameter Tuning**: This process finds the optimal number of trees (`n_estimators`) in the forest by evaluating the model's performance on the validation set.\n",
    "2. **Final Model Training and Evaluation**: With the best `n_estimators` value, the model is retrained and then evaluated on the test set to report its accuracy and classification performance.\n",
    "\n",
    "### Deep Neural Network (DNN) Implementation\n",
    "The `implement_DNN` function encapsulates the process of implementing, tuning, and evaluating a Deep Neural Network model:\n",
    "\n",
    "1. **Feature Scaling**: It scales the features using standardization, which is crucial for neural network performance.\n",
    "2. **Grid Search for Hyperparameter Tuning**: Utilizes `GridSearchCV` to automate the search for the best network architecture (`hidden_layer_sizes`) and activation function over the training data.\n",
    "3. **Model Evaluation**: After identifying the best hyperparameters, it evaluates the model on the test set and reports performance metrics.\n",
    "\n",
    "### K-Nearest Neighbors (KNN) Implementation\n",
    "The `implement_KNN` function addresses the KNN model implementation with:\n",
    "\n",
    "1. **Feature Scaling**: Similar to DNN, it scales the input features to improve model performance.\n",
    "2. **Hyperparameter Tuning**: Searches for the best `n_neighbors` value by evaluating different settings on the validation set.\n",
    "3. **Final Model Training and Evaluation**: With the optimal `n_neighbors` value identified, it trains the KNN model and assesses its accuracy and classification performance on the test set.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(X_resampled, y_resampled)\n",
    "\n",
    "# imbalanced data to check the accuracy of model using 0.05 percentage of data\n",
    "df_to_model_frac = df_to_model.sample(frac=0.05, random_state=42)\n",
    "X2 = df_to_model_frac.drop(['SEVERITY_OF_INJURIES', 'INJURIES_TOTAL', 'INJURIES_INCAPACITATING', 'MOST_SEVERE_INJURY', \n",
    "                      'INJURY_ClASS', 'SEVERITY_CLASS', 'INJURIES_NON_INCAPACITATING', 'INJURIES_NO_INDICATION',\n",
    "                      'INJURIES_REPORTED_NOT_EVIDENT', 'INJURIES_FATAL'], axis=1)\n",
    "y2 = df_to_model_frac['SEVERITY_CLASS']\n",
    "X2 = pd.get_dummies(X2)\n",
    "\n",
    "X_train_frac, X_val_frac, X_test_frac, y_train_frac, y_val_frac, y_test_frac = split_data(X2, y2)\n",
    "\n",
    "# Logistic Regression for imabalanced class\n",
    "rf_imbalanced_classifier = implement_random_forest(X_train_frac, X_val_frac, X_test_frac, y_train_frac, y_val_frac, y_test_frac)\n",
    "\n",
    "# Random Forest\n",
    "rf_classifier = implement_random_forest(X_train, X_val, X_test, y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "\n",
    "\n",
    "This code demonstrates the impact of handling imbalanced datasets in machine learning model performance, specifically using Random Forest classifiers:\n",
    "\n",
    "1. **Data Splitting**: Initially, the dataset is split into training, validation, and test sets twice—once for a balanced dataset (`X_resampled`, `y_resampled`) created through undersampling and once for a fraction (5%) of the original, imbalanced dataset (`df_to_model_frac`).\n",
    "\n",
    "2. **Model Implementation on Imbalanced Data**: A Random Forest model (`rf_imbalanced_classifier`) is trained and evaluated on the small, imbalanced fraction of the dataset. Despite achieving a high accuracy of 98.36%, the model fails to correctly predict any `HIGH SEVERITY` incidents, highlighting the issue with imbalanced data where the model is biased towards the majority class (`LOW SEVERITY`).\n",
    "\n",
    "3. **Model Implementation on Balanced Data**: Another Random Forest model (`rf_classifier`) is then trained on the balanced dataset. This model shows a more balanced performance with an accuracy of 86.72% and is capable of identifying `HIGH SEVERITY` cases with high precision and recall, demonstrating the effectiveness of balancing the dataset through undersampling.\n",
    "\n",
    "The stark difference in performance between the two models underscores the necessity of addressing class imbalance in datasets. While the imbalanced model might appear highly accurate at first glance, its inability to detect `HIGH SEVERITY` incidents renders it practically ineffective for scenarios where identifying such cases is crucial. Conversely, the model trained on balanced data shows significantly improved and more reliable performance across both classes, illustrating the importance of using methods like undersampling to enhance model fairness and effectiveness.\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Features for Random Forest Classifier\n",
    "based on RF classifier I check the important features for detecting the severity of injuries in the accident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_important_feature(rf_classifier):\n",
    "    importances = rf_classifier.feature_importances_\n",
    "    feature_names = X.columns\n",
    "    feature_importances = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "    feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    return feature_importances.head(20)\n",
    "rf_important_feature(rf_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "log_reg = implement_logistic_regression(X_train, X_val, X_test, y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "1. **Implementation of Logistic Regression**: The `implement_logistic_regression` function is called with the training, validation, and test datasets. This function internally:\n",
    "   - Performs hyperparameter tuning to find the optimal regularization strength (`C`) that leads to the best accuracy on the validation set. This is crucial for preventing overfitting while maintaining the model's ability to generalize well to new data.\n",
    "   - Retrains the logistic regression model using the optimal `C` value found through tuning on the entire training dataset (including the validation data) to fully leverage all available data for training.\n",
    "   - Evaluates the final model's performance on the test dataset to assess its generalization capability.\n",
    "\n",
    "2. **Model Performance Evaluation**: The output provides detailed performance metrics for the logistic regression model:\n",
    "   - The `Best C` indicates the optimal regularization strength discovered during hyperparameter tuning, in this case, `C=2`.\n",
    "   - The `Accuracy of linear regressor` shows the overall percentage of correct predictions made by the model on the test set, which is approximately 86.43%. This indicates a high level of accuracy in the model's ability to classify the severity of crashes.\n",
    "   - The `Classification Report` provides a breakdown of precision, recall, and F1-score for each class (`HIGH SEVERITY` and `LOW SEVERITY`). Notably, the model has high recall for `HIGH SEVERITY` cases, indicating it is particularly effective at identifying most of the high severity incidents. However, the precision for `LOW SEVERITY` is higher, suggesting the model is more reliable when it predicts an incident is of low severity.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg_important_features(log_reg):\n",
    "    # Get the feature names\n",
    "    feature_names = X_resampled.columns\n",
    "\n",
    "    # Get the coefficients from the logistic regression model\n",
    "    coefficients = log_reg.coef_[0]  # Assuming binary classification, hence [0]\n",
    "\n",
    "    # Create a series to map feature names to their coefficients\n",
    "    feature_importance = pd.Series(coefficients, index=feature_names)\n",
    "\n",
    "    # Sort the features by their absolute values to see the most significant ones\n",
    "    feature_importance_sorted = feature_importance.abs().sort_values(ascending=False)\n",
    "\n",
    "    return feature_importance_sorted.head(20)\n",
    "\n",
    "log_reg_important_features(log_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "1. **Feature Coefficients Extraction**: It starts by obtaining the logistic regression model's coefficients, which quantify the impact of each feature on the model's prediction. A positive coefficient indicates a feature that increases the likelihood of the target variable being 1, while a negative coefficient suggests a feature that decreases this likelihood.\n",
    "\n",
    "2. **Mapping Features to Coefficients**: The function maps these coefficients to the corresponding feature names from the `X_resampled` dataframe. This mapping is crucial for understanding which features are most influential in the model's decisions.\n",
    "\n",
    "3. **Sorting by Importance**: The coefficients are then sorted by their absolute values in descending order. This sorting helps identify the features with the most significant impact on the model's predictions, regardless of the direction of their effect.\n",
    "\n",
    "4. **Top 20 Features**: Finally, it returns the top 20 features with the highest absolute coefficient values, highlighting those with the greatest influence on the logistic regression model's outcomes.\n",
    "\n",
    "The output reveals the features with the highest importance scores, notably including various `CRASH_TYPE` and `FIRST_CRASH_TYPE` categories, indicating the specific nature of a crash plays a crucial role in predicting its severity. Features like `PRIM_CONTRIBUTORY_CAUSE` related to the primary cause of the crash, and `NUM_UNITS` indicating the number of units involved in the crash, also appear as significant predictors. \n",
    "\n",
    "This insight into feature importance is invaluable for understanding the factors that most influence crash severity predictions, providing guidance for interventions and further research aimed at reducing severe traffic incidents.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.6. Modeling with DNN\n",
    "implement_DNN(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "1. **DNN Implementation**: The `implement_DNN` function is called with the training and test datasets. This function encapsulates the entire process of configuring, training, and evaluating a DNN, including:\n",
    "   - **Hyperparameter Tuning**: Utilizing a grid search to explore different combinations of `activation` functions and `hidden_layer_sizes` configurations. This systematic approach aims to identify the best set of parameters that result in the highest accuracy on the training data.\n",
    "   - **Model Training**: With the optimal parameters identified (`{'activation': 'relu', 'hidden_layer_sizes': (10, 20)}`), the DNN is trained on the training dataset. The `relu` activation function and a two-layer neural network with 10 and 20 neurons, respectively, were found to be the best configuration.\n",
    "   - **Evaluation on Test Data**: The trained DNN model is then evaluated on the unseen test dataset to assess its generalization capability.\n",
    "\n",
    "2. **Model Performance**: The output reports the DNN model's performance metrics:\n",
    "   - **Best Parameters**: Indicates the optimal combination of hyperparameters found through grid search.\n",
    "   - **Accuracy**: The overall accuracy of the DNN model on the test set is approximately 84.45%, demonstrating a strong capability to correctly classify the severity of crashes.\n",
    "   - **Classification Report**: Provides detailed performance metrics, including precision, recall, and F1-score for each severity class (`HIGH SEVERITY` and `LOW SEVERITY`). The model shows higher precision in predicting `LOW SEVERITY` crashes but exhibits a better recall for `HIGH SEVERITY` crashes, indicating it is more effective at identifying a larger proportion of high severity incidents, even though it's slightly more cautious in labeling crashes as low severity.\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.7. Modeling with KNN\n",
    "implement_KNN(X_train, X_val, X_test, y_train, y_val, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "Across the series of code blocks, we implemented and evaluated several machine learning models—Logistic Regression, Random Forest, Deep Neural Network (DNN), and K-Nearest Neighbors (KNN)—to predict crash severity from a dataset. Here's a summary of the key findings and model performances:\n",
    "\n",
    "1. **Random Forest** emerged as a robust model with the best performance, achieving an accuracy of approximately 86.72%. It demonstrated high precision in predicting both `HIGH SEVERITY` and `LOW SEVERITY` crashes.\n",
    "\n",
    "2. **Logistic Regression** followed closely, with an optimal regularization strength `C=2` and an accuracy of around 86.43%. It showed excellent ability to identify `HIGH SEVERITY` incidents.\n",
    "\n",
    "3. **DNN** found an optimal configuration with `activation='relu'` and `hidden_layer_sizes=(10, 20)`, reaching an accuracy of 84.45%. The model balanced precision and recall reasonably well across severity classes.\n",
    "\n",
    "4. **KNN**, with the best `n_neighbors=12`, had a lower accuracy of about 79.79% compared to the other models. It performed better in recall for `HIGH SEVERITY` but had lower precision for `LOW SEVERITY`.\n",
    "\n",
    "Additionally, feature importance analyses for both the Random Forest and Logistic Regression models highlighted the significance of `CRASH_TYPE` and `FIRST_CRASH_TYPE` variables, underscoring the impact of crash characteristics on severity predictions.\n",
    "\n",
    "Overall, each model provided valuable insights into the factors influencing crash severity, with Random Forest and Logistic Regression standing out in terms of accuracy and interpretability. These findings could guide future efforts to improve road safety and prevent severe crashes.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 6. CRISP-DM: Deployment\n",
    "\n",
    "\n",
    "## Deployment Strategy\n",
    "We plan to make the model accessible via an API, hosted on a scalable cloud platform for reliability and flexibility. Continuous monitoring will ensure the model remains accurate and performs well over time.\n",
    "\n",
    "## Integration Points\n",
    "The model will seamlessly integrate into existing systems or applications, with a user interface potentially enhanced by interactive maps like Google Maps. This feature will provide end-users with intuitive, visual interpretations of the model's predictions, enriching the user experience.\n",
    "\n",
    "## Maintenance Plan\n",
    "To maintain model efficacy, we'll implement a schedule for regular updates and retraining with new data. Performance tuning, based on ongoing feedback and evolving data trends, will further refine the model.\n",
    "\n",
    "## Security Measures\n",
    "Ensuring data privacy and compliance with regulations is paramount. We'll implement robust access control measures to define clear permissions for interacting with the model, safeguarding user data.\n",
    "\n",
    "## Success Metrics\n",
    "To evaluate the deployment's success, we will track usage metrics to understand adoption rates and the frequency of use. An impact assessment will measure the tangible benefits and outcomes of the model's predictions in the real world.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Appendix - Extra Functionality & Bonus\n",
    "\n",
    "7.1. Minimal vs. Expected vs. Bonus Functionality\n",
    "7.2. Insights and Beyond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Minimal vs. Expected vs. Bonus Functionality\n",
    "\n",
    "In this section we will detail what we have committed at the beginning of the project.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Minimal Functionality**\n",
    "* Data Collection and Preparation: Basic description and/or cleaning.\n",
    "* Data Analysis: Basic analysis.\n",
    "* Modeling: At least 1 algorith (e.g. Logistic Regression) + Evaluation + Resulst and Discussion.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Expected Functionality**\n",
    "* Data Collection and Preparation: Full description and cleaning.\n",
    "* Data Analysis: Full analysis.\n",
    "* Modeling: 2-3 models (LR, RF and KNN) + Evaluation + Resulst and Discussion.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Bonus Functionality**\n",
    "* Data Collection and Preparation: Advanced. We applied every technique learned during the course.\n",
    "* Data Analysis: Advanced analysis + Insights (check details below).\n",
    "* Modeling: We used 4 algorithms (LR, RF, KNN and DNN).\n",
    "* Imbalanced Data & Sampling: For each model, we comparing results with sampling and without.\n",
    "* Business Understanding: Domain Concepts and Full features details. It might be high investment, but was great to understand the data.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Additional Comments**\n",
    "1. Our code highly organized to functions and modern programing paradigms.\n",
    "2. Well documented.\n",
    "3. Analysis related part with \"cyan\" color to be more user-friendly to read.\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## 7.2. Insights and Beyond\n",
    "\n",
    "In this section, we present different insights and patterns that can be extracted from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_insights = df_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for injury and fatality-related features\n",
    "df_for_insights[['INJURIES_TOTAL', 'INJURIES_FATAL', 'INJURIES_INCAPACITATING', 'INJURIES_NON_INCAPACITATING', 'INJURIES_REPORTED_NOT_EVIDENT', 'INJURIES_NO_INDICATION']].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each severity level\n",
    "print(df_for_insights['INJURIES_FATAL'].value_counts())\n",
    "print(df_for_insights['INJURIES_INCAPACITATING'].value_counts())\n",
    "print(df_for_insights['INJURIES_NON_INCAPACITATING'].value_counts())\n",
    "print(df_for_insights['INJURIES_REPORTED_NOT_EVIDENT'].value_counts())\n",
    "print(df_for_insights['INJURIES_NO_INDICATION'].value_counts())\n",
    "\n",
    "# To get idea about the data ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calculate_ratio_between_two_classes(my_df, feature_1, feature_2):\n",
    "    # Calculate the total sum for each injury category\n",
    "    total_feature_1 = my_df[feature_1].sum()\n",
    "    total_feature_2 = my_df[feature_2].sum()\n",
    "    # Calculate the ratio of incapacitating to fatal injuries\n",
    "    ratio_feature_2_to_feature_1 = total_feature_2 / total_feature_1\n",
    "    print(f\"For every 1 {feature_1}, there are on average {ratio_feature_2_to_feature_1:.2f} {feature_2}\")\n",
    "\n",
    "\n",
    "print(\"Comparing Each Level to its next one:\")\n",
    "calculate_ratio_between_two_classes(df_for_insights, 'INJURIES_FATAL', 'INJURIES_INCAPACITATING')\n",
    "calculate_ratio_between_two_classes(df_for_insights, 'INJURIES_INCAPACITATING', 'INJURIES_NON_INCAPACITATING')\n",
    "calculate_ratio_between_two_classes(df_for_insights, 'INJURIES_NON_INCAPACITATING', 'INJURIES_REPORTED_NOT_EVIDENT')\n",
    "calculate_ratio_between_two_classes(df_for_insights, 'INJURIES_REPORTED_NOT_EVIDENT', 'INJURIES_NO_INDICATION')\n",
    "print()\n",
    "print(\"Comparing Each Level the base (lowest) level\")\n",
    "calculate_ratio_between_two_classes(df_for_insights, 'INJURIES_FATAL', 'INJURIES_NO_INDICATION')\n",
    "calculate_ratio_between_two_classes(df_for_insights, 'INJURIES_INCAPACITATING', 'INJURIES_NO_INDICATION')\n",
    "calculate_ratio_between_two_classes(df_for_insights, 'INJURIES_NON_INCAPACITATING', 'INJURIES_NO_INDICATION')\n",
    "calculate_ratio_between_two_classes(df_for_insights, 'INJURIES_REPORTED_NOT_EVIDENT', 'INJURIES_NO_INDICATION')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio of Non-Zero Injury Instance - Pie Chart\n",
    "# Calculate the number of instances greater than zero for each injury type\n",
    "injury_types = [\n",
    "    'INJURIES_FATAL', \n",
    "    'INJURIES_INCAPACITATING', \n",
    "    'INJURIES_NON_INCAPACITATING', \n",
    "    'INJURIES_REPORTED_NOT_EVIDENT', \n",
    "    'INJURIES_NO_INDICATION'\n",
    "]\n",
    "injury_counts = {injury: (df_for_insights[injury] > 0).sum() for injury in injury_types}\n",
    "\n",
    "# Create labels and sizes for the pie chart\n",
    "labels = injury_counts.keys()\n",
    "sizes = injury_counts.values()\n",
    "colors = plt.cm.tab20c.colors  # Using a color map that has distinct colors for each category\n",
    "\n",
    "# Plotting the pie chart\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie chart is drawn as a circle\n",
    "plt.title('Ratio of Non-Zero Injury Instances')\n",
    "\n",
    "# Show the pie chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 10))  # Adjust grid layout as needed\n",
    "features = ['INJURIES_FATAL', 'INJURIES_INCAPACITATING', 'INJURIES_NON_INCAPACITATING', 'INJURIES_REPORTED_NOT_EVIDENT']\n",
    "titles = ['Fatal Injuries', 'Incapacitating Injuries', 'Non-Incapacitating Injuries', 'Reported Not Evident Injuries']\n",
    "\n",
    "for ax, feature, title in zip(axes.flatten(), features, titles):\n",
    "    ax.hist(df_for_insights[feature], bins=range(int(df_for_insights[feature].max() + 1)), color='skyblue', edgecolor='black')\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.set_xlabel('Number of ' + title, fontsize=14)\n",
    "    ax.set_ylabel('Frequency', fontsize=14)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax.grid(axis='y', alpha=0.75)\n",
    "\n",
    "plt.tight_layout()  # Adjust subplots to fit into the figure area.\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing injuries by month\n",
    "\n",
    "# df_for_insights['CRASH_MONTH'] = pd.to_datetime(df_for_insights['CRASH_DATE']).dt.month\n",
    "monthly_injuries = df.groupby('CRASH_MONTH')['INJURIES_TOTAL'].sum()\n",
    "plt.plot(monthly_injuries.index, monthly_injuries.values)\n",
    "plt.title('Total Injuries by Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Injuries')\n",
    "plt.xticks(range(1, 13))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Injuries by Month (2018 - 2023)\n",
    "\n",
    "# Filter data to exclude years 2015 and 2016, and include up to 2023\n",
    "filtered_df = df_for_insights[(df_for_insights['YEAR'] > 2017) & (df_for_insights['YEAR'] <= 2023)]\n",
    "\n",
    "# Group by year and month, summing total injuries\n",
    "injuries_by_month_year = filtered_df.groupby(['YEAR', 'CRASH_MONTH'])['INJURIES_TOTAL'].sum().reset_index()\n",
    "\n",
    "# Create a line plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "line = sns.lineplot(data=injuries_by_month_year, x='CRASH_MONTH', y='INJURIES_TOTAL', hue='YEAR', palette='Spectral', marker='o', linewidth=2.5)\n",
    "\n",
    "# Enhance the plot\n",
    "plt.title('Total Injuries by Month (2018-2023)', fontsize=16)\n",
    "plt.xlabel('Month', fontsize=14)\n",
    "plt.ylabel('Total Injuries', fontsize=14)\n",
    "plt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(title='Year', title_fontsize='13', fontsize='12', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.tight_layout()  # Adjust layout to make room for the legend\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fatal Traffic Accidents by Day of the Week\n",
    "\n",
    "fatalities_per_day = df_for_insights.groupby('CRASH_DAY_OF_WEEK')['INJURIES_FATAL'].sum()\n",
    "\n",
    "# Days of the week labels\n",
    "days_of_the_week = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "\n",
    "# Creating the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(days_of_the_week, fatalities_per_day, color='tomato')\n",
    "plt.title('Fatal Traffic Accidents by Day of the Week', fontsize=16)\n",
    "plt.xlabel('Day of the Week', fontsize=14)\n",
    "plt.ylabel('Number of Fatalities', fontsize=14)\n",
    "plt.xticks(fontsize=12, rotation=45)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fatal Traffic Accidents by Hour of the Day\n",
    "\n",
    "# Grouping the data by 'CRASH_HOUR' and summing 'INJURIES_FATAL' to get total fatalities per hour\n",
    "fatalities_per_hour = df_for_insights.groupby('CRASH_HOUR')['INJURIES_FATAL'].sum()\n",
    "\n",
    "# Creating the bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(fatalities_per_hour.index, fatalities_per_hour.values, color='royalblue', edgecolor='black')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.title('Fatal Traffic Accidents by Hour of the Day', fontsize=16)\n",
    "plt.xlabel('Hour of the Day (0-23)', fontsize=14)\n",
    "plt.ylabel('Number of Fatalities', fontsize=14)\n",
    "\n",
    "# Ensuring the x-axis covers all hours\n",
    "plt.xticks(range(0, 24), fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fatal and Incapacitating Injuries by Hour of the Day\n",
    "\n",
    "\n",
    "grouped_data = df_for_insights.groupby('CRASH_HOUR')[['INJURIES_FATAL', 'INJURIES_INCAPACITATING']].sum().reset_index()\n",
    "\n",
    "# Setting the positions and width for the bars\n",
    "pos = list(range(len(grouped_data['CRASH_HOUR'])))\n",
    "width = 0.4\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "plt.bar(pos, \n",
    "        grouped_data['INJURIES_FATAL'], \n",
    "        width, \n",
    "        alpha=0.7, \n",
    "        color='red', \n",
    "        label='Fatal Injuries')\n",
    "\n",
    "plt.bar([p + width for p in pos], \n",
    "        grouped_data['INJURIES_INCAPACITATING'],\n",
    "        width, \n",
    "        alpha=0.7, \n",
    "        color='blue', \n",
    "        label='Incapacitating Injuries')\n",
    "\n",
    "# Set the y-axis label\n",
    "ax.set_ylabel('Number of Injuries')\n",
    "\n",
    "# Set the chart's title\n",
    "ax.set_title('Fatal and Incapacitating Injuries by Hour of the Day')\n",
    "\n",
    "# Set the position of the x ticks\n",
    "ax.set_xticks([p + width / 2 for p in pos])\n",
    "\n",
    "# Set the labels for the x ticks\n",
    "ax.set_xticklabels(grouped_data['CRASH_HOUR'])\n",
    "\n",
    "# Setting the x-axis and y-axis limits\n",
    "plt.xlim(min(pos)-width, max(pos)+width*2)\n",
    "plt.ylim([0, max(grouped_data['INJURIES_FATAL'].max(), grouped_data['INJURIES_INCAPACITATING'].max())] )\n",
    "\n",
    "# Adding the legend and showing the plot\n",
    "plt.legend(['Fatal Injuries', 'Incapacitating Injuries'], loc='upper left')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug Only - Helper functions to understand the data better\n",
    "\n",
    "\n",
    "# dump_feature_frequency_to_a_file(df_to_model, 'PRIM_CONTRIBUTORY_CAUSE')\n",
    "\n",
    "# dump_feature_classes_to_a_file(df_to_model, 'PRIM_CONTRIBUTORY_CAUSE')\n",
    "# dump_feature_classes_to_a_file(df_to_model, 'SEC_CONTRIBUTORY_CAUSE')\n",
    "\n",
    "\n",
    "# dump_feature_classes_to_a_file(df_to_model, 'TRAFFIC_CONTROL_DEVICE')\n",
    "# dump_feature_classes_to_a_file(df_to_model, 'WEATHER_CONDITION')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311_pods_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
