{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">Predict Car Traffic Injury</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this document we follow the CRISP-DM process and methodolgies to develop our project.\n",
    "Full details are described in the report document of the project.\n",
    "\n",
    "In the file, we will do:\n",
    "* Coding + Analysis for results.\n",
    "* Code insights.\n",
    "* Apply the practices learned for the business part, including understanding domain concepts and details about each feature.\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to read this document**\n",
    "\n",
    "1. Text in black (or White, depends on reader background) - Describes section or sub-section title and details.\n",
    "\n",
    "<span style=\"color:cyan\">\n",
    "\n",
    "2. Text in \"cyan\" is analysis realted to previous code section.\n",
    "</span>\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CRISP-DM: Business Understanding\n",
    "\n",
    "1.1. Background and Objective<br>\n",
    "1.2. Descriptive Features and Domain Concepts<br>\n",
    "1.3. Domain Concepts Explained<br>\n",
    "1.4. Relevant Resources<br>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<span style=\"color:cyan\">\n",
    "\n",
    "# 1.1. Background and Objective<br>\n",
    "Traffic Accidents globally rank high in causing deaths and injuries. Traffic accidents have significant health care and economic impact.<br>\n",
    "According to World Health Organization traffic accidents is one of the leading death causes worldwide.<br>\n",
    "<br>\n",
    "In this project, we are utilizing advanced machine learning models and data analytics to forecast accident severity for strategic planning and response, aiming to reduce traffic-related injuries, fatalities, and economic costs with targeted preventive measures and efficient resource allocation.<br>\n",
    "<br>\n",
    "We addressing the gap in predictive knowledge of traffic accident severity factors in Chicago's urban landscape to enhancing public health and safety by identifying risk patterns and contributing factors through historical data analysis.<br>\n",
    "\n",
    "</span>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:cyan\">\n",
    "\n",
    "## 1.2. Descriptive Features and Domain Concepts\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "| Column Name                    | Type           | Short Description                                                                                                                         | Domain Concept           |\n",
    "|--------------------------------|----------------|-------------------------------------------------------------------------------------------------------------------------------------------|-------------------------|\n",
    "| CRASH_RECORD_ID                | Text           | Unique identifier for the crash record, linking to the same crash in the Vehicles and People datasets.                                    | Identification          |\n",
    "| CRASH_DATE_EST_I               | Text           | Indicates if the crash date was estimated by a desk officer or reporting party.                                                           | Crash Details           |\n",
    "| CRASH_DATE                     | DateTime       | Date and time of the crash as entered by the reporting officer.                                                                           | Crash Details           |\n",
    "| POSTED_SPEED_LIMIT             | Number         | Posted speed limit as determined by the reporting officer.                                                                                | Environmental Factors   |\n",
    "| TRAFFIC_CONTROL_DEVICE         | Text           | Traffic control device present at the crash location as determined by the reporting officer.                                              | Environmental Factors   |\n",
    "| DEVICE_CONDITION               | Text           | Condition of the traffic control device as determined by the reporting officer.                                                           | Environmental Factors   |\n",
    "| WEATHER_CONDITION              | Text           | Weather condition at the time of the crash as determined by the reporting officer.                                                        | Environmental Factors   |\n",
    "| LIGHTING_CONDITION             | Text           | Lighting condition at the time of the crash as determined by the reporting officer.                                                       | Environmental Factors   |\n",
    "| FIRST_CRASH_TYPE               | Text           | Type of first collision in the crash.                                                                                                     | Crash Details           |\n",
    "| TRAFFICWAY_TYPE                | Text           | Trafficway type as determined by the reporting officer.                                                                                   | Environmental Factors   |\n",
    "| LANE_CNT                       | Number         | Total number of through lanes in either direction, excluding turn lanes, at the crash location as determined by the reporting officer.     | Environmental Factors   |\n",
    "| ALIGNMENT                      | Text           | Street alignment at the crash location as determined by the reporting officer.                                                            | Environmental Factors   |\n",
    "| ROADWAY_SURFACE_COND           | Text           | Road surface condition at the crash location as determined by the reporting officer.                                                      | Environmental Factors   |\n",
    "| ROAD_DEFECT                    | Text           | Road defects present at the crash location as determined by the reporting officer.                                                        | Environmental Factors   |\n",
    "| REPORT_TYPE                    | Text           | Administrative report type of the crash.                                                                                                   | Documentation          |\n",
    "| CRASH_TYPE                     | Text           | General severity classification for the crash.                                                                                             | Crash Details           |\n",
    "| INTERSECTION_RELATED_I         | Text           | Indicates if an intersection played a role in the crash as observed by the police officer.                                                | Environmental Factors   |\n",
    "| NOT_RIGHT_OF_WAY_I             | Text           | Indicates if the crash began or first contact was made outside of the public right-of-way.                                                | Legal                   |\n",
    "| HIT_AND_RUN_I                  | Text           | Indicates if the crash involved a driver who fled the scene without exchanging information and/or rendering aid.                          | Legal                   |\n",
    "| DAMAGE                         | Text           | Estimated damage from the crash as observed in the field.                                                                                 | Crash Outcome           |\n",
    "| DATE_POLICE_NOTIFIED           | DateTime       | Calendar date on which the police were notified of the crash.                                                                             | Documentation          |\n",
    "| PRIM_CONTRIBUTORY_CAUSE        | Text           | Primary factor contributing to the cause of the crash as determined by officer judgment.                                                  | Crash Analysis         |\n",
    "| SEC_CONTRIBUTORY_CAUSE         | Text           | Secondary factor contributing to the cause of the crash as determined by officer judgment.                                                | Crash Analysis         |\n",
    "| STREET_NO                      | Number         | Street address number of the crash location as determined by the reporting officer.                                                       | Location Details       |\n",
    "| STREET_DIRECTION               | Text           | Street address direction (N, E, S, W) of the crash location as determined by the reporting officer.                                       | Location Details       |\n",
    "| STREET_NAME                    | Text           | Street name of the crash location as determined by the reporting officer.                                                                 | Location Details       |\n",
    "| BEAT_OF_OCCURRENCE             | Number         | Chicago Police Department beat ID where the crash occurred.                                                                               | Administrative         |\n",
    "| PHOTOS_TAKEN_I                 | Text           | Indicates if photos were taken at the crash location by the Chicago Police Department.                                                    | Documentation          |\n",
    "| STATEMENTS_TAKEN_I             | Text           | Indicates if statements were taken from units involved in the crash.                                                                      | Documentation          |\n",
    "| DOORING_I                      | Text           | Indicates if the crash involved a vehicle occupant opening a door into the path of a bicyclist.                                           | Crash Type             |\n",
    "| WORK_ZONE_I                    | Text           | Indicates if the crash occurred in an active work zone.                                                                                   | Environmental Factors   |\n",
    "| WORK_ZONE_TYPE                 | Text           | Type of work zone, if any, where the crash occurred.                                                                                      | Environmental Factors   |\n",
    "| WORKERS_PRESENT_I              | Text           | Indicates if construction workers were present in the work zone at the crash location.                                                    | Environmental Factors   |\n",
    "| NUM_UNITS                      | Number         | Number of units involved in the crash, representing different modes of traffic with independent trajectories.                             | Crash Details           |\n",
    "| MOST_SEVERE_INJURY             | Text           | Most severe injury sustained by any person involved in the crash.                                                                         | Injury Analysis        |\n",
    "| INJURIES_TOTAL                 | Number         | Total number of people sustaining fatal, incapacitating, non-incapacitating, and possible injuries as determined by the reporting officer. | Injury Analysis        |\n",
    "| INJURIES_FATAL                 | Number         | Total number of people sustaining fatal injuries in the crash.                                                                            | Injury Analysis        |\n",
    "| INJURIES_INCAPACITATING        | Number         | Total number of people sustaining incapacitating/serious injuries in the crash.                                                           | Injury Analysis        |\n",
    "| INJURIES_NON_INCAPACITATING    | Number         | Total number of people sustaining non-incapacitating injuries in the crash.                                                               | Injury Analysis        |\n",
    "| INJURIES_REPORTED_NOT_EVIDENT  | Number         | Total number of people sustaining possible injuries in the crash as determined by the reporting officer.                                  | Injury Analysis        |\n",
    "| INJURIES_NO_INDICATION         | Number         | Total number of people with no injuries in the crash as determined by the reporting officer.                                              | Injury Analysis        |\n",
    "| INJURIES_UNKNOWN               | Number         | Total number of people for whom the injury status, if any, is unknown.                                                                    | Injury Analysis        |\n",
    "| CRASH_HOUR                     | Number         | Hour of the day when the crash occurred, derived from CRASH_DATE.                                                                         | Time Details           |\n",
    "| CRASH_DAY_OF_WEEK              | Number         | Day of the week when the crash occurred, derived from CRASH_DATE. Sunday=1                                                                | Time Details           |\n",
    "| CRASH_MONTH                    | Number         | Month when the crash occurred, derived from CRASH_DATE.                                                                                   | Time Details           |\n",
    "| LATITUDE                       | Number         | Latitude of the crash location as determined by the reporting officer.                                                                    | Geographic Information |\n",
    "| LONGITUDE                      | Number         | Longitude of the crash location as determined by the reporting officer.                                                                   | Geographic Information |\n",
    "| LOCATION                       | Point          | Geographic location of the crash as determined by the reporting officer, allowing for mapping and geographic analysis.                    | Geographic Information |\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:cyan\">\n",
    "\n",
    "\n",
    "\n",
    "## 1.3. Domain Concepts Explained<br>\n",
    "\n",
    "\n",
    "##### **1. Identification**\n",
    "- **Description**: Features that uniquely identify the crash records or events, often used for linking data across different datasets.\n",
    "- **Example**: \n",
    "  - **CRASH_RECORD_ID**: Serves as a unique identifier for each crash event, enabling connections with related records in vehicles and people datasets.\n",
    "\n",
    "##### **2. Crash Details**\n",
    "- **Description**: Specific details about the crash event, including the type of crash, date, and time, providing basic information on how and when the crash occurred.\n",
    "- **Example**: \n",
    "  - **CRASH_DATE**: Provides the exact date and time the crash happened.\n",
    "  - **FIRST_CRASH_TYPE**: Describes the initial type of collision.\n",
    "\n",
    "##### **3. Environmental Factors**\n",
    "- **Description**: Conditions surrounding the crash, such as weather, lighting, and road conditions, which can influence the occurrence and severity of crashes.\n",
    "- **Example**: \n",
    "  - **WEATHER_CONDITION** and **LIGHTING_CONDITION**: Detail the environmental state at the time of the crash, affecting driving conditions and visibility.\n",
    "\n",
    "##### **4. Legal**\n",
    "- **Description**: Features related to legal aspects of the crash, including compliance with right-of-way rules and hit-and-run incidents.\n",
    "- **Example**: \n",
    "  - **HIT_AND_RUN_I**: Indicates whether a driver involved in the crash fled the scene without providing information or rendering aid.\n",
    "\n",
    "##### **5. Crash Outcome**\n",
    "- **Description**: Descriptions of the immediate consequences of the crash, primarily in terms of physical damage.\n",
    "- **Example**: \n",
    "  - **DAMAGE**: Estimates the monetary damage to property resulting from the crash.\n",
    "\n",
    "##### **6. Documentation**\n",
    "- **Description**: Administrative details about the crash reporting and documentation process, including whether photos or statements were taken.\n",
    "- **Example**: \n",
    "  - **DATE_POLICE_NOTIFIED**: Records when the police were officially notified about the crash.\n",
    "\n",
    "##### **7. Crash Analysis**\n",
    "- **Description**: Factors identified as contributing to the crash, including primary and secondary causes as determined by officer judgment.\n",
    "- **Example**: \n",
    "  - **PRIM_CONTRIBUTORY_CAUSE** and **SEC_CONTRIBUTORY_CAUSE**: Identify the main reasons behind the crash according to the reporting officer.\n",
    "\n",
    "##### **8. Location Details**\n",
    "- **Description**: Specifics about where the crash occurred, including street names and numbers, helping in pinpointing the exact crash location.\n",
    "- **Example**: \n",
    "  - **STREET_NAME**, **STREET_NO**, and **STREET_DIRECTION**: Provide a detailed description of the crash site.\n",
    "\n",
    "##### **9. Administrative**\n",
    "- **Description**: Information related to police and emergency response jurisdictions, such as police beats.\n",
    "- **Example**: \n",
    "  - **BEAT_OF_OCCURRENCE**: Refers to the Chicago Police Department beat where the crash took place.\n",
    "\n",
    "##### **10. Crash Type**\n",
    "- **Description**: Categories that describe specific scenarios or types of crashes, including incidents involving bicycles or work zones.\n",
    "- **Example**: \n",
    "  - **DOORING_I**: Specifies if the crash involved a vehicle door being opened in the path of a bicyclist.\n",
    "\n",
    "##### **11. Injury Analysis**\n",
    "- **Description**: Detailed records of injuries resulting from the crash, categorizing the severity and type of injuries sustained.\n",
    "- **Example**: \n",
    "  - **INJURIES_TOTAL**: Counts the total number of injuries.\n",
    "  - **INJURIES_FATAL**: Counts the number of fatal injuries.\n",
    "\n",
    "##### **12. Time Details**\n",
    "- **Description**: Temporal information about the crash, including the hour, day of the week, and month when the crash occurred.\n",
    "- **Example**: \n",
    "  - **CRASH_HOUR**, **CRASH_DAY_OF_WEEK**, and **CRASH_MONTH**: Provide insights into the timing patterns of crashes.\n",
    "\n",
    "##### **13. Geographic Information**\n",
    "- **Description**: Geospatial data related to the crash location, enabling mapping and location-based analysis.\n",
    "- **Example**: \n",
    "  - **LATITUDE** and **LONGITUDE**: Give precise coordinates of the crash site.\n",
    "  - **LOCATION**: Offers a mappable point of the incident.\n",
    "\n",
    "Each domain concept helps categorize the dataset's features for easier analysis, highlighting different factors and details that contribute to a comprehensive understanding of traffic crashes.\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "## 1.4. Relevant Resources\n",
    "\n",
    "1. Dataset acquired from USA DATA GOV<br>\n",
    "URL: https://catalog.data.gov/dataset/traffic-crashes-crashes\n",
    "\n",
    "<br>\n",
    "\n",
    "2. Full details about this section, please refer to the report document.\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 2. CRISP-DM: Data Understanding\n",
    "\n",
    "In this section, the following will be addressed:<br><br>\n",
    "**2.1. Collect Initial Data**<br>\n",
    "2.1.1. Import the relevant Python packages that are going to be used.<br>\n",
    "2.1.2. Acquire the data.<br>\n",
    "2.1.3. Record of the data acquisition process.<br>\n",
    "2.2.4. HTML Reprot.<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**2.2. Data Description**<br>\n",
    "2.2.1. Shape of the Dataset.<br>\n",
    "2.2.2. Head snippet.<br>\n",
    "2.2.3. Dataset info.<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**2.3. Explore the Data**<br>\n",
    "2.3.1. Basic Statistics - For numeric models, to calculate Mean, Median, mode, ... <br>\n",
    "2.3.2. Tabular Report - For Continuos and Categorical Features (Refer to HTML Reports please). <br>\n",
    "2.3.3. Correlations + Heat Map.<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**2.4. Verify Data Quality**<br>\n",
    "2.4.1. Completnesss - Missing Values Summary.<br>\n",
    "2.4.2. Irregular cardinality, (1, too high for categorical, too low for continuos)<br>\n",
    "2.4.3. Consistency - Handle outliers, out of range data or invalid formats (if any).<br>\n",
    "2.4.4. Uniqueness - Remove duplicated or irrelevant repetition in data.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 2.1. Collect Initial Data\n",
    "\n",
    "\n",
    "This a generic intial step, to start working on the data:<br>\n",
    "2.1.1. Import the relevant Python packages that are going to be used.<br>\n",
    "2.1.2. Acquire the data.<br>\n",
    "2.1.3. Record of the data acquisition process.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1.1. Import the relevant Python packages that are going to be used\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from ydata_profiling import ProfileReport\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1.2. Acquire the data\n",
    "\n",
    "# A generic utilies file with generic functionallity\n",
    "from misc.utilities import acquire_dataset\n",
    "\n",
    "# Download & load the dataset\n",
    "# If the file already downloaded, it will not be re-downloaded (to speed up work efficiency)\n",
    "df = acquire_dataset()\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "\n",
    "2.1.2. Acquire the data<br>\n",
    "\n",
    "The data acquired from: https://catalog.data.gov/dataset/traffic-crashes-crashes\n",
    "<br>\n",
    "The above website belongs to USA government free datasets advised by the course instructions.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1.3. Helper functions\n",
    "\n",
    "def get_continuous_feature_details(my_df, my_feature):\n",
    "    my_df[my_feature] = pd.to_numeric(my_df[my_feature], errors='coerce')\n",
    "    # Printing unique values\n",
    "    unique_values = my_df[my_feature].unique()\n",
    "    sorted_unique_values = np.sort(unique_values)\n",
    "    len_unique_values = len(unique_values)\n",
    "    unique_values_str = ', '.join(map(str, sorted_unique_values[:5])) + ', ..., ' + ', '.join(map(str, sorted_unique_values[-5:]))\n",
    "    max_value = my_df[my_feature].max()\n",
    "    min_value = my_df[my_feature].min()\n",
    "    # print(f\"{my_feature}: Unique Len={len_unique_values}, Unique={sorted_unique_values}, Max={max_value}, Min={min_value}\")\n",
    "    print(f\"{my_feature}:\\n  Unique Len={len_unique_values}\\n  Unique={unique_values_str}\\n  Max={max_value}\\n  Min={min_value}\")\n",
    "\n",
    "    return len_unique_values, sorted_unique_values, max_value, min_value\n",
    "    \n",
    "\n",
    "def dump_feature_frequency_to_a_file(my_df, my_feature):\n",
    "    value_counts = my_df[my_feature].value_counts()\n",
    "    # value_counts.to_csv(f\"{my_feature}_value_counts.csv\", index=True, header=['Frequency'])\n",
    "    # Open a file to write\n",
    "    with open(f\"{my_feature}_value_counts.txt\", 'w') as file:\n",
    "        # Write a header row\n",
    "        file.write(f\"{'Value':<20}{'Frequency':<10}\\n\")\n",
    "        file.write(f\"{'-'*20}{'-'*10}\\n\")\n",
    "        \n",
    "        # Iterate over the Series and write each value and its frequency\n",
    "        for value, count in value_counts.items():\n",
    "            file.write(f\"{value:<20}{count:<10}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "\n",
    "2.1.3. Helper functions<br>\n",
    "\n",
    "Helper functions used periodically.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 2.2. Data Description\n",
    "\n",
    "In this phase we investigate the following aspects:<br>\n",
    "2.2.1. Shape of the Dataset.<br>\n",
    "2.2.2. Head snippet.<br>\n",
    "2.2.3. Dataset info.<br>\n",
    "2.2.4. Numerical Feature Distribution.<br>\n",
    "2.2.5. Catagorical Feature Distribution.<br>\n",
    "2.2.6 HTML report<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Records (Instances)</td>\n",
       "      <td>814788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Features (Columns)</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Aspect  Number\n",
       "0  Records (Instances)  814788\n",
       "1   Features (Columns)      51"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.2.1. Shape of the Dataset.\n",
    "\n",
    "def get_dataset_shape(df):\n",
    "    # Create a new DataFrame to display the shape information\n",
    "    shape_df = pd.DataFrame({\n",
    "        'Aspect': ['Records (Instances)', 'Features (Columns)'],\n",
    "        'Number': [df.shape[0], df.shape[1]]\n",
    "    })\n",
    "    return shape_df\n",
    "\n",
    "# Display the DataFrame\n",
    "get_dataset_shape(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRASH_RECORD_ID</th>\n",
       "      <th>CRASH_DATE_EST_I</th>\n",
       "      <th>CRASH_DATE</th>\n",
       "      <th>POSTED_SPEED_LIMIT</th>\n",
       "      <th>TRAFFIC_CONTROL_DEVICE</th>\n",
       "      <th>DEVICE_CONDITION</th>\n",
       "      <th>WEATHER_CONDITION</th>\n",
       "      <th>LIGHTING_CONDITION</th>\n",
       "      <th>FIRST_CRASH_TYPE</th>\n",
       "      <th>TRAFFICWAY_TYPE</th>\n",
       "      <th>...</th>\n",
       "      <th>INJURIES_UNKNOWN</th>\n",
       "      <th>CRASH_HOUR</th>\n",
       "      <th>CRASH_DAY_OF_WEEK</th>\n",
       "      <th>CRASH_MONTH</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>HOLIDAY_NAME</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6c1659069e9c6285a650e70d6f9b574ed5f64c12888479...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-18 12:50:00</td>\n",
       "      <td>15</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>FUNCTIONING PROPERLY</td>\n",
       "      <td>CLEAR</td>\n",
       "      <td>DAYLIGHT</td>\n",
       "      <td>REAR END</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2023</td>\n",
       "      <td>Aug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5f54a59fcb087b12ae5b1acff96a3caf4f2d37e79f8db4...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-29 14:45:00</td>\n",
       "      <td>30</td>\n",
       "      <td>TRAFFIC SIGNAL</td>\n",
       "      <td>FUNCTIONING PROPERLY</td>\n",
       "      <td>CLEAR</td>\n",
       "      <td>DAYLIGHT</td>\n",
       "      <td>PARKED MOTOR VEHICLE</td>\n",
       "      <td>DIVIDED - W/MEDIAN (NOT RAISED)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>41.854120</td>\n",
       "      <td>-87.665902</td>\n",
       "      <td>POINT (-87.665902342962 41.854120262952)</td>\n",
       "      <td>None</td>\n",
       "      <td>2023</td>\n",
       "      <td>Jul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61fcb8c1eb522a6469b460e2134df3d15f82e81fd93e9c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-18 17:58:00</td>\n",
       "      <td>30</td>\n",
       "      <td>NO CONTROLS</td>\n",
       "      <td>NO CONTROLS</td>\n",
       "      <td>CLEAR</td>\n",
       "      <td>DAYLIGHT</td>\n",
       "      <td>PEDALCYCLIST</td>\n",
       "      <td>NOT DIVIDED</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>41.942976</td>\n",
       "      <td>-87.761883</td>\n",
       "      <td>POINT (-87.761883496974 41.942975745006)</td>\n",
       "      <td>None</td>\n",
       "      <td>2023</td>\n",
       "      <td>Aug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004cd14d0303a9163aad69a2d7f341b7da2a8572b2ab33...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-11-26 08:38:00</td>\n",
       "      <td>25</td>\n",
       "      <td>NO CONTROLS</td>\n",
       "      <td>NO CONTROLS</td>\n",
       "      <td>CLEAR</td>\n",
       "      <td>DAYLIGHT</td>\n",
       "      <td>PEDESTRIAN</td>\n",
       "      <td>ONE-WAY</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2019</td>\n",
       "      <td>Nov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a1d5f0ea90897745365a4cbb06cc60329a120d89753fac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-18 10:45:00</td>\n",
       "      <td>20</td>\n",
       "      <td>NO CONTROLS</td>\n",
       "      <td>NO CONTROLS</td>\n",
       "      <td>CLEAR</td>\n",
       "      <td>DAYLIGHT</td>\n",
       "      <td>FIXED OBJECT</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2023</td>\n",
       "      <td>Aug</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     CRASH_RECORD_ID CRASH_DATE_EST_I  \\\n",
       "0  6c1659069e9c6285a650e70d6f9b574ed5f64c12888479...              NaN   \n",
       "1  5f54a59fcb087b12ae5b1acff96a3caf4f2d37e79f8db4...              NaN   \n",
       "2  61fcb8c1eb522a6469b460e2134df3d15f82e81fd93e9c...              NaN   \n",
       "3  004cd14d0303a9163aad69a2d7f341b7da2a8572b2ab33...              NaN   \n",
       "4  a1d5f0ea90897745365a4cbb06cc60329a120d89753fac...              NaN   \n",
       "\n",
       "           CRASH_DATE  POSTED_SPEED_LIMIT TRAFFIC_CONTROL_DEVICE  \\\n",
       "0 2023-08-18 12:50:00                  15                  OTHER   \n",
       "1 2023-07-29 14:45:00                  30         TRAFFIC SIGNAL   \n",
       "2 2023-08-18 17:58:00                  30            NO CONTROLS   \n",
       "3 2019-11-26 08:38:00                  25            NO CONTROLS   \n",
       "4 2023-08-18 10:45:00                  20            NO CONTROLS   \n",
       "\n",
       "       DEVICE_CONDITION WEATHER_CONDITION LIGHTING_CONDITION  \\\n",
       "0  FUNCTIONING PROPERLY             CLEAR           DAYLIGHT   \n",
       "1  FUNCTIONING PROPERLY             CLEAR           DAYLIGHT   \n",
       "2           NO CONTROLS             CLEAR           DAYLIGHT   \n",
       "3           NO CONTROLS             CLEAR           DAYLIGHT   \n",
       "4           NO CONTROLS             CLEAR           DAYLIGHT   \n",
       "\n",
       "       FIRST_CRASH_TYPE                  TRAFFICWAY_TYPE  ...  \\\n",
       "0              REAR END                            OTHER  ...   \n",
       "1  PARKED MOTOR VEHICLE  DIVIDED - W/MEDIAN (NOT RAISED)  ...   \n",
       "2          PEDALCYCLIST                      NOT DIVIDED  ...   \n",
       "3            PEDESTRIAN                          ONE-WAY  ...   \n",
       "4          FIXED OBJECT                            OTHER  ...   \n",
       "\n",
       "   INJURIES_UNKNOWN CRASH_HOUR CRASH_DAY_OF_WEEK CRASH_MONTH   LATITUDE  \\\n",
       "0               0.0         12                 6           8        NaN   \n",
       "1               0.0         14                 7           7  41.854120   \n",
       "2               0.0         17                 6           8  41.942976   \n",
       "3               0.0          8                 3          11        NaN   \n",
       "4               0.0         10                 6           8        NaN   \n",
       "\n",
       "   LONGITUDE                                  LOCATION HOLIDAY_NAME  YEAR  \\\n",
       "0        NaN                                       NaN         None  2023   \n",
       "1 -87.665902  POINT (-87.665902342962 41.854120262952)         None  2023   \n",
       "2 -87.761883  POINT (-87.761883496974 41.942975745006)         None  2023   \n",
       "3        NaN                                       NaN         None  2019   \n",
       "4        NaN                                       NaN         None  2023   \n",
       "\n",
       "  MONTH  \n",
       "0   Aug  \n",
       "1   Jul  \n",
       "2   Aug  \n",
       "3   Nov  \n",
       "4   Aug  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.2.2. Head snippet\n",
    "\n",
    "def get_dataset_head(df):\n",
    "    return df.head()\n",
    "\n",
    "get_dataset_head(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 814788 entries, 0 to 814787\n",
      "Data columns (total 51 columns):\n",
      " #   Column                         Non-Null Count   Dtype         \n",
      "---  ------                         --------------   -----         \n",
      " 0   CRASH_RECORD_ID                814788 non-null  object        \n",
      " 1   CRASH_DATE_EST_I               60921 non-null   object        \n",
      " 2   CRASH_DATE                     814788 non-null  datetime64[ns]\n",
      " 3   POSTED_SPEED_LIMIT             814788 non-null  int64         \n",
      " 4   TRAFFIC_CONTROL_DEVICE         814788 non-null  object        \n",
      " 5   DEVICE_CONDITION               814788 non-null  object        \n",
      " 6   WEATHER_CONDITION              814788 non-null  object        \n",
      " 7   LIGHTING_CONDITION             814788 non-null  object        \n",
      " 8   FIRST_CRASH_TYPE               814788 non-null  object        \n",
      " 9   TRAFFICWAY_TYPE                814788 non-null  object        \n",
      " 10  LANE_CNT                       199008 non-null  float64       \n",
      " 11  ALIGNMENT                      814788 non-null  object        \n",
      " 12  ROADWAY_SURFACE_COND           814788 non-null  object        \n",
      " 13  ROAD_DEFECT                    814788 non-null  object        \n",
      " 14  REPORT_TYPE                    790622 non-null  object        \n",
      " 15  CRASH_TYPE                     814788 non-null  object        \n",
      " 16  INTERSECTION_RELATED_I         186921 non-null  object        \n",
      " 17  NOT_RIGHT_OF_WAY_I             37585 non-null   object        \n",
      " 18  HIT_AND_RUN_I                  254973 non-null  object        \n",
      " 19  DAMAGE                         814788 non-null  object        \n",
      " 20  DATE_POLICE_NOTIFIED           814788 non-null  object        \n",
      " 21  PRIM_CONTRIBUTORY_CAUSE        814788 non-null  object        \n",
      " 22  SEC_CONTRIBUTORY_CAUSE         814788 non-null  object        \n",
      " 23  STREET_NO                      814788 non-null  int64         \n",
      " 24  STREET_DIRECTION               814784 non-null  object        \n",
      " 25  STREET_NAME                    814787 non-null  object        \n",
      " 26  BEAT_OF_OCCURRENCE             814783 non-null  float64       \n",
      " 27  PHOTOS_TAKEN_I                 10710 non-null   object        \n",
      " 28  STATEMENTS_TAKEN_I             18171 non-null   object        \n",
      " 29  DOORING_I                      2502 non-null    object        \n",
      " 30  WORK_ZONE_I                    4656 non-null    object        \n",
      " 31  WORK_ZONE_TYPE                 3609 non-null    object        \n",
      " 32  WORKERS_PRESENT_I              1190 non-null    object        \n",
      " 33  NUM_UNITS                      814788 non-null  int64         \n",
      " 34  MOST_SEVERE_INJURY             813000 non-null  object        \n",
      " 35  INJURIES_TOTAL                 813012 non-null  float64       \n",
      " 36  INJURIES_FATAL                 813012 non-null  float64       \n",
      " 37  INJURIES_INCAPACITATING        813012 non-null  float64       \n",
      " 38  INJURIES_NON_INCAPACITATING    813012 non-null  float64       \n",
      " 39  INJURIES_REPORTED_NOT_EVIDENT  813012 non-null  float64       \n",
      " 40  INJURIES_NO_INDICATION         813012 non-null  float64       \n",
      " 41  INJURIES_UNKNOWN               813012 non-null  float64       \n",
      " 42  CRASH_HOUR                     814788 non-null  int64         \n",
      " 43  CRASH_DAY_OF_WEEK              814788 non-null  int64         \n",
      " 44  CRASH_MONTH                    814788 non-null  int64         \n",
      " 45  LATITUDE                       809200 non-null  float64       \n",
      " 46  LONGITUDE                      809200 non-null  float64       \n",
      " 47  LOCATION                       809200 non-null  object        \n",
      " 48  HOLIDAY_NAME                   23236 non-null   object        \n",
      " 49  YEAR                           814788 non-null  int32         \n",
      " 50  MONTH                          814788 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(11), int32(1), int64(6), object(32)\n",
      "memory usage: 313.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# 2.2.3. Dataset info\n",
    "\n",
    "def get_dataset_info(df):\n",
    "    df.info()\n",
    "\n",
    "get_dataset_info(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2.4. Numerical Feature Distribution\n",
    "\n",
    "# Function for visualizaiotn of categorical feature distribution\n",
    "def Numerical_feature_Dist(df):\n",
    "    # Select only numeric columns for distribution plots\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    # Determine the number of rows/columns for the subplot grid\n",
    "    num_features = numeric_df.shape[1]\n",
    "    num_rows = int(np.ceil(num_features / 3))  # Adjust the denominator to change the number of columns\n",
    "    # Create a figure and a grid of subplots\n",
    "    plt.figure(figsize=(15, num_rows * 5))\n",
    "    for i, column in enumerate(numeric_df.columns):\n",
    "        plt.subplot(num_rows, 3, i + 1)\n",
    "        sns.histplot(numeric_df[column], kde=True, stat = 'density')\n",
    "        plt.title(f'Distribution of {column}')\n",
    "        plt.xlabel(column)\n",
    "        plt.ylabel('Frequency')\n",
    "    # Adjust layout for better visualization\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Numerical_feature_Dist(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.2.5 Catagorical Feature Distribution\n",
    "\n",
    "\n",
    "# Function for visualizaiotn of categorical feature distribution\n",
    "def Categorical_feature_Dist(df):\n",
    "    # Select only categorical columns\n",
    "    categorical_df = df.select_dtypes(include=['object', 'category']) #exclude=['number']\n",
    "    # Delete some of the columns due ot cardinality issues (they have alot of levels like Id)\n",
    "    categorical_df = categorical_df.drop(['PRIM_CONTRIBUTORY_CAUSE','SEC_CONTRIBUTORY_CAUSE','STREET_NAME','DATE_POLICE_NOTIFIED','LOCATION','CRASH_RECORD_ID' , 'CRASH_DATE_EST_I','CRASH_DATE'] , axis = 1)\n",
    "    # Determine the number of rows/columns for the subplot grid\n",
    "    num_features = categorical_df.shape[1]\n",
    "    num_rows = int(np.ceil(num_features / 3))  # Adjust for desired number of columns per row\n",
    "    plt.figure(figsize=(15, num_rows * 5))\n",
    "    for i, column in enumerate(categorical_df.columns):\n",
    "        plt.subplot(num_rows, 3, i + 1)\n",
    "        ax = sns.countplot(y=categorical_df[column])\n",
    "        total = len(categorical_df[column])  # Total number of data points for the percentage calculation\n",
    "        for p in ax.patches:\n",
    "            percentage = '{:.1f}%'.format(100 * p.get_width()/total)\n",
    "            x = p.get_x() + p.get_width() + 0.02  # Shifts the text to the right side of the bars\n",
    "            y = p.get_y() + p.get_height()/2\n",
    "            ax.annotate(percentage, (x, y))\n",
    "        \n",
    "        plt.title(f'Distribution of {column}')\n",
    "        plt.xlabel('Count')\n",
    "        plt.ylabel(column)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Categorical_feature_Dist(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2.6. HTML report\n",
    "\n",
    "# Disable since it is taking high CPU/Memory resources\n",
    "# profile = ProfileReport(df, title=\"Pandas Profiling Report\",explorative=True)\n",
    "# profile.to_file(\"pandas_profiling_report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "**2.2. Data Description**\n",
    "**Analysis**\n",
    "\n",
    "For all 2.2 section.\n",
    "\n",
    "1. **Determining the Size of the Dataset**:\n",
    "   - our dataset has a considerable size with **814,788 instances** and **51 features** that provide detailed attributes or properties of each instance.\n",
    "\n",
    "2. **Previewing the Data**:\n",
    "   - We have displayed the first few entries of the dataset to get an initial understanding of the data's structure and content. This is like reading the first page of a book to get a sense of the story.\n",
    "\n",
    "3. **Assessing Data Completeness**:\n",
    "   - The dataset's informational output revealed that certain features have missing values. For example, 'LANE_CNT' has many missing values, whereas 'CRASH_RECORD_ID' is complete. This insight is crucial for understanding which features are fully observed and which may require data imputation or cleaning.\n",
    "\n",
    "4. **Visualizing Feature Distributions**:\n",
    "   - We have created a series of plots to visualize the distribution of various features. These plots help us understand the frequency and pattern of different attributes. For example, the 'POSTED_SPEED_LIMIT' plot suggests most data points cluster around specific speed limits (30), while the 'LANE_CNT' plot indicates a right-skewed distribution, meaning lower lane counts are more common.\n",
    "\n",
    "5. **Exploring Categorical Features**:\n",
    "   - For categorical data, We have smartly chosen to remove features with high cardinality to simplify the analysis. Then, we have visualized the remaining categorical features in a grid of plots, displaying the relative frequency of each category within these features. These visualizations help to quickly grasp which categories are most or least common.\n",
    "\n",
    "there are some important thing I want to mention here: \n",
    "\n",
    "\n",
    "# Cloudiness in Illinois\n",
    "\n",
    "Illinois has never been considered the Sunshine State. However, cities in Illinois are considered in the middle ranks of U.S. cities in terms of cloudiness. They are neither as sunny as Las Vegas nor as cloudy as Seattle.\n",
    "\n",
    "Here is a breakdown of the number of days with clear, partly cloudy, and cloudy skies for select cities in Illinois with long-term observations. The daily average sky cover, based on hourly weather observations of cloud types and coverage (in tenths), is classified into three classes:\n",
    "- Clear: daily average sky cover ranging from 0 to 3 tenths\n",
    "- Partly cloudy: daily average sky cover from 4 to 6 tenths\n",
    "- Cloudy: daily average sky cover from 7 to 10 tenths\n",
    "\n",
    "## Annual Average Number of Days by City\n",
    "\n",
    "| City       | Clear | Partly Cloudy | Cloudy |\n",
    "|------------|-------|----------------|--------|\n",
    "| Cairo      | 113   | 104            | 149    |\n",
    "| Chicago    | 84    | 105            | 176    |\n",
    "| Moline     | 101   | 100            | 164    |\n",
    "| Peoria     | 95    | 97             | 172    |\n",
    "| Rockford   | 93    | 98             | 174    |\n",
    "| Springfield| 104   | 94             | 167    |\n",
    "\n",
    "You can also visit the [National Centers for Environmental Information](https://www.ncdc.noaa.gov/) for more information on cloudiness of U.S. cities.\n",
    "\n",
    "\n",
    "but as you can see in our dataset the Clear weather is about 78.5% of our data which means most of the accident happened in clear way and we can see from the report that there are only 84 days with clear weather condition in chicago, so this means that people maybe care more about driving in Cloudy day so there are less accident happening with this condition.\n",
    "<\\span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 2.3. Explore the Data\n",
    "\n",
    "2.3.1. Basic Statistics - For numeric models, to calculate Mean, Median, mode, ... <br>\n",
    "2.3.2. Tabular Report - For Continuos and Categorical Features (Refer to HTML Reports please). <br>\n",
    "2.3.3. Correlations + Heat Map.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>POSTED_SPEED_LIMIT</th>\n",
       "      <td>814788.0</td>\n",
       "      <td>28.405658</td>\n",
       "      <td>6.168020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>9.900000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LANE_CNT</th>\n",
       "      <td>199008.0</td>\n",
       "      <td>13.330213</td>\n",
       "      <td>2961.608593</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.191625e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STREET_NO</th>\n",
       "      <td>814788.0</td>\n",
       "      <td>3689.471208</td>\n",
       "      <td>2886.846405</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1250.000000</td>\n",
       "      <td>3201.000000</td>\n",
       "      <td>5600.000000</td>\n",
       "      <td>4.511000e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BEAT_OF_OCCURRENCE</th>\n",
       "      <td>814783.0</td>\n",
       "      <td>1243.649172</td>\n",
       "      <td>705.298265</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>1211.000000</td>\n",
       "      <td>1822.000000</td>\n",
       "      <td>6.100000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM_UNITS</th>\n",
       "      <td>814788.0</td>\n",
       "      <td>2.034870</td>\n",
       "      <td>0.452707</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.800000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INJURIES_TOTAL</th>\n",
       "      <td>813012.0</td>\n",
       "      <td>0.189815</td>\n",
       "      <td>0.565798</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.100000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INJURIES_FATAL</th>\n",
       "      <td>813012.0</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.037361</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INJURIES_INCAPACITATING</th>\n",
       "      <td>813012.0</td>\n",
       "      <td>0.019938</td>\n",
       "      <td>0.165285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INJURIES_NON_INCAPACITATING</th>\n",
       "      <td>813012.0</td>\n",
       "      <td>0.107078</td>\n",
       "      <td>0.422087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.100000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INJURIES_REPORTED_NOT_EVIDENT</th>\n",
       "      <td>813012.0</td>\n",
       "      <td>0.061608</td>\n",
       "      <td>0.319045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INJURIES_NO_INDICATION</th>\n",
       "      <td>813012.0</td>\n",
       "      <td>2.003335</td>\n",
       "      <td>1.157104</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.100000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INJURIES_UNKNOWN</th>\n",
       "      <td>813012.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRASH_HOUR</th>\n",
       "      <td>814788.0</td>\n",
       "      <td>13.198735</td>\n",
       "      <td>5.569631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>2.300000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRASH_DAY_OF_WEEK</th>\n",
       "      <td>814788.0</td>\n",
       "      <td>4.122872</td>\n",
       "      <td>1.980096</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRASH_MONTH</th>\n",
       "      <td>814788.0</td>\n",
       "      <td>6.669007</td>\n",
       "      <td>3.451716</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.200000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LATITUDE</th>\n",
       "      <td>809200.0</td>\n",
       "      <td>41.854844</td>\n",
       "      <td>0.336912</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.782496</td>\n",
       "      <td>41.874791</td>\n",
       "      <td>41.924379</td>\n",
       "      <td>4.202278e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LONGITUDE</th>\n",
       "      <td>809200.0</td>\n",
       "      <td>-87.673513</td>\n",
       "      <td>0.684812</td>\n",
       "      <td>-87.936193</td>\n",
       "      <td>-87.721747</td>\n",
       "      <td>-87.674163</td>\n",
       "      <td>-87.633389</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEAR</th>\n",
       "      <td>814788.0</td>\n",
       "      <td>2019.883443</td>\n",
       "      <td>2.257667</td>\n",
       "      <td>2013.000000</td>\n",
       "      <td>2018.000000</td>\n",
       "      <td>2020.000000</td>\n",
       "      <td>2022.000000</td>\n",
       "      <td>2.024000e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  count         mean          std  \\\n",
       "POSTED_SPEED_LIMIT             814788.0    28.405658     6.168020   \n",
       "LANE_CNT                       199008.0    13.330213  2961.608593   \n",
       "STREET_NO                      814788.0  3689.471208  2886.846405   \n",
       "BEAT_OF_OCCURRENCE             814783.0  1243.649172   705.298265   \n",
       "NUM_UNITS                      814788.0     2.034870     0.452707   \n",
       "INJURIES_TOTAL                 813012.0     0.189815     0.565798   \n",
       "INJURIES_FATAL                 813012.0     0.001191     0.037361   \n",
       "INJURIES_INCAPACITATING        813012.0     0.019938     0.165285   \n",
       "INJURIES_NON_INCAPACITATING    813012.0     0.107078     0.422087   \n",
       "INJURIES_REPORTED_NOT_EVIDENT  813012.0     0.061608     0.319045   \n",
       "INJURIES_NO_INDICATION         813012.0     2.003335     1.157104   \n",
       "INJURIES_UNKNOWN               813012.0     0.000000     0.000000   \n",
       "CRASH_HOUR                     814788.0    13.198735     5.569631   \n",
       "CRASH_DAY_OF_WEEK              814788.0     4.122872     1.980096   \n",
       "CRASH_MONTH                    814788.0     6.669007     3.451716   \n",
       "LATITUDE                       809200.0    41.854844     0.336912   \n",
       "LONGITUDE                      809200.0   -87.673513     0.684812   \n",
       "YEAR                           814788.0  2019.883443     2.257667   \n",
       "\n",
       "                                       min          25%          50%  \\\n",
       "POSTED_SPEED_LIMIT                0.000000    30.000000    30.000000   \n",
       "LANE_CNT                          0.000000     2.000000     2.000000   \n",
       "STREET_NO                         0.000000  1250.000000  3201.000000   \n",
       "BEAT_OF_OCCURRENCE              111.000000   714.000000  1211.000000   \n",
       "NUM_UNITS                         1.000000     2.000000     2.000000   \n",
       "INJURIES_TOTAL                    0.000000     0.000000     0.000000   \n",
       "INJURIES_FATAL                    0.000000     0.000000     0.000000   \n",
       "INJURIES_INCAPACITATING           0.000000     0.000000     0.000000   \n",
       "INJURIES_NON_INCAPACITATING       0.000000     0.000000     0.000000   \n",
       "INJURIES_REPORTED_NOT_EVIDENT     0.000000     0.000000     0.000000   \n",
       "INJURIES_NO_INDICATION            0.000000     1.000000     2.000000   \n",
       "INJURIES_UNKNOWN                  0.000000     0.000000     0.000000   \n",
       "CRASH_HOUR                        0.000000     9.000000    14.000000   \n",
       "CRASH_DAY_OF_WEEK                 1.000000     2.000000     4.000000   \n",
       "CRASH_MONTH                       1.000000     4.000000     7.000000   \n",
       "LATITUDE                          0.000000    41.782496    41.874791   \n",
       "LONGITUDE                       -87.936193   -87.721747   -87.674163   \n",
       "YEAR                           2013.000000  2018.000000  2020.000000   \n",
       "\n",
       "                                       75%           max  \n",
       "POSTED_SPEED_LIMIT               30.000000  9.900000e+01  \n",
       "LANE_CNT                          4.000000  1.191625e+06  \n",
       "STREET_NO                      5600.000000  4.511000e+05  \n",
       "BEAT_OF_OCCURRENCE             1822.000000  6.100000e+03  \n",
       "NUM_UNITS                         2.000000  1.800000e+01  \n",
       "INJURIES_TOTAL                    0.000000  2.100000e+01  \n",
       "INJURIES_FATAL                    0.000000  4.000000e+00  \n",
       "INJURIES_INCAPACITATING           0.000000  1.000000e+01  \n",
       "INJURIES_NON_INCAPACITATING       0.000000  2.100000e+01  \n",
       "INJURIES_REPORTED_NOT_EVIDENT     0.000000  1.500000e+01  \n",
       "INJURIES_NO_INDICATION            2.000000  6.100000e+01  \n",
       "INJURIES_UNKNOWN                  0.000000  0.000000e+00  \n",
       "CRASH_HOUR                       17.000000  2.300000e+01  \n",
       "CRASH_DAY_OF_WEEK                 6.000000  7.000000e+00  \n",
       "CRASH_MONTH                      10.000000  1.200000e+01  \n",
       "LATITUDE                         41.924379  4.202278e+01  \n",
       "LONGITUDE                       -87.633389  0.000000e+00  \n",
       "YEAR                           2022.000000  2.024000e+03  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.3.1. Continuous Features Tabular Report\n",
    "\n",
    "def get_continuous_features_tabular_report(df):\n",
    "    # continuous_features_tabular_report_df = df.describe(include=['number'])\n",
    "    return df.describe(include=['number'])\n",
    "\n",
    "# Transpose - More friendly print\n",
    "continuous_features_transpose_tabular_report_df = get_continuous_features_tabular_report(df).transpose()\n",
    "\n",
    "continuous_features_transpose_tabular_report_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3.2. Categorical Features Tabular Report\n",
    "\n",
    "def get_categorical_features_tabular_report(df):\n",
    "    return df.describe(exclude=['number'])\n",
    "\n",
    "# Transpose - More friendly print\n",
    "categorical_features_transpose_tabular_report_df = get_categorical_features_tabular_report(df).transpose()\n",
    "\n",
    "categorical_features_transpose_tabular_report_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3.3. Correlation insights + HeatMap\n",
    "\n",
    "def get_correlation_insights(df):\n",
    "    # # Get the relevant columns\n",
    "    subset_df = df[get_continuous_features_tabular_report(df).columns]\n",
    "\n",
    "    # Calculate the correlation matrix\n",
    "    # correlation_matrix = subset_df.corr()\n",
    "    return subset_df.corr()\n",
    "\n",
    "correlation_matrix = get_correlation_insights(df)\n",
    "correlation_matrix\n",
    "\n",
    "# # Create a heatmap using seaborn\n",
    "# plt.figure(figsize=(16, 14))\n",
    "# sns.heatmap(correlation_matrix, cmap='coolwarm', annot=True, fmt=\".2f\", linewidths=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3.4. HeatMap\n",
    "\n",
    "def draw_heatmap(correlation_matrix):\n",
    "    # Create a heatmap using seaborn\n",
    "    plt.figure(figsize=(16, 14))\n",
    "    return sns.heatmap(correlation_matrix, cmap='coolwarm', annot=True, fmt=\".2f\", linewidths=0.8)\n",
    "\n",
    "draw_heatmap(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 2.4. Verify Data Quality\n",
    "\n",
    "2.4.1. Completnesss - Missing Values Summary.<br>\n",
    "2.4.2. Irregular cardinality, could be one of the following cases:<br>\n",
    "* Features with a cardinality of 1.\n",
    "* Too high cardinality for categorical features.\n",
    "* Too low cardinality for continous features.<br>\n",
    "\n",
    "2.4.3. Consistency - Handle outliers.<br>\n",
    "2.4.4. Uniqueness - Remove duplicated or irrelevant repetition in data.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.1. Completnesss - Missing Values Summary\n",
    "\n",
    "# Get missing values details\n",
    "# Retrun value per feature as a numeric value and a percentage\n",
    "def get_missing_values_details(df):\n",
    "    # Check for missing values in each column\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_values_sorted = missing_values.sort_values(ascending=False)\n",
    "    # Check the percentage of missing values for each column\n",
    "    missing_percentage_sorted = (missing_values_sorted / len(df)) * 100\n",
    "    return missing_values_sorted, missing_percentage_sorted\n",
    "\n",
    "\n",
    "def get_completeness_report(df):\n",
    "    missing_values_sorted, missing_percentage_sorted = get_missing_values_details(df)\n",
    "    # Create a DataFrame to summarize the completeness\n",
    "    completeness_report = pd.DataFrame({\n",
    "        'Missing Values': missing_values_sorted,\n",
    "        'Missing Percentage': missing_percentage_sorted\n",
    "    })\n",
    "    return completeness_report\n",
    "\n",
    "completeness_report = get_completeness_report(df)\n",
    "completeness_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section x.y. - Check Ranges & Special Outliers\n",
    "get_continuous_feature_details(df, 'CRASH_HOUR')\n",
    "get_continuous_feature_details(df, 'CRASH_DAY_OF_WEEK')\n",
    "get_continuous_feature_details(df, 'CRASH_MONTH')\n",
    "get_continuous_feature_details(df, 'LATITUDE')\n",
    "get_continuous_feature_details(df, 'LONGITUDE')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### x.y. Check Ranges & Special Outliers\n",
    "##### Analysis\n",
    "\n",
    "This section addresses problems mostly in ranges for special features.\n",
    "For example, week can't be more than 7 days.\n",
    "Let's go over the descriptive features and provide insights.\n",
    "\n",
    "\n",
    "\n",
    "| Feature            | Unique Values Len | Unique Values Example    | Max Value | Min Value | Comments                |\n",
    "|--------------------|-------------------|--------------------------|-----------|-----------|-------------------------|\n",
    "| CRASH_HOUR         | 24 |  [0 1 2 ... 23]  |  23 | 0 | Time of the crash (0-23)|\n",
    "| CRASH_DAY_OF_WEEK  | 7 | [1 2 3 4 5 6 7] | 7 | 1 | Day of the week (1-7)   |\n",
    "| CRASH_MONTH        | 12 | [1 2 3 ... 12] |  12  |  1   | Month of the crash (1-12)|\n",
    "| LATITUDE           |  299299 |  [ 0 41.64467013 41.64469152 ... 42.02273632 ]      |    42.022779861       |     0.0      | Geographical latitude   |\n",
    "| LONGITUDE          |  299262  | [-87.93619295 -87.93587692 ... 0] |   0.0        |     -87.936192947      | Geographical longitude  |\n",
    "\n",
    "<br>\n",
    "\n",
    "As the table illustrates, the following features have valid values:<br>\n",
    "* CRASH_HOUR\n",
    "* CRASH_DAY_OF_WEEK\n",
    "* CRASH_MONTH\n",
    "\n",
    "<br>\n",
    "\n",
    "But the following features has values out of range / outliers in respect to each one type:\n",
    "* LATITUDE - Min value is 0.0\n",
    "* LONGITUDE - Max value is 0.0\n",
    "\n",
    "<br>\n",
    "\n",
    "Doing research in Google Maps and Google for Chicago city LATITUDE and LONGITUDE range (also NaN, but will be addressing in missing values section), here is our findings:<br>\n",
    "LATITUDE valid range: [41.640, 42.023]<br>\n",
    "LONGITUDE valid range: [-87.940, -87.524]<br>\n",
    "\n",
    "We will address that again in the Data Preprocessing section to clean up the out of range values.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.2. Irregular cardinality - Features with a cardinality of 1\n",
    "\n",
    "IRREGULAR_CARD_SINGLE = 1             # To detect features with cardinality of 1\n",
    "\n",
    "def get_irregular_cardinality_with_single_cardinality(df):\n",
    "    unique_counts = df.nunique()\n",
    "    # for feature_name, count in unique_counts.items():\n",
    "    #     if count == 1:\n",
    "    #         print(f\"The feature '{feature_name}' has a unique count of 1 - Irregular Cardinality Issue\")\n",
    "    unique_counts_single_threshold = unique_counts[unique_counts == IRREGULAR_CARD_SINGLE]\n",
    "    return pd.DataFrame(unique_counts_single_threshold)\n",
    "\n",
    "\n",
    "get_irregular_cardinality_with_single_cardinality(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.2. Irregular cardinality - Too high cardinality for categorical features\n",
    "\n",
    "IRREGULAR_CARD_MAX_THRESHOLD = 10     # To detect categorical features with high cardinality\n",
    "\n",
    "def get_unique_counts_exceed_max_threshold_categorical(df):\n",
    "    cardinality = df.select_dtypes(exclude=['number']).apply(lambda x: x.nunique())\n",
    "    high_cardinality_cols = cardinality[cardinality > IRREGULAR_CARD_MAX_THRESHOLD].index.tolist()\n",
    "    # print(\"High Cardinality Columns:\", high_cardinality_cols)\n",
    "    return high_cardinality_cols\n",
    "\n",
    "high_cardinality_cols = get_unique_counts_exceed_max_threshold_categorical(df)\n",
    "# Categorical Features with carinality higher than IRREGULAR_CARD_MAX_THRESHOLD\n",
    "print(f\"Categorical Features with carinality higher than {IRREGULAR_CARD_MAX_THRESHOLD} cardinality are: {high_cardinality_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.2. Irregular cardinality - Too low cardinality for continuous features\n",
    "\n",
    "IRREGULAR_CARD_MIN_THRESHOLD = 10      # To detect continuous features with low cardinality\n",
    "\n",
    "def get_unique_counts_exceed_min_threshold_continuous(df):\n",
    "    continuous = df.select_dtypes(include=['number']).apply(lambda x: x.nunique())\n",
    "    high_cardinality_cols = continuous[continuous < IRREGULAR_CARD_MIN_THRESHOLD].index.tolist()\n",
    "    # print(\"High Cardinality Columns:\", high_cardinality_cols)\n",
    "    return high_cardinality_cols\n",
    "    # cardinality = df.select_dtypes(exclude=['number']).apply(lambda x: x.nunique())\n",
    "    # unique_counts = df.nunique()\n",
    "    # unique_counts_below_min_threshold = unique_counts[unique_counts < IRREGULAR_CARD_MIN_THRESHOLD]\n",
    "    # return pd.DataFrame(unique_counts_below_min_threshold)\n",
    "\n",
    "get_unique_counts_exceed_min_threshold_continuous(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### 2.4.2.  - Irregular Cardinality\n",
    "##### Analysis\n",
    "\n",
    "To do - describe the results from above.\n",
    "\n",
    "Explain why ABOVE NUMBERS ARE OK.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.3. Consistency - Handle outliers\n",
    "\n",
    "def get_outliers_list_of_indices(df, ft):\n",
    "    Q1 = df[ft].quantile(0.25)\n",
    "    Q3 = df[ft].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    ls = df.index[ (df[ft] < lower_bound) | (df[ft] > upper_bound) ]\n",
    "    \n",
    "    return ls\n",
    "\n",
    "# To store the indicies\n",
    "outliers_index_list = []\n",
    "# To store the features (set, so no duplications)\n",
    "outliers_features_set = set()\n",
    "numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "for feature in numeric_columns:\n",
    "    indicies_list = get_outliers_list_of_indices(df, feature)\n",
    "    outliers_index_list.extend(indicies_list)\n",
    "    if len(indicies_list) != 0:\n",
    "        outliers_features_set.add(feature)\n",
    "\n",
    "print(f\"There are {len(outliers_index_list)} outliers\")\n",
    "print(f\"Features set are {outliers_features_set}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### 2.4.2.  - Outliers\n",
    "##### Analysis\n",
    "\n",
    "Outliers inital result shows the following features are outliers:\n",
    "* STREET_NO\n",
    "* POSTED_SPEED_LIMIT\n",
    "* INJURIES_REPORTED_NOT_EVIDENT\n",
    "* LONGITUDE\n",
    "* LANE_CNT\n",
    "* BEAT_OF_OCCURRENCE\n",
    "* INJURIES_TOTAL\n",
    "* INJURIES_INCAPACITATING\n",
    "* INJURIES_NON_INCAPACITATING\n",
    "* INJURIES_NO_INDICATION\n",
    "* LATITUDE\n",
    "* INJURIES_FATAL\n",
    "* NUM_UNITS\n",
    "\n",
    "Let's understand if they are really outliers, or the values are valid (e.g. street number is valid to be in a very big range).\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_continuous_feature_details(df, 'STREET_NO')\n",
    "get_continuous_feature_details(df, 'POSTED_SPEED_LIMIT')\n",
    "get_continuous_feature_details(df, 'INJURIES_REPORTED_NOT_EVIDENT')\n",
    "get_continuous_feature_details(df, 'LONGITUDE')\n",
    "get_continuous_feature_details(df, 'LANE_CNT')\n",
    "get_continuous_feature_details(df, 'BEAT_OF_OCCURRENCE')\n",
    "get_continuous_feature_details(df, 'INJURIES_TOTAL')\n",
    "get_continuous_feature_details(df, 'INJURIES_INCAPACITATING')\n",
    "get_continuous_feature_details(df, 'INJURIES_NON_INCAPACITATING')\n",
    "get_continuous_feature_details(df, 'INJURIES_NO_INDICATION')\n",
    "get_continuous_feature_details(df, 'LATITUDE')\n",
    "get_continuous_feature_details(df, 'INJURIES_FATAL')\n",
    "get_continuous_feature_details(df, 'NUM_UNITS')\n",
    "pass # To avoid printing of the previous line and keep only formatted-pretty prints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### 2.4.2.  - Outliers\n",
    "##### Analysis\n",
    "\n",
    "| Feature                        | Unique Values Len | Unique Values Example                  | Max Value    | Min Value    | Comments                                                                 | Explanation                                        |\n",
    "|--------------------------------|-------------------|----------------------------------------|--------------|--------------|--------------------------------------------------------------------------|----------------------------------------------------|\n",
    "| STREET_NO                      | 11723             | 0, 1, 2, 3, 4, ..., 13787, 13795, 13799, 34453, 451100 | 451100       | 0            | Street numbers in Chicago, large range due to city size and data errors. | Street values can highly vary, it doesn't follow math logic      |\n",
    "| POSTED_SPEED_LIMIT             | 46                | 0, 1, 2, 3, 4, ..., 62, 63, 65, 70, 99 | 99           | 0            | Posted speed limits, including possibly erroneous values.                | Speed limits can highly vary. We checked all values count to verify no outliers in respect to allowed speed - e.g. no instance with value 500 (please refer to the function dump_feature_frequency_to_a_file)       |\n",
    "| INJURIES_REPORTED_NOT_EVIDENT  | 14                | 0.0, 1.0, 2.0, 3.0, 4.0, ..., 9.0, 10.0, 11.0, 15.0 | 15.0         | 0.0          | Number of non-evident injuries reported per crash.                       | Range is reasonable for severe accidents.           |\n",
    "| LONGITUDE                      | 299263            | -87.936192947, ..., -87.524587387, 0.0 | 0.0          | -87.936192947| Longitude values in Chicago, including erroneous 0.0 entries.            | 0.0 likely represents missing or incorrectly entered data. It will be handled in data preprocessing phase. |\n",
    "| LANE_CNT                       | 42                | 0.0, 1.0, 2.0, 3.0, 4.0, ..., 1191625.0| 1191625.0    | 0.0          | Number of lanes, with some unrealistic values due to errors.             | Extreme max value due to data entry errors.        |\n",
    "| BEAT_OF_OCCURRENCE             | 277               | 111.0, ..., 2535.0, 6100.0             | 6100.0       | 111.0        | Police beat codes, including possibly erroneous 6100.0.                 | This is police internal codes and can't be treated as outliers          |\n",
    "| INJURIES_TOTAL                 | 21                | 0.0, 1.0, ..., 19.0, 21.0              | 21.0         | 0.0          | Total number of injuries per crash.                                      | Range is within expected limits for traffic accidents. |\n",
    "| INJURIES_INCAPACITATING        | 11                | 0.0, 1.0, ..., 8.0, 10.0               | 10.0         | 0.0          | Number of incapacitating injuries per crash.                             | Within expected limits, considering severe cases.   |\n",
    "| INJURIES_NON_INCAPACITATING    | 20                | 0.0, 1.0, ..., 19.0, 21.0              | 21.0         | 0.0          | Number of non-incapacitating injuries per crash.                         | Range reflects possible severe accidents.           |\n",
    "| INJURIES_NO_INDICATION         | 49                | 0.0, 1.0, ..., 50.0, 61.0              | 61.0         | 0.0          | People involved in crash with no injuries reported.                      | High values for large accidents, not outliers.     |\n",
    "| LATITUDE                       | 299300            | 0.0, 41.644670132, ..., 42.022779861   | 42.022779861 | 0.0          | Latitude values in Chicago, including erroneous 0.0 entries.             | 0.0 likely represents missing or incorrectly entered data. It will be handled in data preprocessing phase. |\n",
    "| INJURIES_FATAL                 | 6                 | 0.0, 1.0, ..., 3.0, 4.0                | 4.0          | 0.0          | Number of fatal injuries per crash.                                      | Fatalities are rare, but values are within expected limits. |\n",
    "| NUM_UNITS                      | 17                | 1, 2, ..., 16, 18                      | 18           | 1            | Number of units involved in a crash (e.g. cars, motocycles, ...) | Not an outlier, the highest values sounds reasonable |\n",
    "\n",
    "From the above table, we can notice that the following features has outliers:\n",
    "* LANE_CNT\n",
    "* LONGITUDE\n",
    "* LATITUDE\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.4. Uniqueness - Remove duplicated or irrelevant repetition in data\n",
    "\n",
    "def consistency_check_duplicated_instances(df):\n",
    "    duplicate_rows = df.duplicated().sum()\n",
    "    print(f\"Number of duplicate rows: {duplicate_rows}\")\n",
    "    # If you want to actually see the duplicate rows, you can use:\n",
    "    if duplicate_rows > 0:\n",
    "        print(df[df.duplicated(keep=False)].sort_values(by=df.columns.tolist()))\n",
    "\n",
    "consistency_check_duplicated_instances(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 3. CRISP-DM: Data Preparation\n",
    "\n",
    "3.1. Missing Values - Drop or Imputation.<br>\n",
    "3.2. Irregular Cardinality.<br>\n",
    "3.3. Outliers - To remove instances.<br>\n",
    "3.4. Consistency - Invalid formats / Ranges.<br>\n",
    "\n",
    "More details in each section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1. Missing Values (Part-1) - Drop\n",
    "\n",
    "# For each step, we take a copy. It is easier to debug and investigate.\n",
    "df_drop_missing_features = df.copy()\n",
    "\n",
    "# Set the threshold for dropping columns\n",
    "MISSING_THRESHOLD = 30.0\n",
    "\n",
    "# Identify columns that have missing value percentage greater than the threshold\n",
    "columns_to_drop = completeness_report[completeness_report['Missing Percentage'] >= MISSING_THRESHOLD].index\n",
    "\n",
    "# Drop these columns from the DataFrame\n",
    "df_drop_missing_features = df_drop_missing_features.drop(columns=columns_to_drop)\n",
    "\n",
    "# Now, df has the columns dropped where the missing value percentage was higher than 30%\n",
    "print(\"Features with missing instances higher than 30% that has been dropped\")\n",
    "columns_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1. Missing Values (Part-2) - Special handle for LATITUDE and LONGITUDE\n",
    "\n",
    "df_drop_missing_instances = df_drop_missing_features.copy()\n",
    "\n",
    "\n",
    "def drop_missing_instances(my_df, my_feature):\n",
    "    # Remove rows where LATITUDE is NaN\n",
    "    df_cleaned = my_df.dropna(subset=[my_feature])\n",
    "    return df_cleaned\n",
    "drop_missing_instances(df_drop_missing_features, 'LATITUDE')\n",
    "drop_missing_instances(df_drop_missing_features, 'LATITUDE')\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1. Missing Values (Part-2) - Imputation for values\n",
    "\n",
    "\n",
    "df_imputation_missing = df_drop_missing_instances.copy()\n",
    "\n",
    "# Identify columns that have missing value percentage less than the imputation threshold\n",
    "columns_to_impute = completeness_report[completeness_report['Missing Percentage'] < MISSING_THRESHOLD].index\n",
    "\n",
    "\n",
    "# Loop through the columns and perform imputation\n",
    "for column in columns_to_impute:\n",
    "    # if df_prepared[column].dtype == 'float64' or df[column].dtype == 'int64':\n",
    "    if df_imputation_missing[column].dtype == 'numeric':\n",
    "        # Impute numerical columns with the mean value\n",
    "        df_imputation_missing[column].fillna(df_imputation_missing[column].mean(), inplace=True)\n",
    "    else:\n",
    "        # Impute categorical columns with the mode value (the most frequent value)\n",
    "        df_imputation_missing[column].fillna(df_imputation_missing[column].mode()[0], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1. Missing Values (Part-3) - Print for visibility of current situation\n",
    "\n",
    "completeness_report = get_completeness_report(df_imputation_missing)\n",
    "completeness_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### 3.1. Missing Values -Imputation for values\n",
    "##### Analysis\n",
    "\n",
    "# To-Do - Update about the three parts\n",
    "\n",
    "For missing values higher than 30%, we drop them.\n",
    "For missing values lower than 30% - we do imputation. To be more accurate, in the terms of our dataset, missing values under 30% ranges between 0.1%-3% missing values so imputation is a very reasonable choice in this case.\n",
    "\n",
    "Please refer to missing values report in the Data Understanding section.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.y. Drop Duplicated features\n",
    "\n",
    "df_drop_location = df_imputation_missing.copy()\n",
    "\n",
    "df_drop_location = df_drop_location.drop('LOCATION', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### x.y. Duplicated Columns - LOCATION is the pair LATITUDE and LONGITUDE\n",
    "##### Analysis\n",
    "\n",
    "LOCATION is the pair of values of LONGITUDE and LATITUDE, we choose to drop it due to two reasons:\n",
    "1. It is duplication of other features (aggregation of LONGITUDE and LATITUDE in pairs will result in LOCATION).\n",
    "2. It is categorical feature, with high class dimensionality - meaning, in the encoding phase to prepare for modeling, it will generate very big number of derived features. In the other hand, LONGITUDE and LATITUDE is numeric and much easier to the handling in modeling phase.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.y. Drop Duplicated features\n",
    "\n",
    "df_drop_crash_record = df_drop_location.copy()\n",
    "df_drop_crash_record = df_drop_crash_record.drop('CRASH_RECORD_ID', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### x.y. Drop CRASH_ID column\n",
    "##### Analysis\n",
    "\n",
    "It is just ID of the crash, and has no contributation to the prediction model.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section x.y. - Fix Ranges and Special Outliers\n",
    "\n",
    "df_fix_range_issues = df_drop_crash_record.copy()\n",
    "\n",
    "print(\"========================== Values Before ==========================\")\n",
    "get_continuous_feature_details(df_fix_range_issues, 'LATITUDE')\n",
    "get_continuous_feature_details(df_fix_range_issues, 'LONGITUDE')\n",
    "# get_continuous_feature_details(df_fix_range_issues, 'LANE_CNT')\n",
    "\n",
    "# To remove values for my_feature outside a given range range (including min_value and max_value)\n",
    "def remove_invalid_values_outside_given_range(my_df, my_feature, min_value, max_value):\n",
    "    # Create a mask for values within the range\n",
    "    valid_mask = (my_df[my_feature] >= min_value) & (my_df[my_feature] <= max_value)\n",
    "    # Apply the mask to the DataFrame\n",
    "    filtered_df = my_df[valid_mask]\n",
    "    return filtered_df\n",
    "\n",
    "# Define the valid range for LATITUDE\n",
    "min_latitude = 41.640\n",
    "max_latitude = 42.023\n",
    "df_fix_range_issues = remove_invalid_values_outside_given_range(df_fix_range_issues, 'LATITUDE', min_latitude, max_latitude)\n",
    "\n",
    "# Define the valid range for LONGITUDE\n",
    "min_longitude = -87.940\n",
    "max_longitude = -87.524\n",
    "df_fix_range_issues = remove_invalid_values_outside_given_range(df_fix_range_issues, 'LONGITUDE', min_longitude, max_longitude)\n",
    "\n",
    "\n",
    "# Already dropped due to high missing values count\n",
    "# min_lane_count = 0\n",
    "# max_lane_count = 10\n",
    "# df_fix_range_issues = remove_invalid_values_outside_given_range(df_fix_range_issues, 'LANE_CNT', min_lane_count, max_lane_count)\n",
    "\n",
    "\n",
    "print(\"========================== Values After ==========================\")\n",
    "get_continuous_feature_details(df_fix_range_issues, 'LATITUDE')\n",
    "get_continuous_feature_details(df_fix_range_issues, 'LONGITUDE')\n",
    "# get_continuous_feature_details(df_fix_range_issues, 'LANE_CNT')\n",
    "\n",
    "get_continuous_features_tabular_report(df_fix_range_issues).transpose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### x.y. Fix Ranges and Special Outliers\n",
    "##### Analysis\n",
    "\n",
    "LOCATION is the pair of values of LONGITUDE and LATITUDE, we choose to drop it due to two reasons:\n",
    "1. It is duplication of other features (aggregation of LONGITUDE and LATITUDE in pairs will result in LOCATION).\n",
    "2. It is categorical feature, with high class dimensionality - meaning, in the encoding phase to prepare for modeling, it will generate very big number of derived features. In the other hand, LONGITUDE and LATITUDE is numeric and much easier to handling in modeling phase.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.y. - Fix Ranges, special outliers and formats\n",
    "# Please refer to Data Understanding section or to my next comment, I have summarized all here for visibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_values_count = df_fix_range_issues.copy()\n",
    "\n",
    "dump_feature_frequency_to_a_file(df_values_count ,'POSTED_SPEED_LIMIT')\n",
    "dump_feature_frequency_to_a_file(df_values_count ,'DAMAGE')\n",
    "dump_feature_frequency_to_a_file(df_values_count ,'CRASH_TYPE')\n",
    "dump_feature_frequency_to_a_file(df_values_count ,'REPORT_TYPE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3. Outliers - To remove instances.\n",
    "\n",
    "df_clean_outliers = df_fix_range_issues.copy()\n",
    "\n",
    "# def outliers1(df, ft):\n",
    "#     Q1 = df[ft].quantile(0.25)\n",
    "#     Q3 = df[ft].quantile(0.75)\n",
    "#     IQR = Q3 - Q1\n",
    "#     lower_bound = Q1 - 1.5 * IQR\n",
    "#     upper_bound = Q3 + 1.5 * IQR\n",
    "#     ls = df.index[ (df[ft] < lower_bound) | (df[ft] > upper_bound) ]\n",
    "#     return ls\n",
    "\n",
    "# index_list = []\n",
    "# for feature in df_clean_outliers.columns:\n",
    "#     if pd.api.types.is_numeric_dtype(df_clean_outliers[feature]):\n",
    "#         index_list.extend(outliers1(df_clean_outliers, feature))\n",
    "\n",
    "# def remove(df, ls):\n",
    "#     ls = sorted(set(ls))\n",
    "#     df = df.drop(ls)\n",
    "#     return df\n",
    "\n",
    "# print(f\"There are {len(set(index_list))} outliers\")\n",
    "# print(f\"DataFrame Shape Before Removing Outliers {df_clean_outliers.shape}\")\n",
    "# df_clean_outliers = remove(df_clean_outliers, index_list)\n",
    "# print(f\"DataFrame Shape After  Removing Outliers {df_clean_outliers.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### 3.3. Outliers - To remove instances\n",
    "##### Analysis\n",
    "\n",
    "To-Do\n",
    "\n",
    "Please refer to the outliers section report in the Data Understanding section.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4. Consistency - Invalid formats / Ranges\n",
    "\n",
    "df_cleaned = df_clean_outliers.copy()\n",
    "\n",
    "# Convert CRASH_DATE to datetime\n",
    "df_cleaned['CRASH_DATE'] = pd.to_datetime(df_cleaned['CRASH_DATE'])\n",
    "\n",
    "# Extract components from CRASH_DATE\n",
    "df_cleaned['YEAR'] = df_cleaned['CRASH_DATE'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cleaned.shape)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. CRISP-DM: Modeling\n",
    "\n",
    "**Section Overview**\n",
    "\n",
    "4.1. Import relevant ML libs for Modeling.<br>\n",
    "4.2. Formaluize a Numerical Measure for the Target Variable(s).<br>\n",
    "4.3. Undersampling - To fix over-representation in the dataset (unbalanced data).<br>\n",
    "4.4. Modeling with Random Forest.<br>\n",
    "4.5. Modeling with Logestic Regression.<br>\n",
    "4.6. Modeling with DNN.<br>\n",
    "4.7. Modeling with KNN.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1. Import relevant ML libs for Modeling\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2. Formaluize a Numerical Measure for the Target Variable(s)\n",
    "\n",
    "df_to_model = df_cleaned.copy()\n",
    "\n",
    "# Building our numerical metrics for measuring the Severity of injuries based on the reported injuries in the dataset\n",
    "df_to_model['SEVERITY_OF_INJURIES'] = ((0.3 * df_to_model['INJURIES_NON_INCAPACITATING'] + \n",
    "                                 0.6 * df_to_model['INJURIES_INCAPACITATING'] + \n",
    "                                 # Assuming we might use 'INJURIES_INCAPACITATING' or another column for fatal injuries representation\n",
    "                                 0.1 * df_to_model['INJURIES_NO_INDICATION'] + df_to_model['INJURIES_FATAL']) / \n",
    "                                ((df_to_model['INJURIES_NON_INCAPACITATING'] + \n",
    "                                      df_to_model['INJURIES_INCAPACITATING'] + \n",
    "                                      # Again, assuming a placeholder for fatal injuries if needed\n",
    "                                      df_to_model['INJURIES_NO_INDICATION']+df_to_model['INJURIES_FATAL'])))\n",
    "\n",
    "# Code to create the \"INJURY_CLASS\" feature based on \"SEVERITY_OF_INJURIES\"\n",
    "df_to_model['INJURY_ClASS'] = df_to_model['SEVERITY_OF_INJURIES'].apply(lambda x: 'HIGH INJURY' if x > 0.2 else 'LIGHT INJURY')\n",
    "\n",
    "# Code to create the \"SEVERITY_CLASS\" feature based on \"INJURIES_FATAL\" and \"INJURIES_INCAPACITATING\"\n",
    "df_to_model['SEVERITY_CLASS'] = df_to_model.apply(lambda x: 'HIGH SEVERITY' if x['INJURIES_FATAL'] > 0 or x['INJURIES_INCAPACITATING'] > 0 else 'LOW SEVERITY', axis=1)\n",
    "\n",
    "# Display the first few rows to see the new feature\n",
    "df_to_model[['INJURIES_NON_INCAPACITATING','INJURIES_FATAL', 'INJURIES_INCAPACITATING', 'INJURIES_NO_INDICATION', 'SEVERITY_OF_INJURIES','INJURY_ClASS','SEVERITY_CLASS']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### 4.2. Formaluize a Numerical Measure for the Target Variable(s)\n",
    "##### Analysis\n",
    "\n",
    "Our target variable is represented via 4 variables were each one giving the following indications:\n",
    "* INJURIES_NO_INDICATION - No injury or light reported.\n",
    "* INJURIES_NON_INCAPACITATING - Medium injury.\n",
    "* INJURIES_INCAPACITATING - Heavy.\n",
    "* INJURIES_FATAL - Death.\n",
    "\n",
    "To build a mesurement to estimate the fatality of the crash injury crash, we define a weighted equation for each class and normalize it.\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preparing features and target variables \n",
    "X = df_to_model.drop(['SEVERITY_OF_INJURIES','INJURIES_TOTAL' ,'INJURIES_INCAPACITATING', 'MOST_SEVERE_INJURY','INJURY_ClASS','SEVERITY_CLASS',\n",
    "                  'INJURIES_NON_INCAPACITATING', 'SEVERITY_CLASS','INJURIES_NO_INDICATION','CRASH_DATE',\n",
    "                  'INJURIES_UNKNOWN', 'DATE_POLICE_NOTIFIED','INJURIES_REPORTED_NOT_EVIDENT','INJURIES_FATAL','CRASH_RECORD_ID','LOCATION','STREET_NAME'],axis = 1)\n",
    "y = df_to_model['SEVERITY_CLASS']\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# Initialize the random under-sampler due to unbalanced dataset\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "\n",
    "# Resample the dataset\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4. Modeling with Random Forest\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Classifier with balanced class weights\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "\n",
    "# Fitting the model to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_report_results = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy of Random Forest Classifier:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", classification_report_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Features\n",
    "based on RF classifier I check the important features for detecting the severity of injuries in the accident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf_classifier.feature_importances_\n",
    "feature_names = X.columns\n",
    "feature_importances = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "feature_importances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5. Modeling with Logestic Regression\n",
    "\n",
    "# Initialize the Logistic Regression model with balanced class weights\n",
    "log_reg = LogisticRegression( max_iter=10000, random_state=42)\n",
    "\n",
    "# Fitting the model to the training data\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_report_results = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy of linear regressor:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", classification_report_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature names\n",
    "feature_names = X_resampled.columns\n",
    "\n",
    "# Get the coefficients from the logistic regression model\n",
    "coefficients = log_reg.coef_[0]  # Assuming binary classification, hence [0]\n",
    "\n",
    "# Create a series to map feature names to their coefficients\n",
    "feature_importance = pd.Series(coefficients, index=feature_names)\n",
    "\n",
    "# Sort the features by their absolute values to see the most significant ones\n",
    "feature_importance_sorted = feature_importance.abs().sort_values(ascending=False)\n",
    "\n",
    "feature_importance_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.6. Modeling with DNN\n",
    "\n",
    "# It's important to scale your input features for neural networks\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the DNN model\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20,10), activation='relu', solver='adam',\n",
    "                    max_iter=100, random_state=42, verbose=False)\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = mlp.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_report_results = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy of DNN model:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", classification_report_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.7. Modeling with KNN\n",
    "\n",
    "# Initialize the KNN classifier\n",
    "# n_neighbors is set to 10 as an example, but you should tune this parameter\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "# Train the model\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_report_results = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy of KNN model:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", classification_report_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. CRISP-DM: Evaluation\n",
    "\n",
    "in this section I just want to check our model performance on the unseen dataset which is all of them are 'LOW SEVERITY\" and check if it has good accuracy or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_to_evaluate = df_to_model.copy()\n",
    "\n",
    "# Separate instances with \"high injury\" and \"low injury\" classes \n",
    "high_injury_instances = df_to_evaluate[df_to_evaluate['SEVERITY_CLASS'] == 'HIGH SEVERITY']\n",
    "low_injury_instances = df_to_evaluate[df_to_evaluate['SEVERITY_CLASS'] == 'LOW SEVERITY']\n",
    "\n",
    "# Sample the same number of instances with \"low injury\" class as \"high injury\" instances\n",
    "num_high_injury = len(high_injury_instances)\n",
    "low_injury_sampled = low_injury_instances.sample(n=num_high_injury, random_state=42)\n",
    "\n",
    "# Check the model's predictions on the remaining instances with \"low injury\" class\n",
    "low_injury_remaining = low_injury_instances.drop(low_injury_sampled.index)\n",
    "\n",
    "# Concatenate the sampled instances of both classes to create a balanced dataset\n",
    "balanced_data = pd.concat([high_injury_instances, low_injury_sampled])\n",
    "\n",
    "\n",
    "# Prepare features and target variable\n",
    "X_train_balanced = balanced_data.drop(['SEVERITY_OF_INJURIES','INJURIES_TOTAL' ,'INJURIES_INCAPACITATING', 'MOST_SEVERE_INJURY','INJURY_ClASS','SEVERITY_CLASS',\n",
    "                  'INJURIES_NON_INCAPACITATING', 'SEVERITY_CLASS','INJURIES_NO_INDICATION','CRASH_DATE',\n",
    "                  'INJURIES_UNKNOWN', 'DATE_POLICE_NOTIFIED','INJURIES_REPORTED_NOT_EVIDENT','INJURIES_FATAL','CRASH_RECORD_ID','LOCATION','STREET_NAME'],axis = 1)\n",
    "y_train_balanced = balanced_data['SEVERITY_CLASS']\n",
    "\n",
    "X_test_balanced = low_injury_remaining.drop(['SEVERITY_OF_INJURIES','INJURIES_TOTAL' ,'INJURIES_INCAPACITATING', 'MOST_SEVERE_INJURY','INJURY_ClASS','SEVERITY_CLASS',\n",
    "                  'INJURIES_NON_INCAPACITATING', 'SEVERITY_CLASS','INJURIES_NO_INDICATION','CRASH_DATE',\n",
    "                  'INJURIES_UNKNOWN', 'DATE_POLICE_NOTIFIED','INJURIES_REPORTED_NOT_EVIDENT','INJURIES_FATAL','CRASH_RECORD_ID','LOCATION','STREET_NAME'],axis = 1)\n",
    "\n",
    "y_test_balanced = low_injury_remaining['SEVERITY_CLASS']\n",
    "\n",
    "\n",
    "X_train_balanced = pd.get_dummies(X_train_balanced)\n",
    "X_test_balanced = pd.get_dummies(X_test_balanced)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine the training and testing feature data for consistent one-hot encoding\n",
    "combined_features = pd.concat([X_train_balanced, X_test_balanced], axis=0)\n",
    "\n",
    "# Apply get_dummies to the combined dataset\n",
    "combined_features_encoded = pd.get_dummies(combined_features)\n",
    "\n",
    "# Now split them back into training and testing sets\n",
    "X_train_encoded = combined_features_encoded.iloc[:len(X_train_balanced), :]\n",
    "X_test_encoded = combined_features_encoded.iloc[len(X_train_balanced):, :]\n",
    "\n",
    "# Ensure the data is scaled\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
    "X_test_scaled = scaler.transform(X_test_encoded)\n",
    "\n",
    "\n",
    "# Train the KNN model on the scaled, balanced training data\n",
    "knn.fit(X_train_scaled, y_train_balanced)\n",
    "\n",
    "# Make predictions on the scaled, balanced testing set\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the predictions\n",
    "accuracy = accuracy_score(y_test_balanced, y_pred)\n",
    "classification_report_results = classification_report(y_test_balanced, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", classification_report_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. CRISP-DM: Deployment\n",
    "\n",
    "To-Do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Appendix - Extra Functionality & Bonus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_insights = df_cleaned.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Descriptive statistics for injury and fatality-related features\n",
    "df_for_insights[['INJURIES_TOTAL', 'INJURIES_FATAL', 'INJURIES_INCAPACITATING', 'INJURIES_NON_INCAPACITATING', 'INJURIES_REPORTED_NOT_EVIDENT', 'INJURIES_NO_INDICATION']].describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each severity level\n",
    "print(df_for_insights['INJURIES_FATAL'].value_counts())\n",
    "print(df_for_insights['INJURIES_INCAPACITATING'].value_counts())\n",
    "print(df_for_insights['INJURIES_NON_INCAPACITATING'].value_counts())\n",
    "print(df_for_insights['INJURIES_REPORTED_NOT_EVIDENT'].value_counts())\n",
    "print(df_for_insights['INJURIES_NO_INDICATION'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of records for normalization\n",
    "total_fatal = df_for_insights['INJURIES_FATAL'].sum()\n",
    "total_incapacitating = df_for_insights['INJURIES_INCAPACITATING'].sum()\n",
    "total_non_incapacitating = df_for_insights['INJURIES_NON_INCAPACITATING'].sum()\n",
    "total_reported_not_evident = df_for_insights['INJURIES_REPORTED_NOT_EVIDENT'].sum()\n",
    "\n",
    "# Calculate and print the ratio for each level within the categories\n",
    "print(\"Ratios for INJURIES_FATAL:\")\n",
    "print(df_for_insights['INJURIES_FATAL'].value_counts(normalize=True))\n",
    "print(\"\\nRatios for INJURIES_INCAPACITATING:\")\n",
    "print(df_for_insights['INJURIES_INCAPACITATING'].value_counts(normalize=True))\n",
    "print(\"\\nRatios for INJURIES_NON_INCAPACITATING:\")\n",
    "print(df_for_insights['INJURIES_NON_INCAPACITATING'].value_counts(normalize=True))\n",
    "print(\"\\nRatios for INJURIES_REPORTED_NOT_EVIDENT:\")\n",
    "print(df_for_insights['INJURIES_REPORTED_NOT_EVIDENT'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calculate_ratio_between_two_classes(my_df, feature_1, feature_2):\n",
    "    # Calculate the total sum for each injury category\n",
    "    total_feature_1 = df_for_insights[feature_1].sum()\n",
    "    total_feature_2 = df_for_insights[feature_2].sum()\n",
    "    # Calculate the ratio of incapacitating to fatal injuries\n",
    "    if total_fatal > 0:  # Prevent division by zero\n",
    "        ratio_feature_2_to_feature_1 = total_feature_2 / total_feature_1\n",
    "    print(f\"For every 1 {feature_1}, there are on average {ratio_feature_2_to_feature_1:.2f} {feature_2}\")\n",
    "\n",
    "\n",
    "print(\"Comparing Each Level to its next one:\")\n",
    "calculate_ratio_between_two_classes(df_for_insights, 'INJURIES_FATAL', 'INJURIES_INCAPACITATING')\n",
    "calculate_ratio_between_two_classes(df_for_insights, 'INJURIES_INCAPACITATING', 'INJURIES_NON_INCAPACITATING')\n",
    "calculate_ratio_between_two_classes(df_for_insights, 'INJURIES_NON_INCAPACITATING', 'INJURIES_REPORTED_NOT_EVIDENT')\n",
    "calculate_ratio_between_two_classes(df_for_insights, 'INJURIES_REPORTED_NOT_EVIDENT', 'INJURIES_NO_INDICATION')\n",
    "print()\n",
    "print(\"Comparing Each Level the base (lowest) level\")\n",
    "calculate_ratio_between_two_classes(df_for_insights, 'INJURIES_FATAL', 'INJURIES_NO_INDICATION')\n",
    "calculate_ratio_between_two_classes(df_for_insights, 'INJURIES_INCAPACITATING', 'INJURIES_NO_INDICATION')\n",
    "calculate_ratio_between_two_classes(df_for_insights, 'INJURIES_NON_INCAPACITATING', 'INJURIES_NO_INDICATION')\n",
    "calculate_ratio_between_two_classes(df_for_insights, 'INJURIES_REPORTED_NOT_EVIDENT', 'INJURIES_NO_INDICATION')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total sum for each injury category\n",
    "total_fatal = df_for_insights['INJURIES_FATAL'].sum()\n",
    "total_incapacitating = df_for_insights['INJURIES_INCAPACITATING'].sum()\n",
    "\n",
    "# Calculate the ratio of incapacitating to fatal injuries\n",
    "if total_fatal > 0:  # Prevent division by zero\n",
    "    ratio_incapacitating_to_fatal = total_incapacitating / total_fatal\n",
    "\n",
    "# Print the ratio\n",
    "print(\"For every 1 fatal injury, there are on average\", ratio_incapacitating_to_fatal, \"incapacitating injuries.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the normalized ratios for each injury category\n",
    "fatal_ratios = df_for_insights['INJURIES_FATAL'].value_counts(normalize=True)\n",
    "incapacitating_ratios = df_for_insights['INJURIES_INCAPACITATING'].value_counts(normalize=True)\n",
    "non_incapacitating_ratios = df_for_insights['INJURIES_NON_INCAPACITATING'].value_counts(normalize=True)\n",
    "reported_not_evident_ratios = df_for_insights['INJURIES_REPORTED_NOT_EVIDENT'].value_counts(normalize=True)\n",
    "\n",
    "# Define a function to filter and display ratios\n",
    "def display_filtered_ratios(ratios, category_name):\n",
    "    print(f\"Ratios for {category_name} (only > 0):\")\n",
    "    for level, ratio in ratios.items():\n",
    "        if ratio > 0:\n",
    "            print(f\"Level {level}: {ratio:.4f}\")  # Displaying 4 decimal places for clarity\n",
    "\n",
    "# Display the filtered ratios for each category\n",
    "display_filtered_ratios(fatal_ratios, \"INJURIES_FATAL\")\n",
    "display_filtered_ratios(incapacitating_ratios, \"INJURIES_INCAPACITATING\")\n",
    "display_filtered_ratios(non_incapacitating_ratios, \"INJURIES_NON_INCAPACITATING\")\n",
    "display_filtered_ratios(reported_not_evident_ratios, \"INJURIES_REPORTED_NOT_EVIDENT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution of total injuries\n",
    "df_for_insights['INJURIES_TOTAL'].hist(bins=range(int(df_for_insights['INJURIES_TOTAL'].max()+1)))\n",
    "plt.title('Distribution of Total Injuries per Accident')\n",
    "plt.xlabel('Number of Injuries')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Similarly, you can create histograms for other injury-related features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 10))  # Adjust grid layout as needed\n",
    "features = ['INJURIES_FATAL', 'INJURIES_INCAPACITATING', 'INJURIES_NON_INCAPACITATING', 'INJURIES_REPORTED_NOT_EVIDENT']\n",
    "titles = ['Fatal Injuries', 'Incapacitating Injuries', 'Non-Incapacitating Injuries', 'Reported Not Evident Injuries']\n",
    "\n",
    "for ax, feature, title in zip(axes.flatten(), features, titles):\n",
    "    ax.hist(df_for_insights[feature], bins=range(int(df_for_insights[feature].max() + 1)), color='skyblue', edgecolor='black')\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.set_xlabel('Number of ' + title, fontsize=14)\n",
    "    ax.set_ylabel('Frequency', fontsize=14)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax.grid(axis='y', alpha=0.75)\n",
    "\n",
    "plt.tight_layout()  # Adjust subplots to fit into the figure area.\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example for 'INJURIES_TOTAL' with enhanced visualization\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "plt.hist(df['INJURIES_TOTAL'], bins=range(int(df['INJURIES_TOTAL'].max() + 1)), color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Total Injuries per Accident', fontsize=16)  # Set title with bigger font\n",
    "plt.xlabel('Number of Injuries', fontsize=14)  # Set x-axis label with bigger font\n",
    "plt.ylabel('Frequency', fontsize=14)  # Set y-axis label with bigger font\n",
    "plt.xticks(fontsize=12)  # Set x-ticks with bigger font\n",
    "plt.yticks(fontsize=12)  # Set y-ticks with bigger font\n",
    "plt.grid(axis='y', alpha=0.75)  # Add gridlines for better readability\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example data: 'INJURIES_FATAL'\n",
    "fatal_injuries_counts = df['INJURIES_FATAL'].value_counts().sort_index()\n",
    "\n",
    "# Creating the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(fatal_injuries_counts.index.astype(str), fatal_injuries_counts.values, color='skyblue', edgecolor='black')\n",
    "\n",
    "# Adding value labels on top of each bar\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, yval + 0.05, yval, ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "# Setting chart title and labels\n",
    "plt.title('Distribution of Fatal Injuries per Accident', fontsize=16)\n",
    "plt.xlabel('Number of Fatal Injuries', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "\n",
    "# Setting font size for x and y ticks\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Analyzing injuries by month\n",
    "df_for_insights['CRASH_MONTH'] = pd.to_datetime(df_for_insights['CRASH_DATE']).dt.month\n",
    "monthly_injuries = df.groupby('CRASH_MONTH')['INJURIES_TOTAL'].sum()\n",
    "plt.plot(monthly_injuries.index, monthly_injuries.values)\n",
    "plt.title('Total Injuries by Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Injuries')\n",
    "plt.xticks(range(1, 13))  # Set x-ticks to month numbers\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming df is your DataFrame and it includes 'CRASH_DATE' and 'INJURIES_TOTAL' columns\n",
    "# Convert 'CRASH_DATE' to datetime if it's not already\n",
    "df_for_insights['CRASH_DATE'] = pd.to_datetime(df_for_insights['CRASH_DATE'])\n",
    "\n",
    "# Extract year and month from 'CRASH_DATE'\n",
    "df_for_insights['YEAR'] = df_for_insights['CRASH_DATE'].dt.year\n",
    "df_for_insights['MONTH'] = df_for_insights['CRASH_DATE'].dt.month\n",
    "\n",
    "# Filter data for years 2015 to 2023\n",
    "filtered_df = df_for_insights[(df_for_insights['YEAR'] >= 2015) & (df_for_insights['YEAR'] <= 2023)]\n",
    "\n",
    "# Group by year and month, summing total injuries\n",
    "injuries_by_month_year = filtered_df.groupby(['YEAR', 'MONTH'])['INJURIES_TOTAL'].sum().reset_index()\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = sns.scatterplot(data=injuries_by_month_year, x='MONTH', y='INJURIES_TOTAL', hue='YEAR', palette='Spectral', s=100)\n",
    "\n",
    "# Enhance the plot\n",
    "plt.title('Total Injuries by Month (2015-2023)', fontsize=16)\n",
    "plt.xlabel('Month', fontsize=14)\n",
    "plt.ylabel('Total Injuries', fontsize=14)\n",
    "plt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(title='Year', title_fontsize='13', fontsize='12')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming df is your DataFrame and it includes 'CRASH_DATE' and 'INJURIES_TOTAL' columns\n",
    "# Ensure 'CRASH_DATE' is in datetime format\n",
    "df_for_insights['CRASH_DATE'] = pd.to_datetime(df_for_insights['CRASH_DATE'])\n",
    "\n",
    "# Extract year and month from 'CRASH_DATE'\n",
    "df_for_insights['YEAR'] = df_for_insights['CRASH_DATE'].dt.year\n",
    "df_for_insights['MONTH'] = df_for_insights['CRASH_DATE'].dt.month\n",
    "\n",
    "# Filter data to exclude years 2015 and 2016, and include up to 2023\n",
    "filtered_df = df_for_insights[(df_for_insights['YEAR'] > 2017) & (df_for_insights['YEAR'] <= 2023)]\n",
    "\n",
    "# Group by year and month, summing total injuries\n",
    "injuries_by_month_year = filtered_df.groupby(['YEAR', 'MONTH'])['INJURIES_TOTAL'].sum().reset_index()\n",
    "\n",
    "# Create a line plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "line = sns.lineplot(data=injuries_by_month_year, x='MONTH', y='INJURIES_TOTAL', hue='YEAR', palette='Spectral', marker='o', linewidth=2.5)\n",
    "\n",
    "# Enhance the plot\n",
    "plt.title('Total Injuries by Month (2017-2023)', fontsize=16)\n",
    "plt.xlabel('Month', fontsize=14)\n",
    "plt.ylabel('Total Injuries', fontsize=14)\n",
    "plt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(title='Year', title_fontsize='13', fontsize='12', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.tight_layout()  # Adjust layout to make room for the legend\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sample data loading and preparation steps\n",
    "# Ensure 'CRASH_DATE' is in datetime format\n",
    "df_for_insights['CRASH_DATE'] = pd.to_datetime(df_for_insights['CRASH_DATE'])\n",
    "\n",
    "# Extract year and month from 'CRASH_DATE'\n",
    "df_for_insights['YEAR'] = df_for_insights['CRASH_DATE'].dt.year\n",
    "df_for_insights['MONTH'] = df_for_insights['CRASH_DATE'].dt.strftime('%b')  # Convert month to abbreviated month name\n",
    "\n",
    "# Filter data to include only the desired years, for example, 2017 to 2023\n",
    "filtered_df = df_for_insights[(df_for_insights['YEAR'] >= 2021) & (df_for_insights['YEAR'] <= 2023)]\n",
    "\n",
    "# Group by year and month, summing total fatalities\n",
    "fatalities_by_month_year = filtered_df.groupby(['YEAR', 'MONTH'])['INJURIES_INCAPACITATING'].sum().reset_index()\n",
    "\n",
    "# Pivot the data for plotting\n",
    "pivot_df = fatalities_by_month_year.pivot_table(index='MONTH', columns='YEAR', values='INJURIES_INCAPACITATING', fill_value=0)\n",
    "\n",
    "# Since we want to preserve the month order, we might need to reindex the pivot_df\n",
    "month_order = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "pivot_df = pivot_df.reindex(month_order)\n",
    "\n",
    "# Plot\n",
    "pivot_df.plot(kind='bar', figsize=(14, 8), width=0.8)\n",
    "plt.title('Fatal Traffic Accidents Per Month (2017-2023)', fontsize=16)\n",
    "plt.xlabel('Month', fontsize=14)\n",
    "plt.ylabel('Number of Fatalities', fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(title='Year', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sample data loading and preparation steps\n",
    "# Assuming 'df' is your DataFrame and it includes 'CRASH_DATE' and 'INJURIES_INCAPACITATING' columns\n",
    "# Ensure 'CRASH_DATE' is in datetime format\n",
    "df['CRASH_DATE'] = pd.to_datetime(df['CRASH_DATE'])\n",
    "\n",
    "# Extract year and month from 'CRASH_DATE'\n",
    "df['YEAR'] = df['CRASH_DATE'].dt.year\n",
    "df['MONTH'] = df['CRASH_DATE'].dt.strftime('%b')  # Convert month to abbreviated month name\n",
    "\n",
    "# Filter data to include only the desired years, for example, 2021 to 2023\n",
    "filtered_df = df[(df['YEAR'] >= 2021) & (df['YEAR'] <= 2023)]\n",
    "\n",
    "# Group by year and month, summing total incapacitating injuries\n",
    "incapacitating_injuries_by_month_year = filtered_df.groupby(['YEAR', 'MONTH'])['INJURIES_INCAPACITATING'].sum().reset_index()\n",
    "\n",
    "# Pivot the data for plotting\n",
    "pivot_df = incapacitating_injuries_by_month_year.pivot_table(index='MONTH', columns='YEAR', values='INJURIES_INCAPACITATING', fill_value=0)\n",
    "\n",
    "# Since we want to preserve the month order, we might need to reindex the pivot_df\n",
    "month_order = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "pivot_df = pivot_df.reindex(month_order)\n",
    "\n",
    "# Plot\n",
    "pivot_df.plot(kind='bar', figsize=(14, 8), width=0.8)\n",
    "plt.title('Incapacitating Traffic Injuries Per Month (2021-2023)', fontsize=16)\n",
    "plt.xlabel('Month', fontsize=14)\n",
    "plt.ylabel('Number of Incapacitating Injuries', fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(title='Year', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame loading and grouping (replace with your actual DataFrame variable)\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "fatalities_per_day = df_for_insights.groupby('CRASH_DAY_OF_WEEK')['INJURIES_FATAL'].sum()\n",
    "\n",
    "# Days of the week labels\n",
    "days_of_the_week = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "\n",
    "# Creating the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(days_of_the_week, fatalities_per_day, color='tomato')\n",
    "plt.title('Fatal Traffic Accidents by Day of the Week', fontsize=16)\n",
    "plt.xlabel('Day of the Week', fontsize=14)\n",
    "plt.ylabel('Number of Fatalities', fontsize=14)\n",
    "plt.xticks(fontsize=12, rotation=45)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame is named df and it includes 'CRASH_HOUR' and 'INJURIES_FATAL' columns\n",
    "# Note: Replace 'your_dataset.csv' with the actual path to your dataset\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Grouping the data by 'CRASH_HOUR' and summing 'INJURIES_FATAL' to get total fatalities per hour\n",
    "fatalities_per_hour = df_for_insights.groupby('CRASH_HOUR')['INJURIES_FATAL'].sum()\n",
    "\n",
    "# Creating the bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(fatalities_per_hour.index, fatalities_per_hour.values, color='royalblue', edgecolor='black')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.title('Fatal Traffic Accidents by Hour of the Day', fontsize=16)\n",
    "plt.xlabel('Hour of the Day (0-23)', fontsize=14)\n",
    "plt.ylabel('Number of Fatalities', fontsize=14)\n",
    "\n",
    "# Ensuring the x-axis covers all hours\n",
    "plt.xticks(range(0, 24), fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame is named df and includes 'CRASH_HOUR', 'INJURIES_FATAL', and 'INJURIES_INCAPACITATING' columns\n",
    "# Note: Replace 'your_dataset.csv' with the actual path to your dataset\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Group the data by 'CRASH_HOUR' and calculate the sum for both 'INJURIES_FATAL' and 'INJURIES_INCAPACITATING'\n",
    "grouped_data = df_for_insights.groupby('CRASH_HOUR')[['INJURIES_FATAL', 'INJURIES_INCAPACITATING']].sum().reset_index()\n",
    "\n",
    "# Setting the positions and width for the bars\n",
    "pos = list(range(len(grouped_data['CRASH_HOUR'])))\n",
    "width = 0.4\n",
    "\n",
    "# Plotting the bars\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Create a bar with 'INJURIES_FATAL' data,\n",
    "# in position pos,\n",
    "plt.bar(pos, \n",
    "        grouped_data['INJURIES_FATAL'], \n",
    "        width, \n",
    "        alpha=0.7, \n",
    "        color='red', \n",
    "        label='Fatal Injuries')\n",
    "\n",
    "# Create a bar with 'INJURIES_INCAPACITATING' data,\n",
    "# in position pos + some width buffer,\n",
    "plt.bar([p + width for p in pos], \n",
    "        grouped_data['INJURIES_INCAPACITATING'],\n",
    "        width, \n",
    "        alpha=0.7, \n",
    "        color='blue', \n",
    "        label='Incapacitating Injuries')\n",
    "\n",
    "# Set the y-axis label\n",
    "ax.set_ylabel('Number of Injuries')\n",
    "\n",
    "# Set the chart's title\n",
    "ax.set_title('Fatal and Incapacitating Injuries by Hour of the Day')\n",
    "\n",
    "# Set the position of the x ticks\n",
    "ax.set_xticks([p + width / 2 for p in pos])\n",
    "\n",
    "# Set the labels for the x ticks\n",
    "ax.set_xticklabels(grouped_data['CRASH_HOUR'])\n",
    "\n",
    "# Setting the x-axis and y-axis limits\n",
    "plt.xlim(min(pos)-width, max(pos)+width*2)\n",
    "plt.ylim([0, max(grouped_data['INJURIES_FATAL'].max(), grouped_data['INJURIES_INCAPACITATING'].max())] )\n",
    "\n",
    "# Adding the legend and showing the plot\n",
    "plt.legend(['Fatal Injuries', 'Incapacitating Injuries'], loc='upper left')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample DataFrame loading (replace with actual loading code)\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "df_for_insights['CRASH_DATE'] = pd.to_datetime(df['CRASH_DATE'])\n",
    "\n",
    "# Define holidays for each year\n",
    "holidays = {\n",
    "    'New Year\\'s Day': {year: pd.Timestamp(f'{year}-01-01') for year in range(2015, 2024)},\n",
    "    'Independence Day': {year: pd.Timestamp(f'{year}-07-04') for year in range(2015, 2024)},\n",
    "    'Thanksgiving Day': {\n",
    "        2015: pd.Timestamp('2015-11-26'),\n",
    "        2016: pd.Timestamp('2016-11-24'),\n",
    "        2017: pd.Timestamp('2017-11-23'),\n",
    "        2018: pd.Timestamp('2018-11-22'),\n",
    "        2019: pd.Timestamp('2019-11-28'),\n",
    "        2020: pd.Timestamp('2020-11-26'),\n",
    "        2021: pd.Timestamp('2021-11-25'),\n",
    "        2022: pd.Timestamp('2022-11-24'),\n",
    "        2023: pd.Timestamp('2023-11-23'),\n",
    "    },\n",
    "    'Christmas Day': {year: pd.Timestamp(f'{year}-12-25') for year in range(2015, 2024)},\n",
    "}\n",
    "\n",
    "# Initialize a list to store results\n",
    "results = []\n",
    "\n",
    "# Calculate INJURIES_FATAL for each holiday and year\n",
    "for holiday_name, dates in holidays.items():\n",
    "    for year, date in dates.items():\n",
    "        fatal_injuries = df_for_insights[(df_for_insights['CRASH_DATE'].dt.date == date.date())]['INJURIES_FATAL'].sum()\n",
    "        results.append({'Holiday': holiday_name, 'Year': year, 'Fatal Injuries': fatal_injuries})\n",
    "\n",
    "# Convert results to a DataFrame for easier analysis and plotting\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Pivot the results for plotting\n",
    "# pivot_results = results_df.pivot(\"Year\", \"Holiday\", \"Fatal Injuries\")\n",
    "pivot_results = results_df.pivot_table(index='Year', columns='Holiday', values='Fatal Injuries', fill_value=0)\n",
    "\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.title('Fatal Injuries During Holidays (2015-2023)', fontsize=16)\n",
    "sns.heatmap(pivot_results, annot=True, cmap=\"YlGnBu\", fmt=\".0f\", linewidths=.5)\n",
    "plt.xlabel('Holiday', fontsize=14)\n",
    "plt.ylabel('Year', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311_pods_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
