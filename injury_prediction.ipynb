{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">Predict Car Traffic Injury</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this document we follow the CRISP-DM process and methodolgies...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CRISP-DM: Business Understanding\n",
    "\n",
    "bla bla bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 2. CRISP-DM: Data Understanding\n",
    "\n",
    "In this section, the following will be addressed:<br><br>\n",
    "**2.1. Collect Initial Data**<br>\n",
    "2.1.1. Import the relevant Python packages that are going to be used.<br>\n",
    "2.1.2. Acquire the data.<br>\n",
    "2.1.3. Record of the data acquisition process.<br>\n",
    "2.2.4. HTML Reprot.<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**2.2. Data Description**<br>\n",
    "2.2.1. Shape of the Dataset.<br>\n",
    "2.2.2. Head snippet.<br>\n",
    "2.2.3. Dataset info.<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**2.3. Explore the Data**<br>\n",
    "2.3.1. Basic Statistics - For numeric models, to calculate Mean, Median, mode, ... <br>\n",
    "2.3.2. Tabular Report - For Continuos and Categorical Features (Refer to HTML Reports please). <br>\n",
    "2.3.3. Correlations + Heat Map.<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**2.4. Verify Data Quality**<br>\n",
    "2.4.1. Completnesss - Missing Values Summary.<br>\n",
    "2.4.2. Irregular cardinality, (1, too high for categorical, too low for continuos)<br>\n",
    "2.4.3. Consistency - Handle outliers, out of range data or invalid formats (if any).<br>\n",
    "2.4.4. Uniqueness - Remove duplicated or irrelevant repetition in data.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 2.1. Collect Initial Data\n",
    "\n",
    "\n",
    "This a generic intial step, to start working on the data:<br>\n",
    "2.1.1. Import the relevant Python packages that are going to be used.<br>\n",
    "2.1.2. Acquire the data.<br>\n",
    "2.1.3. Record of the data acquisition process.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1.1. Import the relevant Python packages that are going to be used\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from ydata_profiling import ProfileReport\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists...\n"
     ]
    }
   ],
   "source": [
    "# 2.1.2. Acquire the data\n",
    "\n",
    "# A generic utilies file with generic functionallity\n",
    "from misc.utilities import acquire_dataset\n",
    "\n",
    "# Download & load the dataset\n",
    "# If the file already downloaded, it will not be re-downloaded (to speed up work efficiency)\n",
    "df = acquire_dataset()\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "\n",
    "2.1.3. Record of the data acquisition process.<br>\n",
    "\n",
    "The data acquired from: https://catalog.data.gov/dataset/traffic-crashes-crashes\n",
    "<br>\n",
    "The above website belongs to USA government free datasets advised by the course instructions.\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 2.2. Data Description\n",
    "\n",
    "In this phase we investigate the following aspects:<br>\n",
    "2.2.1. Shape of the Dataset.<br>\n",
    "2.2.2. Head snippet.<br>\n",
    "2.2.3. Dataset info.<br>\n",
    "2.2.4. HTML Reprot.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Records (Instances)</td>\n",
       "      <td>814788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Features (Columns)</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Aspect  Number\n",
       "0  Records (Instances)  814788\n",
       "1   Features (Columns)      48"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.2.1. Shape of the Dataset.\n",
    "\n",
    "def get_dataset_shape(df):\n",
    "    # Create a new DataFrame to display the shape information\n",
    "    shape_df = pd.DataFrame({\n",
    "        'Aspect': ['Records (Instances)', 'Features (Columns)'],\n",
    "        'Number': [df.shape[0], df.shape[1]]\n",
    "    })\n",
    "    return shape_df\n",
    "\n",
    "# Display the DataFrame\n",
    "get_dataset_shape(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2.2. Head snippet\n",
    "\n",
    "def get_dataset_head(df):\n",
    "    df.head()\n",
    "\n",
    "get_dataset_head(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2.3. Dataset info\n",
    "\n",
    "def get_dataset_info(df):\n",
    "    df.info()\n",
    "\n",
    "get_dataset_info(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2.4. Pandas-HTML Profiling Reprot.\n",
    "\n",
    "# profile = ProfileReport(df, title=\"Pandas Profiling Report\",explorative=True)\n",
    "# profile.to_file(\"pandas_report.html\")\n",
    "\n",
    "# Disable this code in production code - it requires a strong PC to be to run it\n",
    "# or to run it with smaller / configured set of profiling - please let me know if to share the report.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 2.3. Explore the Data\n",
    "\n",
    "2.3.1. Basic Statistics - For numeric models, to calculate Mean, Median, mode, ... <br>\n",
    "2.3.2. Tabular Report - For Continuos and Categorical Features (Refer to HTML Reports please). <br>\n",
    "2.3.3. Correlations + Heat Map.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3.1. Continuous Features Tabular Report\n",
    "\n",
    "def get_continuous_features_tabular_report(df):\n",
    "    # continuous_features_tabular_report_df = df.describe(include=['number'])\n",
    "    return df.describe(include=['number'])\n",
    "\n",
    "# Transpose - More friendly print\n",
    "continuous_features_transpose_tabular_report_df = get_continuous_features_tabular_report(df).transpose()\n",
    "\n",
    "continuous_features_transpose_tabular_report_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3.2. Categorical Features Tabular Report\n",
    "\n",
    "def get_categorical_features_tabular_report(df):\n",
    "    return df.describe(exclude=['number'])\n",
    "\n",
    "# Transpose - More friendly print\n",
    "categorical_features_transpose_tabular_report_df = get_categorical_features_tabular_report(df).transpose()\n",
    "\n",
    "categorical_features_transpose_tabular_report_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POSTED_SPEED_LIMIT</th>\n",
       "      <th>LANE_CNT</th>\n",
       "      <th>STREET_NO</th>\n",
       "      <th>BEAT_OF_OCCURRENCE</th>\n",
       "      <th>NUM_UNITS</th>\n",
       "      <th>INJURIES_TOTAL</th>\n",
       "      <th>INJURIES_FATAL</th>\n",
       "      <th>INJURIES_INCAPACITATING</th>\n",
       "      <th>INJURIES_NON_INCAPACITATING</th>\n",
       "      <th>INJURIES_REPORTED_NOT_EVIDENT</th>\n",
       "      <th>INJURIES_NO_INDICATION</th>\n",
       "      <th>INJURIES_UNKNOWN</th>\n",
       "      <th>CRASH_HOUR</th>\n",
       "      <th>CRASH_DAY_OF_WEEK</th>\n",
       "      <th>CRASH_MONTH</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>POSTED_SPEED_LIMIT</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>-0.019587</td>\n",
       "      <td>-0.035460</td>\n",
       "      <td>0.047738</td>\n",
       "      <td>0.076964</td>\n",
       "      <td>0.007388</td>\n",
       "      <td>0.029120</td>\n",
       "      <td>0.059093</td>\n",
       "      <td>0.042361</td>\n",
       "      <td>0.101507</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014174</td>\n",
       "      <td>0.008060</td>\n",
       "      <td>0.010672</td>\n",
       "      <td>-0.003566</td>\n",
       "      <td>0.006602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LANE_CNT</th>\n",
       "      <td>0.000979</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.000889</td>\n",
       "      <td>0.003390</td>\n",
       "      <td>-0.000278</td>\n",
       "      <td>-0.001129</td>\n",
       "      <td>-0.000102</td>\n",
       "      <td>-0.000415</td>\n",
       "      <td>-0.000838</td>\n",
       "      <td>-0.000672</td>\n",
       "      <td>-0.001819</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.002889</td>\n",
       "      <td>-0.001943</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.000092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STREET_NO</th>\n",
       "      <td>-0.019587</td>\n",
       "      <td>-0.000889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.008733</td>\n",
       "      <td>0.007034</td>\n",
       "      <td>0.010944</td>\n",
       "      <td>0.006430</td>\n",
       "      <td>0.003728</td>\n",
       "      <td>0.008295</td>\n",
       "      <td>0.005750</td>\n",
       "      <td>-0.038327</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.000383</td>\n",
       "      <td>-0.008104</td>\n",
       "      <td>-0.000668</td>\n",
       "      <td>-0.074045</td>\n",
       "      <td>-0.011236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BEAT_OF_OCCURRENCE</th>\n",
       "      <td>-0.035460</td>\n",
       "      <td>0.003390</td>\n",
       "      <td>-0.008733</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.021971</td>\n",
       "      <td>-0.038895</td>\n",
       "      <td>-0.007791</td>\n",
       "      <td>-0.014683</td>\n",
       "      <td>-0.032311</td>\n",
       "      <td>-0.017710</td>\n",
       "      <td>-0.009602</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006597</td>\n",
       "      <td>0.003654</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.153892</td>\n",
       "      <td>-0.042504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM_UNITS</th>\n",
       "      <td>0.047738</td>\n",
       "      <td>-0.000278</td>\n",
       "      <td>0.007034</td>\n",
       "      <td>0.021971</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.108469</td>\n",
       "      <td>0.008572</td>\n",
       "      <td>0.038248</td>\n",
       "      <td>0.083439</td>\n",
       "      <td>0.061154</td>\n",
       "      <td>0.169555</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005129</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.010064</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>-0.002546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INJURIES_TOTAL</th>\n",
       "      <td>0.076964</td>\n",
       "      <td>-0.001129</td>\n",
       "      <td>0.010944</td>\n",
       "      <td>-0.038895</td>\n",
       "      <td>0.108469</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.098466</td>\n",
       "      <td>0.338678</td>\n",
       "      <td>0.767255</td>\n",
       "      <td>0.571369</td>\n",
       "      <td>-0.184474</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002474</td>\n",
       "      <td>-0.005047</td>\n",
       "      <td>0.012008</td>\n",
       "      <td>-0.016592</td>\n",
       "      <td>0.006440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INJURIES_FATAL</th>\n",
       "      <td>0.007388</td>\n",
       "      <td>-0.000102</td>\n",
       "      <td>0.006430</td>\n",
       "      <td>-0.007791</td>\n",
       "      <td>0.008572</td>\n",
       "      <td>0.098466</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.046947</td>\n",
       "      <td>0.022412</td>\n",
       "      <td>0.003546</td>\n",
       "      <td>-0.030791</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.006175</td>\n",
       "      <td>-0.002200</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>-0.002671</td>\n",
       "      <td>0.000127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INJURIES_INCAPACITATING</th>\n",
       "      <td>0.029120</td>\n",
       "      <td>-0.000415</td>\n",
       "      <td>0.003728</td>\n",
       "      <td>-0.014683</td>\n",
       "      <td>0.038248</td>\n",
       "      <td>0.338678</td>\n",
       "      <td>0.046947</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.053584</td>\n",
       "      <td>0.006166</td>\n",
       "      <td>-0.082205</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.004742</td>\n",
       "      <td>-0.002863</td>\n",
       "      <td>0.005132</td>\n",
       "      <td>-0.002838</td>\n",
       "      <td>0.000346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INJURIES_NON_INCAPACITATING</th>\n",
       "      <td>0.059093</td>\n",
       "      <td>-0.000838</td>\n",
       "      <td>0.008295</td>\n",
       "      <td>-0.032311</td>\n",
       "      <td>0.083439</td>\n",
       "      <td>0.767255</td>\n",
       "      <td>0.022412</td>\n",
       "      <td>0.053584</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007304</td>\n",
       "      <td>-0.149288</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>-0.004966</td>\n",
       "      <td>0.009950</td>\n",
       "      <td>-0.009233</td>\n",
       "      <td>0.001703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INJURIES_REPORTED_NOT_EVIDENT</th>\n",
       "      <td>0.042361</td>\n",
       "      <td>-0.000672</td>\n",
       "      <td>0.005750</td>\n",
       "      <td>-0.017710</td>\n",
       "      <td>0.061154</td>\n",
       "      <td>0.571369</td>\n",
       "      <td>0.003546</td>\n",
       "      <td>0.006166</td>\n",
       "      <td>0.007304</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.083452</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006322</td>\n",
       "      <td>-0.000640</td>\n",
       "      <td>0.005282</td>\n",
       "      <td>-0.015423</td>\n",
       "      <td>0.008974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INJURIES_NO_INDICATION</th>\n",
       "      <td>0.101507</td>\n",
       "      <td>-0.001819</td>\n",
       "      <td>-0.038327</td>\n",
       "      <td>-0.009602</td>\n",
       "      <td>0.169555</td>\n",
       "      <td>-0.184474</td>\n",
       "      <td>-0.030791</td>\n",
       "      <td>-0.082205</td>\n",
       "      <td>-0.149288</td>\n",
       "      <td>-0.083452</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.081208</td>\n",
       "      <td>0.018476</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>0.001407</td>\n",
       "      <td>0.000955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INJURIES_UNKNOWN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRASH_HOUR</th>\n",
       "      <td>0.014174</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>-0.000383</td>\n",
       "      <td>0.006597</td>\n",
       "      <td>0.005129</td>\n",
       "      <td>0.002474</td>\n",
       "      <td>-0.006175</td>\n",
       "      <td>-0.004742</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>0.006322</td>\n",
       "      <td>0.081208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.061556</td>\n",
       "      <td>0.003758</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>0.001354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRASH_DAY_OF_WEEK</th>\n",
       "      <td>0.008060</td>\n",
       "      <td>0.002889</td>\n",
       "      <td>-0.008104</td>\n",
       "      <td>0.003654</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>-0.005047</td>\n",
       "      <td>-0.002200</td>\n",
       "      <td>-0.002863</td>\n",
       "      <td>-0.004966</td>\n",
       "      <td>-0.000640</td>\n",
       "      <td>0.018476</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.061556</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.001676</td>\n",
       "      <td>0.001337</td>\n",
       "      <td>-0.000301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRASH_MONTH</th>\n",
       "      <td>0.010672</td>\n",
       "      <td>-0.001943</td>\n",
       "      <td>-0.000668</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.010064</td>\n",
       "      <td>0.012008</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>0.005132</td>\n",
       "      <td>0.009950</td>\n",
       "      <td>0.005282</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003758</td>\n",
       "      <td>-0.001676</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001377</td>\n",
       "      <td>-0.000791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LATITUDE</th>\n",
       "      <td>-0.003566</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>-0.074045</td>\n",
       "      <td>0.153892</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>-0.016592</td>\n",
       "      <td>-0.002671</td>\n",
       "      <td>-0.002838</td>\n",
       "      <td>-0.009233</td>\n",
       "      <td>-0.015423</td>\n",
       "      <td>0.001407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>0.001337</td>\n",
       "      <td>0.001377</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.973608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LONGITUDE</th>\n",
       "      <td>0.006602</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>-0.011236</td>\n",
       "      <td>-0.042504</td>\n",
       "      <td>-0.002546</td>\n",
       "      <td>0.006440</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.001703</td>\n",
       "      <td>0.008974</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>-0.000301</td>\n",
       "      <td>-0.000791</td>\n",
       "      <td>-0.973608</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               POSTED_SPEED_LIMIT  LANE_CNT  STREET_NO  \\\n",
       "POSTED_SPEED_LIMIT                       1.000000  0.000979  -0.019587   \n",
       "LANE_CNT                                 0.000979  1.000000  -0.000889   \n",
       "STREET_NO                               -0.019587 -0.000889   1.000000   \n",
       "BEAT_OF_OCCURRENCE                      -0.035460  0.003390  -0.008733   \n",
       "NUM_UNITS                                0.047738 -0.000278   0.007034   \n",
       "INJURIES_TOTAL                           0.076964 -0.001129   0.010944   \n",
       "INJURIES_FATAL                           0.007388 -0.000102   0.006430   \n",
       "INJURIES_INCAPACITATING                  0.029120 -0.000415   0.003728   \n",
       "INJURIES_NON_INCAPACITATING              0.059093 -0.000838   0.008295   \n",
       "INJURIES_REPORTED_NOT_EVIDENT            0.042361 -0.000672   0.005750   \n",
       "INJURIES_NO_INDICATION                   0.101507 -0.001819  -0.038327   \n",
       "INJURIES_UNKNOWN                              NaN       NaN        NaN   \n",
       "CRASH_HOUR                               0.014174  0.000268  -0.000383   \n",
       "CRASH_DAY_OF_WEEK                        0.008060  0.002889  -0.008104   \n",
       "CRASH_MONTH                              0.010672 -0.001943  -0.000668   \n",
       "LATITUDE                                -0.003566  0.000833  -0.074045   \n",
       "LONGITUDE                                0.006602  0.000092  -0.011236   \n",
       "\n",
       "                               BEAT_OF_OCCURRENCE  NUM_UNITS  INJURIES_TOTAL  \\\n",
       "POSTED_SPEED_LIMIT                      -0.035460   0.047738        0.076964   \n",
       "LANE_CNT                                 0.003390  -0.000278       -0.001129   \n",
       "STREET_NO                               -0.008733   0.007034        0.010944   \n",
       "BEAT_OF_OCCURRENCE                       1.000000   0.021971       -0.038895   \n",
       "NUM_UNITS                                0.021971   1.000000        0.108469   \n",
       "INJURIES_TOTAL                          -0.038895   0.108469        1.000000   \n",
       "INJURIES_FATAL                          -0.007791   0.008572        0.098466   \n",
       "INJURIES_INCAPACITATING                 -0.014683   0.038248        0.338678   \n",
       "INJURIES_NON_INCAPACITATING             -0.032311   0.083439        0.767255   \n",
       "INJURIES_REPORTED_NOT_EVIDENT           -0.017710   0.061154        0.571369   \n",
       "INJURIES_NO_INDICATION                  -0.009602   0.169555       -0.184474   \n",
       "INJURIES_UNKNOWN                              NaN        NaN             NaN   \n",
       "CRASH_HOUR                               0.006597   0.005129        0.002474   \n",
       "CRASH_DAY_OF_WEEK                        0.003654   0.000960       -0.005047   \n",
       "CRASH_MONTH                              0.000440   0.010064        0.012008   \n",
       "LATITUDE                                 0.153892   0.003436       -0.016592   \n",
       "LONGITUDE                               -0.042504  -0.002546        0.006440   \n",
       "\n",
       "                               INJURIES_FATAL  INJURIES_INCAPACITATING  \\\n",
       "POSTED_SPEED_LIMIT                   0.007388                 0.029120   \n",
       "LANE_CNT                            -0.000102                -0.000415   \n",
       "STREET_NO                            0.006430                 0.003728   \n",
       "BEAT_OF_OCCURRENCE                  -0.007791                -0.014683   \n",
       "NUM_UNITS                            0.008572                 0.038248   \n",
       "INJURIES_TOTAL                       0.098466                 0.338678   \n",
       "INJURIES_FATAL                       1.000000                 0.046947   \n",
       "INJURIES_INCAPACITATING              0.046947                 1.000000   \n",
       "INJURIES_NON_INCAPACITATING          0.022412                 0.053584   \n",
       "INJURIES_REPORTED_NOT_EVIDENT        0.003546                 0.006166   \n",
       "INJURIES_NO_INDICATION              -0.030791                -0.082205   \n",
       "INJURIES_UNKNOWN                          NaN                      NaN   \n",
       "CRASH_HOUR                          -0.006175                -0.004742   \n",
       "CRASH_DAY_OF_WEEK                   -0.002200                -0.002863   \n",
       "CRASH_MONTH                          0.001628                 0.005132   \n",
       "LATITUDE                            -0.002671                -0.002838   \n",
       "LONGITUDE                            0.000127                 0.000346   \n",
       "\n",
       "                               INJURIES_NON_INCAPACITATING  \\\n",
       "POSTED_SPEED_LIMIT                                0.059093   \n",
       "LANE_CNT                                         -0.000838   \n",
       "STREET_NO                                         0.008295   \n",
       "BEAT_OF_OCCURRENCE                               -0.032311   \n",
       "NUM_UNITS                                         0.083439   \n",
       "INJURIES_TOTAL                                    0.767255   \n",
       "INJURIES_FATAL                                    0.022412   \n",
       "INJURIES_INCAPACITATING                           0.053584   \n",
       "INJURIES_NON_INCAPACITATING                       1.000000   \n",
       "INJURIES_REPORTED_NOT_EVIDENT                     0.007304   \n",
       "INJURIES_NO_INDICATION                           -0.149288   \n",
       "INJURIES_UNKNOWN                                       NaN   \n",
       "CRASH_HOUR                                        0.000942   \n",
       "CRASH_DAY_OF_WEEK                                -0.004966   \n",
       "CRASH_MONTH                                       0.009950   \n",
       "LATITUDE                                         -0.009233   \n",
       "LONGITUDE                                         0.001703   \n",
       "\n",
       "                               INJURIES_REPORTED_NOT_EVIDENT  \\\n",
       "POSTED_SPEED_LIMIT                                  0.042361   \n",
       "LANE_CNT                                           -0.000672   \n",
       "STREET_NO                                           0.005750   \n",
       "BEAT_OF_OCCURRENCE                                 -0.017710   \n",
       "NUM_UNITS                                           0.061154   \n",
       "INJURIES_TOTAL                                      0.571369   \n",
       "INJURIES_FATAL                                      0.003546   \n",
       "INJURIES_INCAPACITATING                             0.006166   \n",
       "INJURIES_NON_INCAPACITATING                         0.007304   \n",
       "INJURIES_REPORTED_NOT_EVIDENT                       1.000000   \n",
       "INJURIES_NO_INDICATION                             -0.083452   \n",
       "INJURIES_UNKNOWN                                         NaN   \n",
       "CRASH_HOUR                                          0.006322   \n",
       "CRASH_DAY_OF_WEEK                                  -0.000640   \n",
       "CRASH_MONTH                                         0.005282   \n",
       "LATITUDE                                           -0.015423   \n",
       "LONGITUDE                                           0.008974   \n",
       "\n",
       "                               INJURIES_NO_INDICATION  INJURIES_UNKNOWN  \\\n",
       "POSTED_SPEED_LIMIT                           0.101507               NaN   \n",
       "LANE_CNT                                    -0.001819               NaN   \n",
       "STREET_NO                                   -0.038327               NaN   \n",
       "BEAT_OF_OCCURRENCE                          -0.009602               NaN   \n",
       "NUM_UNITS                                    0.169555               NaN   \n",
       "INJURIES_TOTAL                              -0.184474               NaN   \n",
       "INJURIES_FATAL                              -0.030791               NaN   \n",
       "INJURIES_INCAPACITATING                     -0.082205               NaN   \n",
       "INJURIES_NON_INCAPACITATING                 -0.149288               NaN   \n",
       "INJURIES_REPORTED_NOT_EVIDENT               -0.083452               NaN   \n",
       "INJURIES_NO_INDICATION                       1.000000               NaN   \n",
       "INJURIES_UNKNOWN                                  NaN               NaN   \n",
       "CRASH_HOUR                                   0.081208               NaN   \n",
       "CRASH_DAY_OF_WEEK                            0.018476               NaN   \n",
       "CRASH_MONTH                                  0.001519               NaN   \n",
       "LATITUDE                                     0.001407               NaN   \n",
       "LONGITUDE                                    0.000955               NaN   \n",
       "\n",
       "                               CRASH_HOUR  CRASH_DAY_OF_WEEK  CRASH_MONTH  \\\n",
       "POSTED_SPEED_LIMIT               0.014174           0.008060     0.010672   \n",
       "LANE_CNT                         0.000268           0.002889    -0.001943   \n",
       "STREET_NO                       -0.000383          -0.008104    -0.000668   \n",
       "BEAT_OF_OCCURRENCE               0.006597           0.003654     0.000440   \n",
       "NUM_UNITS                        0.005129           0.000960     0.010064   \n",
       "INJURIES_TOTAL                   0.002474          -0.005047     0.012008   \n",
       "INJURIES_FATAL                  -0.006175          -0.002200     0.001628   \n",
       "INJURIES_INCAPACITATING         -0.004742          -0.002863     0.005132   \n",
       "INJURIES_NON_INCAPACITATING      0.000942          -0.004966     0.009950   \n",
       "INJURIES_REPORTED_NOT_EVIDENT    0.006322          -0.000640     0.005282   \n",
       "INJURIES_NO_INDICATION           0.081208           0.018476     0.001519   \n",
       "INJURIES_UNKNOWN                      NaN                NaN          NaN   \n",
       "CRASH_HOUR                       1.000000           0.061556     0.003758   \n",
       "CRASH_DAY_OF_WEEK                0.061556           1.000000    -0.001676   \n",
       "CRASH_MONTH                      0.003758          -0.001676     1.000000   \n",
       "LATITUDE                         0.001244           0.001337     0.001377   \n",
       "LONGITUDE                        0.001354          -0.000301    -0.000791   \n",
       "\n",
       "                               LATITUDE  LONGITUDE  \n",
       "POSTED_SPEED_LIMIT            -0.003566   0.006602  \n",
       "LANE_CNT                       0.000833   0.000092  \n",
       "STREET_NO                     -0.074045  -0.011236  \n",
       "BEAT_OF_OCCURRENCE             0.153892  -0.042504  \n",
       "NUM_UNITS                      0.003436  -0.002546  \n",
       "INJURIES_TOTAL                -0.016592   0.006440  \n",
       "INJURIES_FATAL                -0.002671   0.000127  \n",
       "INJURIES_INCAPACITATING       -0.002838   0.000346  \n",
       "INJURIES_NON_INCAPACITATING   -0.009233   0.001703  \n",
       "INJURIES_REPORTED_NOT_EVIDENT -0.015423   0.008974  \n",
       "INJURIES_NO_INDICATION         0.001407   0.000955  \n",
       "INJURIES_UNKNOWN                    NaN        NaN  \n",
       "CRASH_HOUR                     0.001244   0.001354  \n",
       "CRASH_DAY_OF_WEEK              0.001337  -0.000301  \n",
       "CRASH_MONTH                    0.001377  -0.000791  \n",
       "LATITUDE                       1.000000  -0.973608  \n",
       "LONGITUDE                     -0.973608   1.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.3.3. Correlation insights + HeatMap\n",
    "\n",
    "def get_correlation_insights(df):\n",
    "    # # Get the relevant columns\n",
    "    subset_df = df[get_continuous_features_tabular_report(df).columns]\n",
    "\n",
    "    # Calculate the correlation matrix\n",
    "    # correlation_matrix = subset_df.corr()\n",
    "    return subset_df.corr()\n",
    "\n",
    "correlation_matrix = get_correlation_insights(df)\n",
    "correlation_matrix\n",
    "\n",
    "# # Create a heatmap using seaborn\n",
    "# plt.figure(figsize=(16, 14))\n",
    "# sns.heatmap(correlation_matrix, cmap='coolwarm', annot=True, fmt=\".2f\", linewidths=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3.4. HeatMap\n",
    "\n",
    "def draw_heatmap(correlation_matrix):\n",
    "    # Create a heatmap using seaborn\n",
    "    plt.figure(figsize=(16, 14))\n",
    "    return sns.heatmap(correlation_matrix, cmap='coolwarm', annot=True, fmt=\".2f\", linewidths=0.8)\n",
    "\n",
    "draw_heatmap(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 2.4. Verify Data Quality\n",
    "\n",
    "2.4.1. Completnesss - Missing Values Summary.<br>\n",
    "2.4.2. Irregular cardinality, could be one of the following cases:<br>\n",
    "* Features with a cardinality of 1.\n",
    "* Too high cardinality for categorical features.\n",
    "* Too low cardinality for continous features.<br>\n",
    "\n",
    "2.4.3. Consistency - Handle outliers, out of range data or invalid formats (if any).<br>\n",
    "2.4.4. Uniqueness - Remove duplicated or irrelevant repetition in data.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.1. Completnesss - Missing Values Summary\n",
    "\n",
    "# Get missing values details\n",
    "# Retrun value per feature as a numeric value and a percentage\n",
    "def get_missing_values_details(df):\n",
    "    # Check for missing values in each column\n",
    "    missing_values = df.isnull().sum()\n",
    "\n",
    "    # Check for missing values in the whole DataFrame\n",
    "    total_missing = missing_values.sum()\n",
    "    missing_values_sorted = missing_values.sort_values(ascending=False)\n",
    "\n",
    "    # Check the percentage of missing values for each column\n",
    "    missing_percentage_sorted = (missing_values_sorted / len(df)) * 100\n",
    "    return missing_values_sorted, missing_percentage_sorted\n",
    "\n",
    "\n",
    "def get_completeness_report(df):\n",
    "    missing_values_sorted, missing_percentage_sorted = get_missing_values_details(df)\n",
    "    # Create a DataFrame to summarize the completeness\n",
    "    completeness_report = pd.DataFrame({\n",
    "        'Missing Values': missing_values_sorted,\n",
    "        'Missing Percentage': missing_percentage_sorted\n",
    "    })\n",
    "    return completeness_report\n",
    "\n",
    "get_completeness_report(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>INJURIES_UNKNOWN</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "INJURIES_UNKNOWN  1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.4.2. Irregular cardinality - Features with a cardinality of 1\n",
    "\n",
    "IRREGULAR_CARD_SINGLE = 1             # To detect features with cardinality of 1\n",
    "\n",
    "def get_irregular_cardinality_with_single_cardinality(df):\n",
    "    unique_counts = df.nunique()\n",
    "    # for feature_name, count in unique_counts.items():\n",
    "    #     if count == 1:\n",
    "    #         print(f\"The feature '{feature_name}' has a unique count of 1 - Irregular Cardinality Issue\")\n",
    "    unique_counts_single_threshold = unique_counts[unique_counts == IRREGULAR_CARD_SINGLE]\n",
    "    return pd.DataFrame(unique_counts_single_threshold)\n",
    "\n",
    "\n",
    "get_irregular_cardinality_with_single_cardinality(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Features with carinality higher than 10 cardinality are: ['CRASH_RECORD_ID', 'CRASH_DATE', 'TRAFFIC_CONTROL_DEVICE', 'WEATHER_CONDITION', 'FIRST_CRASH_TYPE', 'TRAFFICWAY_TYPE', 'DATE_POLICE_NOTIFIED', 'PRIM_CONTRIBUTORY_CAUSE', 'SEC_CONTRIBUTORY_CAUSE', 'STREET_NAME', 'LOCATION']\n"
     ]
    }
   ],
   "source": [
    "# 2.4.2. Irregular cardinality - Too high cardinality for categorical features\n",
    "\n",
    "IRREGULAR_CARD_MAX_THRESHOLD = 10     # To detect categorical features with high cardinality\n",
    "\n",
    "def get_unique_counts_exceed_max_threshold_categorical(df):\n",
    "    cardinality = df.select_dtypes(exclude=['number']).apply(lambda x: x.nunique())\n",
    "    high_cardinality_cols = cardinality[cardinality > IRREGULAR_CARD_MAX_THRESHOLD].index.tolist()\n",
    "    # print(\"High Cardinality Columns:\", high_cardinality_cols)\n",
    "    return high_cardinality_cols\n",
    "\n",
    "high_cardinality_cols = get_unique_counts_exceed_max_threshold_categorical(df)\n",
    "# Categorical Features with carinality higher than IRREGULAR_CARD_MAX_THRESHOLD\n",
    "print(f\"Categorical Features with carinality higher than {IRREGULAR_CARD_MAX_THRESHOLD} cardinality are: {high_cardinality_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INJURIES_FATAL', 'INJURIES_UNKNOWN', 'CRASH_DAY_OF_WEEK']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.4.2. Irregular cardinality - Too low cardinality for continuous features\n",
    "\n",
    "IRREGULAR_CARD_MIN_THRESHOLD = 10      # To detect continuous features with low cardinality\n",
    "\n",
    "def get_unique_counts_exceed_max_threshold_continuous(df):\n",
    "    continuous = df.select_dtypes(include=['number']).apply(lambda x: x.nunique())\n",
    "    high_cardinality_cols = continuous[continuous < IRREGULAR_CARD_MIN_THRESHOLD].index.tolist()\n",
    "    # print(\"High Cardinality Columns:\", high_cardinality_cols)\n",
    "    return high_cardinality_cols\n",
    "    # cardinality = df.select_dtypes(exclude=['number']).apply(lambda x: x.nunique())\n",
    "    # unique_counts = df.nunique()\n",
    "    # unique_counts_below_min_threshold = unique_counts[unique_counts < IRREGULAR_CARD_MIN_THRESHOLD]\n",
    "    # return pd.DataFrame(unique_counts_below_min_threshold)\n",
    "\n",
    "get_unique_counts_exceed_max_threshold_continuous(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### 2.4.2.  - Irregular Cardinality\n",
    "##### Analysis\n",
    "\n",
    "To do - describe the results from above.\n",
    "\n",
    "Explain why ABOVE NUMBERS ARE OK.\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.3. Consistency - Handle outliers, out of range data or invalid formats (if any)\n",
    "\n",
    "## FIX ME\n",
    "\n",
    "\n",
    "# Outliers\n",
    "# def outliers(df, ft):\n",
    "#     Q1 = df[ft].quantile(0.25)\n",
    "#     Q3 = df[ft].quantile(0.75)\n",
    "#     IQR = Q3 - Q1\n",
    "#     lower_bound = Q1 - 1.5 * IQR\n",
    "#     upper_bound = Q3 + 1.5 * IQR\n",
    "#     ls = df.index[ (df[ft] < lower_bound) | (df[ft] > upper_bound) ]\n",
    "#     return ls\n",
    "\n",
    "# index_list = []\n",
    "# for feature in df.columns:\n",
    "#     if pd.api.types.is_numeric_dtype(df[feature]):\n",
    "#         index_list.extend(outliers(continuous_features_df, feature))\n",
    "\n",
    "# # def remove(df, ls):\n",
    "# #     ls = sorted(set(ls))\n",
    "# #     df = df.drop(ls)\n",
    "# #     return df\n",
    "\n",
    "# print(f\"There are {len(set(index_list))} outliers\")\n",
    "# print(f\"DataFrame Shape Before Removing Outliers {df.shape}\")\n",
    "# df_cleaned_after_outliers = remove(df_prepared, index_list)\n",
    "# print(f\"DataFrame Shape After  Removing Outliers {df_cleaned_after_outliers.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.4. Uniqueness - Remove duplicated or irrelevant repetition in data\n",
    "\n",
    "def consistency_check_duplicated_instances(df):\n",
    "    duplicate_rows = df.duplicated().sum()\n",
    "    print(f\"Number of duplicate rows: {duplicate_rows}\")\n",
    "    # If you want to actually see the duplicate rows, you can use:\n",
    "    if duplicate_rows > 0:\n",
    "        print(df[df.duplicated(keep=False)].sort_values(by=df.columns.tolist()))\n",
    "\n",
    "consistency_check_duplicated_instances(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 3. CRISP-DM: Data Preparation\n",
    "\n",
    "3.1. Missing Values - Drop or Imputation.<br>\n",
    "3.2. Irregular Cardinality.<br>\n",
    "3.3. Outliers - To remove instances.<br>\n",
    "3.4. Consistency - Invalid formats / Ranges.<br>\n",
    "\n",
    "More details in each section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1. Missing Values - Drop or Imputation\n",
    "\n",
    "# Set the threshold for dropping columns\n",
    "MISSING_THRESHOLD = 30.0\n",
    "\n",
    "# Identify columns that have missing value percentage greater than the threshold\n",
    "columns_to_drop = completeness_report[completeness_report['Missing Percentage'] >= MISSING_THRESHOLD].index\n",
    "\n",
    "# Drop these columns from the DataFrame\n",
    "df_after_drop_missing = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Now, df has the columns dropped where the missing value percentage was higher than 30%\n",
    "print(\"Features with missing instances higher than 30% that has been dropped\")\n",
    "columns_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1. Missing Values -Imputation for values\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Your pandas code here\n",
    "\n",
    "# Identify columns that have missing value percentage less than the imputation threshold\n",
    "columns_to_impute = completeness_report[completeness_report['Missing Percentage'] < MISSING_THRESHOLD].index\n",
    "\n",
    "df_after_imputation_missing = df_after_drop_missing.copy()\n",
    "\n",
    "# Loop through the columns and perform imputation\n",
    "for column in columns_to_impute:\n",
    "    # if df_prepared[column].dtype == 'float64' or df[column].dtype == 'int64':\n",
    "    if df_after_imputation_missing[column].dtype == 'numeric':\n",
    "        # Impute numerical columns with the mean value\n",
    "        df_after_imputation_missing[column].fillna(df[column].mean(), inplace=True)\n",
    "    else:\n",
    "        # Impute categorical columns with the mode value (the most frequent value)\n",
    "        df_after_imputation_missing[column].fillna(df[column].mode()[0], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### 3.1. Missing Values -Imputation for values\n",
    "##### Analysis\n",
    "\n",
    "For missing values higher than 30%, we drop them.\n",
    "For missing values lower than 30% - we do imputation. To be more accurate, in the terms of our dataset, missing values under 30% ranges between 0.1%-3% missing values so imputation is a very reasonable choice in this case.\n",
    "\n",
    "Please refer to missing values report in the Data Understanding section.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3. Outliers - To remove instances.\n",
    "\n",
    "df_clean_outliers = df_after_imputation_missing.copy()\n",
    "\n",
    "# def outliers1(df, ft):\n",
    "#     Q1 = df[ft].quantile(0.25)\n",
    "#     Q3 = df[ft].quantile(0.75)\n",
    "#     IQR = Q3 - Q1\n",
    "#     lower_bound = Q1 - 1.5 * IQR\n",
    "#     upper_bound = Q3 + 1.5 * IQR\n",
    "#     ls = df.index[ (df[ft] < lower_bound) | (df[ft] > upper_bound) ]\n",
    "#     return ls\n",
    "\n",
    "# index_list = []\n",
    "# for feature in df_prepared.columns:\n",
    "#     if pd.api.types.is_numeric_dtype(df_prepared[feature]):\n",
    "#         index_list.extend(outliers1(df_prepared, feature))\n",
    "\n",
    "# def remove(df, ls):\n",
    "#     ls = sorted(set(ls))\n",
    "#     df = df.drop(ls)\n",
    "#     return df\n",
    "\n",
    "# print(f\"There are {len(set(index_list))} outliers\")\n",
    "# print(f\"DataFrame Shape Before Removing Outliers {df_prepared.shape}\")\n",
    "# df_cleaned_after_outliers = remove(df_prepared, index_list)\n",
    "# print(f\"DataFrame Shape After  Removing Outliers {df_cleaned_after_outliers.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### 3.3. Outliers - To remove instances\n",
    "##### Analysis\n",
    "\n",
    "To-Do\n",
    "\n",
    "Please refer to the outliers section report in the Data Understanding section.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4. Consistency - Invalid formats / Ranges\n",
    "\n",
    "df_cleaned = df_clean_outliers.copy()\n",
    "\n",
    "# Convert CRASH_DATE to datetime\n",
    "df_cleaned['CRASH_DATE'] = pd.to_datetime(df_cleaned['CRASH_DATE'])\n",
    "\n",
    "# Extract components from CRASH_DATE\n",
    "df_cleaned['YEAR'] = df_cleaned['CRASH_DATE'].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. CRISP-DM: Modeling\n",
    "\n",
    "**Section Overview**\n",
    "\n",
    "4.1. Import relevant ML libs for Modeling.<br>\n",
    "4.2. Formaluize a Numerical Measure for the Target Variable(s).<br>\n",
    "4.3. Undersampling - To fix over-representation in the dataset (unbalanced data).<br>\n",
    "4.4. Modeling with Random Forest.<br>\n",
    "4.5. Modeling with Logestic Regression.<br>\n",
    "4.6. Modeling with DNN.<br>\n",
    "4.7. Modeling with KNN.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1. Import relevant ML libs for Modeling\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2. Formaluize a Numerical Measure for the Target Variable(s)\n",
    "\n",
    "df_to_model = df_cleaned.copy()\n",
    "\n",
    "# Building our numerical metrics for measuring the Severity of injuries based on the reported injuries in the dataset\n",
    "df_to_model['SEVERITY_OF_INJURIES'] = ((0.3 * df_to_model['INJURIES_NON_INCAPACITATING'] + \n",
    "                                 0.6 * df_to_model['INJURIES_INCAPACITATING'] + \n",
    "                                 # Assuming we might use 'INJURIES_INCAPACITATING' or another column for fatal injuries representation\n",
    "                                 0.1 * df_to_model['INJURIES_NO_INDICATION'] + df_to_model['INJURIES_FATAL']) / \n",
    "                                ((df_to_model['INJURIES_NON_INCAPACITATING'] + \n",
    "                                      df_to_model['INJURIES_INCAPACITATING'] + \n",
    "                                      # Again, assuming a placeholder for fatal injuries if needed\n",
    "                                      df_to_model['INJURIES_NO_INDICATION']+df_to_model['INJURIES_FATAL'])))\n",
    "\n",
    "# Code to create the \"INJURY_CLASS\" feature based on \"SEVERITY_OF_INJURIES\"\n",
    "df_to_model['INJURY_ClASS'] = df_to_model['SEVERITY_OF_INJURIES'].apply(lambda x: 'HIGH INJURY' if x > 0.2 else 'LIGHT INJURY')\n",
    "\n",
    "# Code to create the \"SEVERITY_CLASS\" feature based on \"INJURIES_FATAL\" and \"INJURIES_INCAPACITATING\"\n",
    "df_to_model['SEVERITY_CLASS'] = df_to_model.apply(lambda x: 'HIGH SEVERITY' if x['INJURIES_FATAL'] > 0 or x['INJURIES_INCAPACITATING'] > 0 else 'LOW SEVERITY', axis=1)\n",
    "\n",
    "# Display the first few rows to see the new feature\n",
    "df_to_model[['INJURIES_NON_INCAPACITATING','INJURIES_FATAL', 'INJURIES_INCAPACITATING', 'INJURIES_NO_INDICATION', 'SEVERITY_OF_INJURIES','INJURY_ClASS','SEVERITY_CLASS']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### 4.2. Formaluize a Numerical Measure for the Target Variable(s)\n",
    "##### Analysis\n",
    "\n",
    "Our target variable is represented via 4 variables were each one giving the following indications:\n",
    "* INJURIES_NO_INDICATION - No injury or light reported.\n",
    "* INJURIES_NON_INCAPACITATING - Medium injury.\n",
    "* INJURIES_INCAPACITATING - Heavy.\n",
    "* INJURIES_FATAL - Death.\n",
    "\n",
    "To build a mesurement to estimate the fatality of the crash injury crash, we define a weighted equation for each class and normalize it.\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preparing features and target variables \n",
    "X = df_to_model.drop(['SEVERITY_OF_INJURIES','INJURIES_TOTAL' ,'INJURIES_INCAPACITATING', 'MOST_SEVERE_INJURY','INJURY_ClASS','SEVERITY_CLASS',\n",
    "                  'INJURIES_NON_INCAPACITATING', 'SEVERITY_CLASS','INJURIES_NO_INDICATION','CRASH_DATE',\n",
    "                  'INJURIES_UNKNOWN', 'DATE_POLICE_NOTIFIED','INJURIES_REPORTED_NOT_EVIDENT','INJURIES_FATAL','CRASH_RECORD_ID','LOCATION','STREET_NAME'],axis = 1)\n",
    "y = df_to_model['SEVERITY_CLASS']\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# Initialize the random under-sampler due to unbalanced dataset\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "\n",
    "# Resample the dataset\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4. Modeling with Random Forest\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Classifier with balanced class weights\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "\n",
    "# Fitting the model to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_report_results = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy of Random Forest Classifier:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", classification_report_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Features\n",
    "based on RF classifier I check the important features for detecting the severity of injuries in the accident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf_classifier.feature_importances_\n",
    "feature_names = X.columns\n",
    "feature_importances = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "feature_importances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5. Modeling with Logestic Regression\n",
    "\n",
    "# Initialize the Logistic Regression model with balanced class weights\n",
    "log_reg = LogisticRegression( max_iter=10000, random_state=42)\n",
    "\n",
    "# Fitting the model to the training data\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_report_results = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy of linear regressor:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", classification_report_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature names\n",
    "feature_names = X_resampled.columns\n",
    "\n",
    "# Get the coefficients from the logistic regression model\n",
    "coefficients = log_reg.coef_[0]  # Assuming binary classification, hence [0]\n",
    "\n",
    "# Create a series to map feature names to their coefficients\n",
    "feature_importance = pd.Series(coefficients, index=feature_names)\n",
    "\n",
    "# Sort the features by their absolute values to see the most significant ones\n",
    "feature_importance_sorted = feature_importance.abs().sort_values(ascending=False)\n",
    "\n",
    "feature_importance_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.6. Modeling with DNN\n",
    "\n",
    "# It's important to scale your input features for neural networks\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the DNN model\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20,10), activation='relu', solver='adam',\n",
    "                    max_iter=100, random_state=42, verbose=False)\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = mlp.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_report_results = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy of DNN model:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", classification_report_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.7. Modeling with KNN\n",
    "\n",
    "# Initialize the KNN classifier\n",
    "# n_neighbors is set to 10 as an example, but you should tune this parameter\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "# Train the model\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_report_results = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy of KNN model:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", classification_report_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. CRISP-DM: Evaluation\n",
    "\n",
    "in this section I just want to check our model performance on the unseen dataset which is all of them are 'LOW SEVERITY\" and check if it has good accuracy or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_to_evaluate = df_to_model.copy()\n",
    "\n",
    "# Separate instances with \"high injury\" and \"low injury\" classes \n",
    "high_injury_instances = df_to_evaluate[df_to_evaluate['SEVERITY_CLASS'] == 'HIGH SEVERITY']\n",
    "low_injury_instances = df_to_evaluate[df_to_evaluate['SEVERITY_CLASS'] == 'LOW SEVERITY']\n",
    "\n",
    "# Sample the same number of instances with \"low injury\" class as \"high injury\" instances\n",
    "num_high_injury = len(high_injury_instances)\n",
    "low_injury_sampled = low_injury_instances.sample(n=num_high_injury, random_state=42)\n",
    "\n",
    "# Check the model's predictions on the remaining instances with \"low injury\" class\n",
    "low_injury_remaining = low_injury_instances.drop(low_injury_sampled.index)\n",
    "\n",
    "# Concatenate the sampled instances of both classes to create a balanced dataset\n",
    "balanced_data = pd.concat([high_injury_instances, low_injury_sampled])\n",
    "\n",
    "\n",
    "# Prepare features and target variable\n",
    "X_train_balanced = balanced_data.drop(['SEVERITY_OF_INJURIES','INJURIES_TOTAL' ,'INJURIES_INCAPACITATING', 'MOST_SEVERE_INJURY','INJURY_ClASS','SEVERITY_CLASS',\n",
    "                  'INJURIES_NON_INCAPACITATING', 'SEVERITY_CLASS','INJURIES_NO_INDICATION','CRASH_DATE',\n",
    "                  'INJURIES_UNKNOWN', 'DATE_POLICE_NOTIFIED','INJURIES_REPORTED_NOT_EVIDENT','INJURIES_FATAL','CRASH_RECORD_ID','LOCATION','STREET_NAME'],axis = 1)\n",
    "y_train_balanced = balanced_data['SEVERITY_CLASS']\n",
    "\n",
    "X_test_balanced = low_injury_remaining.drop(['SEVERITY_OF_INJURIES','INJURIES_TOTAL' ,'INJURIES_INCAPACITATING', 'MOST_SEVERE_INJURY','INJURY_ClASS','SEVERITY_CLASS',\n",
    "                  'INJURIES_NON_INCAPACITATING', 'SEVERITY_CLASS','INJURIES_NO_INDICATION','CRASH_DATE',\n",
    "                  'INJURIES_UNKNOWN', 'DATE_POLICE_NOTIFIED','INJURIES_REPORTED_NOT_EVIDENT','INJURIES_FATAL','CRASH_RECORD_ID','LOCATION','STREET_NAME'],axis = 1)\n",
    "\n",
    "y_test_balanced = low_injury_remaining['SEVERITY_CLASS']\n",
    "\n",
    "\n",
    "X_train_balanced = pd.get_dummies(X_train_balanced)\n",
    "X_test_balanced = pd.get_dummies(X_test_balanced)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine the training and testing feature data for consistent one-hot encoding\n",
    "combined_features = pd.concat([X_train_balanced, X_test_balanced], axis=0)\n",
    "\n",
    "# Apply get_dummies to the combined dataset\n",
    "combined_features_encoded = pd.get_dummies(combined_features)\n",
    "\n",
    "# Now split them back into training and testing sets\n",
    "X_train_encoded = combined_features_encoded.iloc[:len(X_train_balanced), :]\n",
    "X_test_encoded = combined_features_encoded.iloc[len(X_train_balanced):, :]\n",
    "\n",
    "# Ensure the data is scaled\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
    "X_test_scaled = scaler.transform(X_test_encoded)\n",
    "\n",
    "\n",
    "# Train the KNN model on the scaled, balanced training data\n",
    "knn.fit(X_train_scaled, y_train_balanced)\n",
    "\n",
    "# Make predictions on the scaled, balanced testing set\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the predictions\n",
    "accuracy = accuracy_score(y_test_balanced, y_pred)\n",
    "classification_report_results = classification_report(y_test_balanced, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", classification_report_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. CRISP-DM: Deployment\n",
    "\n",
    "To-Do."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311_pods_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
