{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">Predict Car Traffic Injury</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this document we follow the CRISP-DM process and methodolgies...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CRISP-DM: Business Understanding\n",
    "\n",
    "bla bla bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:cyan\">\n",
    "\n",
    "\n",
    "| Column Name                    | Type           | Short Description                                                                                                                         | Domain Concept           |\n",
    "|--------------------------------|----------------|-------------------------------------------------------------------------------------------------------------------------------------------|-------------------------|\n",
    "| CRASH_RECORD_ID                | Text           | Unique identifier for the crash record, linking to the same crash in the Vehicles and People datasets.                                    | Identification          |\n",
    "| CRASH_DATE_EST_I               | Text           | Indicates if the crash date was estimated by a desk officer or reporting party.                                                           | Crash Details           |\n",
    "| CRASH_DATE                     | DateTime       | Date and time of the crash as entered by the reporting officer.                                                                           | Crash Details           |\n",
    "| POSTED_SPEED_LIMIT             | Number         | Posted speed limit as determined by the reporting officer.                                                                                | Environmental Factors   |\n",
    "| TRAFFIC_CONTROL_DEVICE         | Text           | Traffic control device present at the crash location as determined by the reporting officer.                                              | Environmental Factors   |\n",
    "| DEVICE_CONDITION               | Text           | Condition of the traffic control device as determined by the reporting officer.                                                           | Environmental Factors   |\n",
    "| WEATHER_CONDITION              | Text           | Weather condition at the time of the crash as determined by the reporting officer.                                                        | Environmental Factors   |\n",
    "| LIGHTING_CONDITION             | Text           | Lighting condition at the time of the crash as determined by the reporting officer.                                                       | Environmental Factors   |\n",
    "| FIRST_CRASH_TYPE               | Text           | Type of first collision in the crash.                                                                                                     | Crash Details           |\n",
    "| TRAFFICWAY_TYPE                | Text           | Trafficway type as determined by the reporting officer.                                                                                   | Environmental Factors   |\n",
    "| LANE_CNT                       | Number         | Total number of through lanes in either direction, excluding turn lanes, at the crash location as determined by the reporting officer.     | Environmental Factors   |\n",
    "| ALIGNMENT                      | Text           | Street alignment at the crash location as determined by the reporting officer.                                                            | Environmental Factors   |\n",
    "| ROADWAY_SURFACE_COND           | Text           | Road surface condition at the crash location as determined by the reporting officer.                                                      | Environmental Factors   |\n",
    "| ROAD_DEFECT                    | Text           | Road defects present at the crash location as determined by the reporting officer.                                                        | Environmental Factors   |\n",
    "| REPORT_TYPE                    | Text           | Administrative report type of the crash.                                                                                                   | Documentation          |\n",
    "| CRASH_TYPE                     | Text           | General severity classification for the crash.                                                                                             | Crash Details           |\n",
    "| INTERSECTION_RELATED_I         | Text           | Indicates if an intersection played a role in the crash as observed by the police officer.                                                | Environmental Factors   |\n",
    "| NOT_RIGHT_OF_WAY_I             | Text           | Indicates if the crash began or first contact was made outside of the public right-of-way.                                                | Legal                   |\n",
    "| HIT_AND_RUN_I                  | Text           | Indicates if the crash involved a driver who fled the scene without exchanging information and/or rendering aid.                          | Legal                   |\n",
    "| DAMAGE                         | Text           | Estimated damage from the crash as observed in the field.                                                                                 | Crash Outcome           |\n",
    "| DATE_POLICE_NOTIFIED           | DateTime       | Calendar date on which the police were notified of the crash.                                                                             | Documentation          |\n",
    "| PRIM_CONTRIBUTORY_CAUSE        | Text           | Primary factor contributing to the cause of the crash as determined by officer judgment.                                                  | Crash Analysis         |\n",
    "| SEC_CONTRIBUTORY_CAUSE         | Text           | Secondary factor contributing to the cause of the crash as determined by officer judgment.                                                | Crash Analysis         |\n",
    "| STREET_NO                      | Number         | Street address number of the crash location as determined by the reporting officer.                                                       | Location Details       |\n",
    "| STREET_DIRECTION               | Text           | Street address direction (N, E, S, W) of the crash location as determined by the reporting officer.                                       | Location Details       |\n",
    "| STREET_NAME                    | Text           | Street name of the crash location as determined by the reporting officer.                                                                 | Location Details       |\n",
    "| BEAT_OF_OCCURRENCE             | Number         | Chicago Police Department beat ID where the crash occurred.                                                                               | Administrative         |\n",
    "| PHOTOS_TAKEN_I                 | Text           | Indicates if photos were taken at the crash location by the Chicago Police Department.                                                    | Documentation          |\n",
    "| STATEMENTS_TAKEN_I             | Text           | Indicates if statements were taken from units involved in the crash.                                                                      | Documentation          |\n",
    "| DOORING_I                      | Text           | Indicates if the crash involved a vehicle occupant opening a door into the path of a bicyclist.                                           | Crash Type             |\n",
    "| WORK_ZONE_I                    | Text           | Indicates if the crash occurred in an active work zone.                                                                                   | Environmental Factors   |\n",
    "| WORK_ZONE_TYPE                 | Text           | Type of work zone, if any, where the crash occurred.                                                                                      | Environmental Factors   |\n",
    "| WORKERS_PRESENT_I              | Text           | Indicates if construction workers were present in the work zone at the crash location.                                                    | Environmental Factors   |\n",
    "| NUM_UNITS                      | Number         | Number of units involved in the crash, representing different modes of traffic with independent trajectories.                             | Crash Details           |\n",
    "| MOST_SEVERE_INJURY             | Text           | Most severe injury sustained by any person involved in the crash.                                                                         | Injury Analysis        |\n",
    "| INJURIES_TOTAL                 | Number         | Total number of people sustaining fatal, incapacitating, non-incapacitating, and possible injuries as determined by the reporting officer. | Injury Analysis        |\n",
    "| INJURIES_FATAL                 | Number         | Total number of people sustaining fatal injuries in the crash.                                                                            | Injury Analysis        |\n",
    "| INJURIES_INCAPACITATING        | Number         | Total number of people sustaining incapacitating/serious injuries in the crash.                                                           | Injury Analysis        |\n",
    "| INJURIES_NON_INCAPACITATING    | Number         | Total number of people sustaining non-incapacitating injuries in the crash.                                                               | Injury Analysis        |\n",
    "| INJURIES_REPORTED_NOT_EVIDENT  | Number         | Total number of people sustaining possible injuries in the crash as determined by the reporting officer.                                  | Injury Analysis        |\n",
    "| INJURIES_NO_INDICATION         | Number         | Total number of people with no injuries in the crash as determined by the reporting officer.                                              | Injury Analysis        |\n",
    "| INJURIES_UNKNOWN               | Number         | Total number of people for whom the injury status, if any, is unknown.                                                                    | Injury Analysis        |\n",
    "| CRASH_HOUR                     | Number         | Hour of the day when the crash occurred, derived from CRASH_DATE.                                                                         | Time Details           |\n",
    "| CRASH_DAY_OF_WEEK              | Number         | Day of the week when the crash occurred, derived from CRASH_DATE. Sunday=1                                                                | Time Details           |\n",
    "| CRASH_MONTH                    | Number         | Month when the crash occurred, derived from CRASH_DATE.                                                                                   | Time Details           |\n",
    "| LATITUDE                       | Number         | Latitude of the crash location as determined by the reporting officer.                                                                    | Geographic Information |\n",
    "| LONGITUDE                      | Number         | Longitude of the crash location as determined by the reporting officer.                                                                   | Geographic Information |\n",
    "| LOCATION                       | Point          | Geographic location of the crash as determined by the reporting officer, allowing for mapping and geographic analysis.                    | Geographic Information |\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:cyan\">\n",
    "\n",
    "\n",
    "\n",
    "### **Domain Concepts Explained**\n",
    "\n",
    "##### **1. Identification**\n",
    "- **Description**: Features that uniquely identify the crash records or events, often used for linking data across different datasets.\n",
    "- **Example**: \n",
    "  - **CRASH_RECORD_ID**: Serves as a unique identifier for each crash event, enabling connections with related records in vehicles and people datasets.\n",
    "\n",
    "##### **2. Crash Details**\n",
    "- **Description**: Specific details about the crash event, including the type of crash, date, and time, providing basic information on how and when the crash occurred.\n",
    "- **Example**: \n",
    "  - **CRASH_DATE**: Provides the exact date and time the crash happened.\n",
    "  - **FIRST_CRASH_TYPE**: Describes the initial type of collision.\n",
    "\n",
    "##### **3. Environmental Factors**\n",
    "- **Description**: Conditions surrounding the crash, such as weather, lighting, and road conditions, which can influence the occurrence and severity of crashes.\n",
    "- **Example**: \n",
    "  - **WEATHER_CONDITION** and **LIGHTING_CONDITION**: Detail the environmental state at the time of the crash, affecting driving conditions and visibility.\n",
    "\n",
    "##### **4. Legal**\n",
    "- **Description**: Features related to legal aspects of the crash, including compliance with right-of-way rules and hit-and-run incidents.\n",
    "- **Example**: \n",
    "  - **HIT_AND_RUN_I**: Indicates whether a driver involved in the crash fled the scene without providing information or rendering aid.\n",
    "\n",
    "##### **5. Crash Outcome**\n",
    "- **Description**: Descriptions of the immediate consequences of the crash, primarily in terms of physical damage.\n",
    "- **Example**: \n",
    "  - **DAMAGE**: Estimates the monetary damage to property resulting from the crash.\n",
    "\n",
    "##### **6. Documentation**\n",
    "- **Description**: Administrative details about the crash reporting and documentation process, including whether photos or statements were taken.\n",
    "- **Example**: \n",
    "  - **DATE_POLICE_NOTIFIED**: Records when the police were officially notified about the crash.\n",
    "\n",
    "##### **7. Crash Analysis**\n",
    "- **Description**: Factors identified as contributing to the crash, including primary and secondary causes as determined by officer judgment.\n",
    "- **Example**: \n",
    "  - **PRIM_CONTRIBUTORY_CAUSE** and **SEC_CONTRIBUTORY_CAUSE**: Identify the main reasons behind the crash according to the reporting officer.\n",
    "\n",
    "##### **8. Location Details**\n",
    "- **Description**: Specifics about where the crash occurred, including street names and numbers, helping in pinpointing the exact crash location.\n",
    "- **Example**: \n",
    "  - **STREET_NAME**, **STREET_NO**, and **STREET_DIRECTION**: Provide a detailed description of the crash site.\n",
    "\n",
    "##### **9. Administrative**\n",
    "- **Description**: Information related to police and emergency response jurisdictions, such as police beats.\n",
    "- **Example**: \n",
    "  - **BEAT_OF_OCCURRENCE**: Refers to the Chicago Police Department beat where the crash took place.\n",
    "\n",
    "##### **10. Crash Type**\n",
    "- **Description**: Categories that describe specific scenarios or types of crashes, including incidents involving bicycles or work zones.\n",
    "- **Example**: \n",
    "  - **DOORING_I**: Specifies if the crash involved a vehicle door being opened in the path of a bicyclist.\n",
    "\n",
    "##### **11. Injury Analysis**\n",
    "- **Description**: Detailed records of injuries resulting from the crash, categorizing the severity and type of injuries sustained.\n",
    "- **Example**: \n",
    "  - **INJURIES_TOTAL**: Counts the total number of injuries.\n",
    "  - **INJURIES_FATAL**: Counts the number of fatal injuries.\n",
    "\n",
    "##### **12. Time Details**\n",
    "- **Description**: Temporal information about the crash, including the hour, day of the week, and month when the crash occurred.\n",
    "- **Example**: \n",
    "  - **CRASH_HOUR**, **CRASH_DAY_OF_WEEK**, and **CRASH_MONTH**: Provide insights into the timing patterns of crashes.\n",
    "\n",
    "##### **13. Geographic Information**\n",
    "- **Description**: Geospatial data related to the crash location, enabling mapping and location-based analysis.\n",
    "- **Example**: \n",
    "  - **LATITUDE** and **LONGITUDE**: Give precise coordinates of the crash site.\n",
    "  - **LOCATION**: Offers a mappable point of the incident.\n",
    "\n",
    "Each domain concept helps categorize the dataset's features for easier analysis, highlighting different factors and details that contribute to a comprehensive understanding of traffic crashes.\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 2. CRISP-DM: Data Understanding\n",
    "\n",
    "In this section, the following will be addressed:<br><br>\n",
    "**2.1. Collect Initial Data**<br>\n",
    "2.1.1. Import the relevant Python packages that are going to be used.<br>\n",
    "2.1.2. Acquire the data.<br>\n",
    "2.1.3. Record of the data acquisition process.<br>\n",
    "2.2.4. HTML Reprot.<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**2.2. Data Description**<br>\n",
    "2.2.1. Shape of the Dataset.<br>\n",
    "2.2.2. Head snippet.<br>\n",
    "2.2.3. Dataset info.<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**2.3. Explore the Data**<br>\n",
    "2.3.1. Basic Statistics - For numeric models, to calculate Mean, Median, mode, ... <br>\n",
    "2.3.2. Tabular Report - For Continuos and Categorical Features (Refer to HTML Reports please). <br>\n",
    "2.3.3. Correlations + Heat Map.<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**2.4. Verify Data Quality**<br>\n",
    "2.4.1. Completnesss - Missing Values Summary.<br>\n",
    "2.4.2. Irregular cardinality, (1, too high for categorical, too low for continuos)<br>\n",
    "2.4.3. Consistency - Handle outliers, out of range data or invalid formats (if any).<br>\n",
    "2.4.4. Uniqueness - Remove duplicated or irrelevant repetition in data.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 2.1. Collect Initial Data\n",
    "\n",
    "\n",
    "This a generic intial step, to start working on the data:<br>\n",
    "2.1.1. Import the relevant Python packages that are going to be used.<br>\n",
    "2.1.2. Acquire the data.<br>\n",
    "2.1.3. Record of the data acquisition process.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1.1. Import the relevant Python packages that are going to be used\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from ydata_profiling import ProfileReport\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1.2. Acquire the data\n",
    "\n",
    "# A generic utilies file with generic functionallity\n",
    "from misc.utilities import acquire_dataset\n",
    "\n",
    "# Download & load the dataset\n",
    "# If the file already downloaded, it will not be re-downloaded (to speed up work efficiency)\n",
    "df = acquire_dataset()\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "\n",
    "2.1.3. Record of the data acquisition process.<br>\n",
    "\n",
    "The data acquired from: https://catalog.data.gov/dataset/traffic-crashes-crashes\n",
    "<br>\n",
    "The above website belongs to USA government free datasets advised by the course instructions.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def get_continuous_feature_details(my_df, my_feature):\n",
    "    my_df[my_feature] = pd.to_numeric(my_df[my_feature], errors='coerce')\n",
    "    # Printing unique values\n",
    "    unique_values = my_df[my_feature].unique()\n",
    "    sorted_unique_values = np.sort(unique_values)\n",
    "    len_unique_values = len(unique_values)\n",
    "    unique_values_str = ', '.join(map(str, sorted_unique_values[:5])) + ', ..., ' + ', '.join(map(str, sorted_unique_values[-5:]))\n",
    "    max_value = my_df[my_feature].max()\n",
    "    min_value = my_df[my_feature].min()\n",
    "    # print(f\"{my_feature}: Unique Len={len_unique_values}, Unique={sorted_unique_values}, Max={max_value}, Min={min_value}\")\n",
    "    print(f\"{my_feature}:\\n  Unique Len={len_unique_values}\\n  Unique={unique_values_str}\\n  Max={max_value}\\n  Min={min_value}\")\n",
    "\n",
    "    return len_unique_values, sorted_unique_values, max_value, min_value\n",
    "    \n",
    "\n",
    "def dump_feature_frequency_to_a_file(my_df, my_feature):\n",
    "    value_counts = my_df[my_feature].value_counts()\n",
    "    # value_counts.to_csv(f\"{my_feature}_value_counts.csv\", index=True, header=['Frequency'])\n",
    "    # Open a file to write\n",
    "    with open(f\"{my_feature}_value_counts.txt\", 'w') as file:\n",
    "        # Write a header row\n",
    "        file.write(f\"{'Value':<20}{'Frequency':<10}\\n\")\n",
    "        file.write(f\"{'-'*20}{'-'*10}\\n\")\n",
    "        \n",
    "        # Iterate over the Series and write each value and its frequency\n",
    "        for value, count in value_counts.items():\n",
    "            file.write(f\"{value:<20}{count:<10}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 2.2. Data Description\n",
    "\n",
    "In this phase we investigate the following aspects:<br>\n",
    "2.2.1. Shape of the Dataset.<br>\n",
    "2.2.2. Head snippet.<br>\n",
    "2.2.3. Dataset info.<br>\n",
    "2.2.4. Numerical Feature Distribution.<br>\n",
    "2.2.5. Catagorical Feature Distribution.<br>\n",
    "2.2.6 HTML report<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2.1. Shape of the Dataset.\n",
    "\n",
    "def get_dataset_shape(df):\n",
    "    # Create a new DataFrame to display the shape information\n",
    "    shape_df = pd.DataFrame({\n",
    "        'Aspect': ['Records (Instances)', 'Features (Columns)'],\n",
    "        'Number': [df.shape[0], df.shape[1]]\n",
    "    })\n",
    "    return shape_df\n",
    "\n",
    "# Display the DataFrame\n",
    "get_dataset_shape(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2.2. Head snippet\n",
    "\n",
    "def get_dataset_head(df):\n",
    "    df.head()\n",
    "\n",
    "get_dataset_head(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2.3. Dataset info\n",
    "\n",
    "def get_dataset_info(df):\n",
    "    df.info()\n",
    "\n",
    "get_dataset_info(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2.4. Numerical Feature Distribution\n",
    "\n",
    "# Filter out deprecated warnings from Seaborn\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Select only numeric columns for distribution plots\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Determine the number of rows/columns for the subplot grid\n",
    "num_features = numeric_df.shape[1]\n",
    "num_rows = int(np.ceil(num_features / 3))  # Adjust the denominator to change the number of columns\n",
    "\n",
    "# Create a figure and a grid of subplots\n",
    "plt.figure(figsize=(15, num_rows * 5))\n",
    "\n",
    "for i, column in enumerate(numeric_df.columns):\n",
    "    plt.subplot(num_rows, 3, i + 1)\n",
    "    sns.histplot(numeric_df[column], kde=True, stat = 'density')\n",
    "\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "# Adjust layout for better visualization\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.2.5 Catagorical Feature Distribution\n",
    "\n",
    "# Select only categorical columns\n",
    "categorical_df = df.select_dtypes(include=['object', 'category']) #exclude=['number']\n",
    "\n",
    "# Delete some of the columns due ot cardinality issues ( they have alot of levels like Id)\n",
    "categorical_df = categorical_df.drop(['PRIM_CONTRIBUTORY_CAUSE','SEC_CONTRIBUTORY_CAUSE','STREET_NAME','DATE_POLICE_NOTIFIED','LOCATION','CRASH_RECORD_ID' , 'CRASH_DATE_EST_I','CRASH_DATE'] , axis = 1)\n",
    "\n",
    "# Determine the number of rows/columns for the subplot grid\n",
    "num_features = categorical_df.shape[1]\n",
    "num_rows = int(np.ceil(num_features / 3))  # Adjust for desired number of columns per row\n",
    "\n",
    "plt.figure(figsize=(15, num_rows * 5))\n",
    "\n",
    "for i, column in enumerate(categorical_df.columns):\n",
    "    plt.subplot(num_rows, 3, i + 1)\n",
    "    ax = sns.countplot(y=categorical_df[column])\n",
    "    total = len(categorical_df[column])  # Total number of data points for the percentage calculation\n",
    "    for p in ax.patches:\n",
    "        percentage = '{:.1f}%'.format(100 * p.get_width()/total)\n",
    "        x = p.get_x() + p.get_width() + 0.02  # Shifts the text to the right side of the bars\n",
    "        y = p.get_y() + p.get_height()/2\n",
    "        ax.annotate(percentage, (x, y))\n",
    "    \n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel(column)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 2.3. Explore the Data\n",
    "\n",
    "2.3.1. Basic Statistics - For numeric models, to calculate Mean, Median, mode, ... <br>\n",
    "2.3.2. Tabular Report - For Continuos and Categorical Features (Refer to HTML Reports please). <br>\n",
    "2.3.3. Correlations + Heat Map.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3.1. Continuous Features Tabular Report\n",
    "\n",
    "def get_continuous_features_tabular_report(df):\n",
    "    # continuous_features_tabular_report_df = df.describe(include=['number'])\n",
    "    return df.describe(include=['number'])\n",
    "\n",
    "# Transpose - More friendly print\n",
    "continuous_features_transpose_tabular_report_df = get_continuous_features_tabular_report(df).transpose()\n",
    "\n",
    "continuous_features_transpose_tabular_report_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3.2. Categorical Features Tabular Report\n",
    "\n",
    "def get_categorical_features_tabular_report(df):\n",
    "    return df.describe(exclude=['number'])\n",
    "\n",
    "# Transpose - More friendly print\n",
    "categorical_features_transpose_tabular_report_df = get_categorical_features_tabular_report(df).transpose()\n",
    "\n",
    "categorical_features_transpose_tabular_report_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3.3. Correlation insights + HeatMap\n",
    "\n",
    "def get_correlation_insights(df):\n",
    "    # # Get the relevant columns\n",
    "    subset_df = df[get_continuous_features_tabular_report(df).columns]\n",
    "\n",
    "    # Calculate the correlation matrix\n",
    "    # correlation_matrix = subset_df.corr()\n",
    "    return subset_df.corr()\n",
    "\n",
    "correlation_matrix = get_correlation_insights(df)\n",
    "correlation_matrix\n",
    "\n",
    "# # Create a heatmap using seaborn\n",
    "# plt.figure(figsize=(16, 14))\n",
    "# sns.heatmap(correlation_matrix, cmap='coolwarm', annot=True, fmt=\".2f\", linewidths=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3.4. HeatMap\n",
    "\n",
    "def draw_heatmap(correlation_matrix):\n",
    "    # Create a heatmap using seaborn\n",
    "    plt.figure(figsize=(16, 14))\n",
    "    return sns.heatmap(correlation_matrix, cmap='coolwarm', annot=True, fmt=\".2f\", linewidths=0.8)\n",
    "\n",
    "draw_heatmap(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 2.4. Verify Data Quality\n",
    "\n",
    "2.4.1. Completnesss - Missing Values Summary.<br>\n",
    "2.4.2. Irregular cardinality, could be one of the following cases:<br>\n",
    "* Features with a cardinality of 1.\n",
    "* Too high cardinality for categorical features.\n",
    "* Too low cardinality for continous features.<br>\n",
    "\n",
    "2.4.3. Consistency - Handle outliers.<br>\n",
    "2.4.4. Uniqueness - Remove duplicated or irrelevant repetition in data.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.1. Completnesss - Missing Values Summary\n",
    "\n",
    "# Get missing values details\n",
    "# Retrun value per feature as a numeric value and a percentage\n",
    "def get_missing_values_details(df):\n",
    "    # Check for missing values in each column\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_values_sorted = missing_values.sort_values(ascending=False)\n",
    "    # Check the percentage of missing values for each column\n",
    "    missing_percentage_sorted = (missing_values_sorted / len(df)) * 100\n",
    "    return missing_values_sorted, missing_percentage_sorted\n",
    "\n",
    "\n",
    "def get_completeness_report(df):\n",
    "    missing_values_sorted, missing_percentage_sorted = get_missing_values_details(df)\n",
    "    # Create a DataFrame to summarize the completeness\n",
    "    completeness_report = pd.DataFrame({\n",
    "        'Missing Values': missing_values_sorted,\n",
    "        'Missing Percentage': missing_percentage_sorted\n",
    "    })\n",
    "    return completeness_report\n",
    "\n",
    "completeness_report = get_completeness_report(df)\n",
    "completeness_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section x.y. - Check Ranges & Special Outliers\n",
    "get_continuous_feature_details(df, 'CRASH_HOUR')\n",
    "get_continuous_feature_details(df, 'CRASH_DAY_OF_WEEK')\n",
    "get_continuous_feature_details(df, 'CRASH_MONTH')\n",
    "get_continuous_feature_details(df, 'LATITUDE')\n",
    "get_continuous_feature_details(df, 'LONGITUDE')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### x.y. Check Ranges & Special Outliers\n",
    "##### Analysis\n",
    "\n",
    "This section addresses problems mostly in ranges for special features.\n",
    "For example, week can't be more than 7 days.\n",
    "Let's go over the descriptive features and provide insights.\n",
    "\n",
    "\n",
    "\n",
    "| Feature            | Unique Values Len | Unique Values Example    | Max Value | Min Value | Comments                |\n",
    "|--------------------|-------------------|--------------------------|-----------|-----------|-------------------------|\n",
    "| CRASH_HOUR         | 24 |  [0 1 2 ... 23]  |  23 | 0 | Time of the crash (0-23)|\n",
    "| CRASH_DAY_OF_WEEK  | 7 | [1 2 3 4 5 6 7] | 7 | 1 | Day of the week (1-7)   |\n",
    "| CRASH_MONTH        | 12 | [1 2 3 ... 12] |  12  |  1   | Month of the crash (1-12)|\n",
    "| LATITUDE           |  299299 |  [ 0 41.64467013 41.64469152 ... 42.02273632 ]      |    42.022779861       |     0.0      | Geographical latitude   |\n",
    "| LONGITUDE          |  299262  | [-87.93619295 -87.93587692 ... 0] |   0.0        |     -87.936192947      | Geographical longitude  |\n",
    "\n",
    "<br>\n",
    "\n",
    "As the table illustrates, the following features have valid values:<br>\n",
    "* CRASH_HOUR\n",
    "* CRASH_DAY_OF_WEEK\n",
    "* CRASH_MONTH\n",
    "\n",
    "<br>\n",
    "\n",
    "But the following features has values out of range / outliers in respect to each one type:\n",
    "* LATITUDE - Min value is 0.0\n",
    "* LONGITUDE - Max value is 0.0\n",
    "\n",
    "<br>\n",
    "\n",
    "Doing research in Google Maps and Google for Chicago city LATITUDE and LONGITUDE range (also NaN, but will be addressing in missing values section), here is our findings:<br>\n",
    "LATITUDE valid range: [41.640, 42.023]<br>\n",
    "LONGITUDE valid range: [-87.940, -87.524]<br>\n",
    "\n",
    "We will address that again in the Data Preprocessing section to clean up the out of range values.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.2. Irregular cardinality - Features with a cardinality of 1\n",
    "\n",
    "IRREGULAR_CARD_SINGLE = 1             # To detect features with cardinality of 1\n",
    "\n",
    "def get_irregular_cardinality_with_single_cardinality(df):\n",
    "    unique_counts = df.nunique()\n",
    "    # for feature_name, count in unique_counts.items():\n",
    "    #     if count == 1:\n",
    "    #         print(f\"The feature '{feature_name}' has a unique count of 1 - Irregular Cardinality Issue\")\n",
    "    unique_counts_single_threshold = unique_counts[unique_counts == IRREGULAR_CARD_SINGLE]\n",
    "    return pd.DataFrame(unique_counts_single_threshold)\n",
    "\n",
    "\n",
    "get_irregular_cardinality_with_single_cardinality(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.2. Irregular cardinality - Too high cardinality for categorical features\n",
    "\n",
    "IRREGULAR_CARD_MAX_THRESHOLD = 10     # To detect categorical features with high cardinality\n",
    "\n",
    "def get_unique_counts_exceed_max_threshold_categorical(df):\n",
    "    cardinality = df.select_dtypes(exclude=['number']).apply(lambda x: x.nunique())\n",
    "    high_cardinality_cols = cardinality[cardinality > IRREGULAR_CARD_MAX_THRESHOLD].index.tolist()\n",
    "    # print(\"High Cardinality Columns:\", high_cardinality_cols)\n",
    "    return high_cardinality_cols\n",
    "\n",
    "high_cardinality_cols = get_unique_counts_exceed_max_threshold_categorical(df)\n",
    "# Categorical Features with carinality higher than IRREGULAR_CARD_MAX_THRESHOLD\n",
    "print(f\"Categorical Features with carinality higher than {IRREGULAR_CARD_MAX_THRESHOLD} cardinality are: {high_cardinality_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.2. Irregular cardinality - Too low cardinality for continuous features\n",
    "\n",
    "IRREGULAR_CARD_MIN_THRESHOLD = 10      # To detect continuous features with low cardinality\n",
    "\n",
    "def get_unique_counts_exceed_min_threshold_continuous(df):\n",
    "    continuous = df.select_dtypes(include=['number']).apply(lambda x: x.nunique())\n",
    "    high_cardinality_cols = continuous[continuous < IRREGULAR_CARD_MIN_THRESHOLD].index.tolist()\n",
    "    # print(\"High Cardinality Columns:\", high_cardinality_cols)\n",
    "    return high_cardinality_cols\n",
    "    # cardinality = df.select_dtypes(exclude=['number']).apply(lambda x: x.nunique())\n",
    "    # unique_counts = df.nunique()\n",
    "    # unique_counts_below_min_threshold = unique_counts[unique_counts < IRREGULAR_CARD_MIN_THRESHOLD]\n",
    "    # return pd.DataFrame(unique_counts_below_min_threshold)\n",
    "\n",
    "get_unique_counts_exceed_min_threshold_continuous(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### 2.4.2.  - Irregular Cardinality\n",
    "##### Analysis\n",
    "\n",
    "To do - describe the results from above.\n",
    "\n",
    "Explain why ABOVE NUMBERS ARE OK.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.3. Consistency - Handle outliers\n",
    "\n",
    "def get_outliers_list_of_indices(df, ft):\n",
    "    Q1 = df[ft].quantile(0.25)\n",
    "    Q3 = df[ft].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    ls = df.index[ (df[ft] < lower_bound) | (df[ft] > upper_bound) ]\n",
    "    \n",
    "    return ls\n",
    "\n",
    "# To store the indicies\n",
    "outliers_index_list = []\n",
    "# To store the features (set, so no duplications)\n",
    "outliers_features_set = set()\n",
    "numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "for feature in numeric_columns:\n",
    "    indicies_list = get_outliers_list_of_indices(df, feature)\n",
    "    outliers_index_list.extend(indicies_list)\n",
    "    if len(indicies_list) != 0:\n",
    "        outliers_features_set.add(feature)\n",
    "\n",
    "print(f\"There are {len(outliers_index_list)} outliers\")\n",
    "print(f\"Features set are {outliers_features_set}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### 2.4.2.  - Outliers\n",
    "##### Analysis\n",
    "\n",
    "Outliers inital result shows the following features are outliers:\n",
    "* STREET_NO\n",
    "* POSTED_SPEED_LIMIT\n",
    "* INJURIES_REPORTED_NOT_EVIDENT\n",
    "* LONGITUDE\n",
    "* LANE_CNT\n",
    "* BEAT_OF_OCCURRENCE\n",
    "* INJURIES_TOTAL\n",
    "* INJURIES_INCAPACITATING\n",
    "* INJURIES_NON_INCAPACITATING\n",
    "* INJURIES_NO_INDICATION\n",
    "* LATITUDE\n",
    "* INJURIES_FATAL\n",
    "* NUM_UNITS\n",
    "\n",
    "Let's understand if they are really outliers, or the values are valid (e.g. street number is valid to be in a very big range).\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_continuous_feature_details(df, 'STREET_NO')\n",
    "get_continuous_feature_details(df, 'POSTED_SPEED_LIMIT')\n",
    "get_continuous_feature_details(df, 'INJURIES_REPORTED_NOT_EVIDENT')\n",
    "get_continuous_feature_details(df, 'LONGITUDE')\n",
    "get_continuous_feature_details(df, 'LANE_CNT')\n",
    "get_continuous_feature_details(df, 'BEAT_OF_OCCURRENCE')\n",
    "get_continuous_feature_details(df, 'INJURIES_TOTAL')\n",
    "get_continuous_feature_details(df, 'INJURIES_INCAPACITATING')\n",
    "get_continuous_feature_details(df, 'INJURIES_NON_INCAPACITATING')\n",
    "get_continuous_feature_details(df, 'INJURIES_NO_INDICATION')\n",
    "get_continuous_feature_details(df, 'LATITUDE')\n",
    "get_continuous_feature_details(df, 'INJURIES_FATAL')\n",
    "get_continuous_feature_details(df, 'NUM_UNITS')\n",
    "pass # To avoid printing of the previous line and keep only formatted-pretty prints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### 2.4.2.  - Outliers\n",
    "##### Analysis\n",
    "\n",
    "| Feature                        | Unique Values Len | Unique Values Example                  | Max Value    | Min Value    | Comments                                                                 | Explanation                                        |\n",
    "|--------------------------------|-------------------|----------------------------------------|--------------|--------------|--------------------------------------------------------------------------|----------------------------------------------------|\n",
    "| STREET_NO                      | 11723             | 0, 1, 2, 3, 4, ..., 13787, 13795, 13799, 34453, 451100 | 451100       | 0            | Street numbers in Chicago, large range due to city size and data errors. | Street values can highly vary, it doesn't follow math logic      |\n",
    "| POSTED_SPEED_LIMIT             | 46                | 0, 1, 2, 3, 4, ..., 62, 63, 65, 70, 99 | 99           | 0            | Posted speed limits, including possibly erroneous values.                | Speed limits can highly vary. We checked all values count to verify no outliers in respect to allowed speed - e.g. no instance with value 500 (please refer to the function dump_feature_frequency_to_a_file)       |\n",
    "| INJURIES_REPORTED_NOT_EVIDENT  | 14                | 0.0, 1.0, 2.0, 3.0, 4.0, ..., 9.0, 10.0, 11.0, 15.0 | 15.0         | 0.0          | Number of non-evident injuries reported per crash.                       | Range is reasonable for severe accidents.           |\n",
    "| LONGITUDE                      | 299263            | -87.936192947, ..., -87.524587387, 0.0 | 0.0          | -87.936192947| Longitude values in Chicago, including erroneous 0.0 entries.            | 0.0 likely represents missing or incorrectly entered data. It will be handled in data preprocessing phase. |\n",
    "| LANE_CNT                       | 42                | 0.0, 1.0, 2.0, 3.0, 4.0, ..., 1191625.0| 1191625.0    | 0.0          | Number of lanes, with some unrealistic values due to errors.             | Extreme max value due to data entry errors.        |\n",
    "| BEAT_OF_OCCURRENCE             | 277               | 111.0, ..., 2535.0, 6100.0             | 6100.0       | 111.0        | Police beat codes, including possibly erroneous 6100.0.                 | This is police internal codes and can't be treated as outliers          |\n",
    "| INJURIES_TOTAL                 | 21                | 0.0, 1.0, ..., 19.0, 21.0              | 21.0         | 0.0          | Total number of injuries per crash.                                      | Range is within expected limits for traffic accidents. |\n",
    "| INJURIES_INCAPACITATING        | 11                | 0.0, 1.0, ..., 8.0, 10.0               | 10.0         | 0.0          | Number of incapacitating injuries per crash.                             | Within expected limits, considering severe cases.   |\n",
    "| INJURIES_NON_INCAPACITATING    | 20                | 0.0, 1.0, ..., 19.0, 21.0              | 21.0         | 0.0          | Number of non-incapacitating injuries per crash.                         | Range reflects possible severe accidents.           |\n",
    "| INJURIES_NO_INDICATION         | 49                | 0.0, 1.0, ..., 50.0, 61.0              | 61.0         | 0.0          | People involved in crash with no injuries reported.                      | High values for large accidents, not outliers.     |\n",
    "| LATITUDE                       | 299300            | 0.0, 41.644670132, ..., 42.022779861   | 42.022779861 | 0.0          | Latitude values in Chicago, including erroneous 0.0 entries.             | 0.0 likely represents missing or incorrectly entered data. It will be handled in data preprocessing phase. |\n",
    "| INJURIES_FATAL                 | 6                 | 0.0, 1.0, ..., 3.0, 4.0                | 4.0          | 0.0          | Number of fatal injuries per crash.                                      | Fatalities are rare, but values are within expected limits. |\n",
    "| NUM_UNITS                      | 17                | 1, 2, ..., 16, 18                      | 18           | 1            | Number of units involved in a crash (e.g. cars, motocycles, ...) | Not an outlier, the highest values sounds reasonable |\n",
    "\n",
    "From the above table, we can notice that the following features has outliers:\n",
    "* LANE_CNT\n",
    "* LONGITUDE\n",
    "* LATITUDE\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.4. Uniqueness - Remove duplicated or irrelevant repetition in data\n",
    "\n",
    "def consistency_check_duplicated_instances(df):\n",
    "    duplicate_rows = df.duplicated().sum()\n",
    "    print(f\"Number of duplicate rows: {duplicate_rows}\")\n",
    "    # If you want to actually see the duplicate rows, you can use:\n",
    "    if duplicate_rows > 0:\n",
    "        print(df[df.duplicated(keep=False)].sort_values(by=df.columns.tolist()))\n",
    "\n",
    "consistency_check_duplicated_instances(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 3. CRISP-DM: Data Preparation\n",
    "\n",
    "3.1. Missing Values - Drop or Imputation.<br>\n",
    "3.2. Irregular Cardinality.<br>\n",
    "3.3. Outliers - To remove instances.<br>\n",
    "3.4. Consistency - Invalid formats / Ranges.<br>\n",
    "\n",
    "More details in each section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1. Missing Values (Part-1) - Drop\n",
    "\n",
    "# For each step, we take a copy. It is easier to debug and investigate.\n",
    "df_drop_missing_features = df.copy()\n",
    "\n",
    "# Set the threshold for dropping columns\n",
    "MISSING_THRESHOLD = 30.0\n",
    "\n",
    "# Identify columns that have missing value percentage greater than the threshold\n",
    "columns_to_drop = completeness_report[completeness_report['Missing Percentage'] >= MISSING_THRESHOLD].index\n",
    "\n",
    "# Drop these columns from the DataFrame\n",
    "df_drop_missing_features = df_drop_missing_features.drop(columns=columns_to_drop)\n",
    "\n",
    "# Now, df has the columns dropped where the missing value percentage was higher than 30%\n",
    "print(\"Features with missing instances higher than 30% that has been dropped\")\n",
    "columns_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1. Missing Values (Part-2) - Special handle for LATITUDE and LONGITUDE\n",
    "\n",
    "df_drop_missing_instances = df_drop_missing_features.copy()\n",
    "\n",
    "\n",
    "def drop_missing_instances(my_df, my_feature):\n",
    "    # Remove rows where LATITUDE is NaN\n",
    "    df_cleaned = my_df.dropna(subset=[my_feature])\n",
    "    return df_cleaned\n",
    "drop_missing_instances(df_drop_missing_features, 'LATITUDE')\n",
    "drop_missing_instances(df_drop_missing_features, 'LATITUDE')\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1. Missing Values (Part-2) - Imputation for values\n",
    "\n",
    "\n",
    "df_imputation_missing = df_drop_missing_instances.copy()\n",
    "\n",
    "# Identify columns that have missing value percentage less than the imputation threshold\n",
    "columns_to_impute = completeness_report[completeness_report['Missing Percentage'] < MISSING_THRESHOLD].index\n",
    "\n",
    "\n",
    "# Loop through the columns and perform imputation\n",
    "for column in columns_to_impute:\n",
    "    # if df_prepared[column].dtype == 'float64' or df[column].dtype == 'int64':\n",
    "    if df_imputation_missing[column].dtype == 'numeric':\n",
    "        # Impute numerical columns with the mean value\n",
    "        df_imputation_missing[column].fillna(df_imputation_missing[column].mean(), inplace=True)\n",
    "    else:\n",
    "        # Impute categorical columns with the mode value (the most frequent value)\n",
    "        df_imputation_missing[column].fillna(df_imputation_missing[column].mode()[0], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1. Missing Values (Part-3) - Print for visibility of current situation\n",
    "\n",
    "completeness_report = get_completeness_report(df_imputation_missing)\n",
    "completeness_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### 3.1. Missing Values -Imputation for values\n",
    "##### Analysis\n",
    "\n",
    "# To-Do - Update about the three parts\n",
    "\n",
    "For missing values higher than 30%, we drop them.\n",
    "For missing values lower than 30% - we do imputation. To be more accurate, in the terms of our dataset, missing values under 30% ranges between 0.1%-3% missing values so imputation is a very reasonable choice in this case.\n",
    "\n",
    "Please refer to missing values report in the Data Understanding section.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.y. Drop Duplicated features\n",
    "\n",
    "df_drop_location = df_imputation_missing.copy()\n",
    "\n",
    "df_drop_location = df_drop_location.drop('LOCATION', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### x.y. Duplicated Columns - LOCATION is the pair LATITUDE and LONGITUDE\n",
    "##### Analysis\n",
    "\n",
    "LOCATION is the pair of values of LONGITUDE and LATITUDE, we choose to drop it due to two reasons:\n",
    "1. It is duplication of other features (aggregation of LONGITUDE and LATITUDE in pairs will result in LOCATION).\n",
    "2. It is categorical feature, with high class dimensionality - meaning, in the encoding phase to prepare for modeling, it will generate very big number of derived features. In the other hand, LONGITUDE and LATITUDE is numeric and much easier to the handling in modeling phase.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.y. Drop Duplicated features\n",
    "\n",
    "df_drop_crash_record = df_drop_location.copy()\n",
    "df_drop_crash_record = df_drop_crash_record.drop('CRASH_RECORD_ID', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### x.y. Drop CRASH_ID column\n",
    "##### Analysis\n",
    "\n",
    "It is just ID of the crash, and has no contributation to the prediction model.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section x.y. - Fix Ranges and Special Outliers\n",
    "\n",
    "df_fix_range_issues = df_drop_crash_record.copy()\n",
    "\n",
    "print(\"========================== Values Before ==========================\")\n",
    "get_continuous_feature_details(df_fix_range_issues, 'LATITUDE')\n",
    "get_continuous_feature_details(df_fix_range_issues, 'LONGITUDE')\n",
    "# get_continuous_feature_details(df_fix_range_issues, 'LANE_CNT')\n",
    "\n",
    "# To remove values for my_feature outside a given range range (including min_value and max_value)\n",
    "def remove_invalid_values_outside_given_range(my_df, my_feature, min_value, max_value):\n",
    "    # Create a mask for values within the range\n",
    "    valid_mask = (my_df[my_feature] >= min_value) & (my_df[my_feature] <= max_value)\n",
    "    # Apply the mask to the DataFrame\n",
    "    filtered_df = my_df[valid_mask]\n",
    "    return filtered_df\n",
    "\n",
    "# Define the valid range for LATITUDE\n",
    "min_latitude = 41.640\n",
    "max_latitude = 42.023\n",
    "df_fix_range_issues = remove_invalid_values_outside_given_range(df_fix_range_issues, 'LATITUDE', min_latitude, max_latitude)\n",
    "\n",
    "# Define the valid range for LONGITUDE\n",
    "min_longitude = -87.940\n",
    "max_longitude = -87.524\n",
    "df_fix_range_issues = remove_invalid_values_outside_given_range(df_fix_range_issues, 'LONGITUDE', min_longitude, max_longitude)\n",
    "\n",
    "\n",
    "# Already dropped due to high missing values count\n",
    "# min_lane_count = 0\n",
    "# max_lane_count = 10\n",
    "# df_fix_range_issues = remove_invalid_values_outside_given_range(df_fix_range_issues, 'LANE_CNT', min_lane_count, max_lane_count)\n",
    "\n",
    "\n",
    "print(\"========================== Values After ==========================\")\n",
    "get_continuous_feature_details(df_fix_range_issues, 'LATITUDE')\n",
    "get_continuous_feature_details(df_fix_range_issues, 'LONGITUDE')\n",
    "# get_continuous_feature_details(df_fix_range_issues, 'LANE_CNT')\n",
    "\n",
    "get_continuous_features_tabular_report(df_fix_range_issues).transpose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### x.y. Fix Ranges and Special Outliers\n",
    "##### Analysis\n",
    "\n",
    "LOCATION is the pair of values of LONGITUDE and LATITUDE, we choose to drop it due to two reasons:\n",
    "1. It is duplication of other features (aggregation of LONGITUDE and LATITUDE in pairs will result in LOCATION).\n",
    "2. It is categorical feature, with high class dimensionality - meaning, in the encoding phase to prepare for modeling, it will generate very big number of derived features. In the other hand, LONGITUDE and LATITUDE is numeric and much easier to handling in modeling phase.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.y. - Fix Ranges, special outliers and formats\n",
    "# Please refer to Data Understanding section or to my next comment, I have summarized all here for visibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_values_count = df_fix_range_issues.copy()\n",
    "\n",
    "dump_feature_frequency_to_a_file(df_values_count ,'POSTED_SPEED_LIMIT')\n",
    "dump_feature_frequency_to_a_file(df_values_count ,'DAMAGE')\n",
    "dump_feature_frequency_to_a_file(df_values_count ,'CRASH_TYPE')\n",
    "dump_feature_frequency_to_a_file(df_values_count ,'REPORT_TYPE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3. Outliers - To remove instances.\n",
    "\n",
    "df_clean_outliers = df_fix_range_issues.copy()\n",
    "\n",
    "def outliers1(df, ft):\n",
    "    Q1 = df[ft].quantile(0.25)\n",
    "    Q3 = df[ft].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    ls = df.index[ (df[ft] < lower_bound) | (df[ft] > upper_bound) ]\n",
    "    return ls\n",
    "\n",
    "index_list = []\n",
    "for feature in df_clean_outliers.columns:\n",
    "    if pd.api.types.is_numeric_dtype(df_clean_outliers[feature]):\n",
    "        index_list.extend(outliers1(df_clean_outliers, feature))\n",
    "\n",
    "def remove(df, ls):\n",
    "    ls = sorted(set(ls))\n",
    "    df = df.drop(ls)\n",
    "    return df\n",
    "\n",
    "print(f\"There are {len(set(index_list))} outliers\")\n",
    "print(f\"DataFrame Shape Before Removing Outliers {df_clean_outliers.shape}\")\n",
    "df_clean_outliers = remove(df_clean_outliers, index_list)\n",
    "print(f\"DataFrame Shape After  Removing Outliers {df_clean_outliers.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### 3.3. Outliers - To remove instances\n",
    "##### Analysis\n",
    "\n",
    "To-Do\n",
    "\n",
    "Please refer to the outliers section report in the Data Understanding section.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4. Consistency - Invalid formats / Ranges\n",
    "\n",
    "df_cleaned = df_clean_outliers.copy()\n",
    "\n",
    "# Convert CRASH_DATE to datetime\n",
    "df_cleaned['CRASH_DATE'] = pd.to_datetime(df_cleaned['CRASH_DATE'])\n",
    "\n",
    "# Extract components from CRASH_DATE\n",
    "df_cleaned['YEAR'] = df_cleaned['CRASH_DATE'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cleaned.shape)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. CRISP-DM: Modeling\n",
    "\n",
    "**Section Overview**\n",
    "\n",
    "4.1. Import relevant ML libs for Modeling.<br>\n",
    "4.2. Formaluize a Numerical Measure for the Target Variable(s).<br>\n",
    "4.3. Undersampling - To fix over-representation in the dataset (unbalanced data).<br>\n",
    "4.4. Modeling with Random Forest.<br>\n",
    "4.5. Modeling with Logestic Regression.<br>\n",
    "4.6. Modeling with DNN.<br>\n",
    "4.7. Modeling with KNN.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1. Import relevant ML libs for Modeling\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2. Formaluize a Numerical Measure for the Target Variable(s)\n",
    "\n",
    "df_to_model = df_cleaned.copy()\n",
    "\n",
    "# Building our numerical metrics for measuring the Severity of injuries based on the reported injuries in the dataset\n",
    "df_to_model['SEVERITY_OF_INJURIES'] = ((0.3 * df_to_model['INJURIES_NON_INCAPACITATING'] + \n",
    "                                 0.6 * df_to_model['INJURIES_INCAPACITATING'] + \n",
    "                                 # Assuming we might use 'INJURIES_INCAPACITATING' or another column for fatal injuries representation\n",
    "                                 0.1 * df_to_model['INJURIES_NO_INDICATION'] + df_to_model['INJURIES_FATAL']) / \n",
    "                                ((df_to_model['INJURIES_NON_INCAPACITATING'] + \n",
    "                                      df_to_model['INJURIES_INCAPACITATING'] + \n",
    "                                      # Again, assuming a placeholder for fatal injuries if needed\n",
    "                                      df_to_model['INJURIES_NO_INDICATION']+df_to_model['INJURIES_FATAL'])))\n",
    "\n",
    "# Code to create the \"INJURY_CLASS\" feature based on \"SEVERITY_OF_INJURIES\"\n",
    "df_to_model['INJURY_ClASS'] = df_to_model['SEVERITY_OF_INJURIES'].apply(lambda x: 'HIGH INJURY' if x > 0.2 else 'LIGHT INJURY')\n",
    "\n",
    "# Code to create the \"SEVERITY_CLASS\" feature based on \"INJURIES_FATAL\" and \"INJURIES_INCAPACITATING\"\n",
    "df_to_model['SEVERITY_CLASS'] = df_to_model.apply(lambda x: 'HIGH SEVERITY' if x['INJURIES_FATAL'] > 0 or x['INJURIES_INCAPACITATING'] > 0 else 'LOW SEVERITY', axis=1)\n",
    "\n",
    "# Display the first few rows to see the new feature\n",
    "df_to_model[['INJURIES_NON_INCAPACITATING','INJURIES_FATAL', 'INJURIES_INCAPACITATING', 'INJURIES_NO_INDICATION', 'SEVERITY_OF_INJURIES','INJURY_ClASS','SEVERITY_CLASS']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">\n",
    "\n",
    "##### 4.2. Formaluize a Numerical Measure for the Target Variable(s)\n",
    "##### Analysis\n",
    "\n",
    "Our target variable is represented via 4 variables were each one giving the following indications:\n",
    "* INJURIES_NO_INDICATION - No injury or light reported.\n",
    "* INJURIES_NON_INCAPACITATING - Medium injury.\n",
    "* INJURIES_INCAPACITATING - Heavy.\n",
    "* INJURIES_FATAL - Death.\n",
    "\n",
    "To build a mesurement to estimate the fatality of the crash injury crash, we define a weighted equation for each class and normalize it.\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preparing features and target variables \n",
    "X = df_to_model.drop(['SEVERITY_OF_INJURIES','INJURIES_TOTAL' ,'INJURIES_INCAPACITATING', 'MOST_SEVERE_INJURY','INJURY_ClASS','SEVERITY_CLASS',\n",
    "                  'INJURIES_NON_INCAPACITATING', 'SEVERITY_CLASS','INJURIES_NO_INDICATION','CRASH_DATE',\n",
    "                  'INJURIES_UNKNOWN', 'DATE_POLICE_NOTIFIED','INJURIES_REPORTED_NOT_EVIDENT','INJURIES_FATAL','CRASH_RECORD_ID','LOCATION','STREET_NAME'],axis = 1)\n",
    "y = df_to_model['SEVERITY_CLASS']\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# Initialize the random under-sampler due to unbalanced dataset\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "\n",
    "# Resample the dataset\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4. Modeling with Random Forest\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Classifier with balanced class weights\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "\n",
    "# Fitting the model to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_report_results = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy of Random Forest Classifier:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", classification_report_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Features\n",
    "based on RF classifier I check the important features for detecting the severity of injuries in the accident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf_classifier.feature_importances_\n",
    "feature_names = X.columns\n",
    "feature_importances = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "feature_importances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5. Modeling with Logestic Regression\n",
    "\n",
    "# Initialize the Logistic Regression model with balanced class weights\n",
    "log_reg = LogisticRegression( max_iter=10000, random_state=42)\n",
    "\n",
    "# Fitting the model to the training data\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_report_results = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy of linear regressor:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", classification_report_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature names\n",
    "feature_names = X_resampled.columns\n",
    "\n",
    "# Get the coefficients from the logistic regression model\n",
    "coefficients = log_reg.coef_[0]  # Assuming binary classification, hence [0]\n",
    "\n",
    "# Create a series to map feature names to their coefficients\n",
    "feature_importance = pd.Series(coefficients, index=feature_names)\n",
    "\n",
    "# Sort the features by their absolute values to see the most significant ones\n",
    "feature_importance_sorted = feature_importance.abs().sort_values(ascending=False)\n",
    "\n",
    "feature_importance_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.6. Modeling with DNN\n",
    "\n",
    "# It's important to scale your input features for neural networks\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the DNN model\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20,10), activation='relu', solver='adam',\n",
    "                    max_iter=100, random_state=42, verbose=False)\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = mlp.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_report_results = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy of DNN model:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", classification_report_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.7. Modeling with KNN\n",
    "\n",
    "# Initialize the KNN classifier\n",
    "# n_neighbors is set to 10 as an example, but you should tune this parameter\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "# Train the model\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_report_results = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy of KNN model:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", classification_report_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. CRISP-DM: Evaluation\n",
    "\n",
    "in this section I just want to check our model performance on the unseen dataset which is all of them are 'LOW SEVERITY\" and check if it has good accuracy or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_to_evaluate = df_to_model.copy()\n",
    "\n",
    "# Separate instances with \"high injury\" and \"low injury\" classes \n",
    "high_injury_instances = df_to_evaluate[df_to_evaluate['SEVERITY_CLASS'] == 'HIGH SEVERITY']\n",
    "low_injury_instances = df_to_evaluate[df_to_evaluate['SEVERITY_CLASS'] == 'LOW SEVERITY']\n",
    "\n",
    "# Sample the same number of instances with \"low injury\" class as \"high injury\" instances\n",
    "num_high_injury = len(high_injury_instances)\n",
    "low_injury_sampled = low_injury_instances.sample(n=num_high_injury, random_state=42)\n",
    "\n",
    "# Check the model's predictions on the remaining instances with \"low injury\" class\n",
    "low_injury_remaining = low_injury_instances.drop(low_injury_sampled.index)\n",
    "\n",
    "# Concatenate the sampled instances of both classes to create a balanced dataset\n",
    "balanced_data = pd.concat([high_injury_instances, low_injury_sampled])\n",
    "\n",
    "\n",
    "# Prepare features and target variable\n",
    "X_train_balanced = balanced_data.drop(['SEVERITY_OF_INJURIES','INJURIES_TOTAL' ,'INJURIES_INCAPACITATING', 'MOST_SEVERE_INJURY','INJURY_ClASS','SEVERITY_CLASS',\n",
    "                  'INJURIES_NON_INCAPACITATING', 'SEVERITY_CLASS','INJURIES_NO_INDICATION','CRASH_DATE',\n",
    "                  'INJURIES_UNKNOWN', 'DATE_POLICE_NOTIFIED','INJURIES_REPORTED_NOT_EVIDENT','INJURIES_FATAL','CRASH_RECORD_ID','LOCATION','STREET_NAME'],axis = 1)\n",
    "y_train_balanced = balanced_data['SEVERITY_CLASS']\n",
    "\n",
    "X_test_balanced = low_injury_remaining.drop(['SEVERITY_OF_INJURIES','INJURIES_TOTAL' ,'INJURIES_INCAPACITATING', 'MOST_SEVERE_INJURY','INJURY_ClASS','SEVERITY_CLASS',\n",
    "                  'INJURIES_NON_INCAPACITATING', 'SEVERITY_CLASS','INJURIES_NO_INDICATION','CRASH_DATE',\n",
    "                  'INJURIES_UNKNOWN', 'DATE_POLICE_NOTIFIED','INJURIES_REPORTED_NOT_EVIDENT','INJURIES_FATAL','CRASH_RECORD_ID','LOCATION','STREET_NAME'],axis = 1)\n",
    "\n",
    "y_test_balanced = low_injury_remaining['SEVERITY_CLASS']\n",
    "\n",
    "\n",
    "X_train_balanced = pd.get_dummies(X_train_balanced)\n",
    "X_test_balanced = pd.get_dummies(X_test_balanced)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine the training and testing feature data for consistent one-hot encoding\n",
    "combined_features = pd.concat([X_train_balanced, X_test_balanced], axis=0)\n",
    "\n",
    "# Apply get_dummies to the combined dataset\n",
    "combined_features_encoded = pd.get_dummies(combined_features)\n",
    "\n",
    "# Now split them back into training and testing sets\n",
    "X_train_encoded = combined_features_encoded.iloc[:len(X_train_balanced), :]\n",
    "X_test_encoded = combined_features_encoded.iloc[len(X_train_balanced):, :]\n",
    "\n",
    "# Ensure the data is scaled\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
    "X_test_scaled = scaler.transform(X_test_encoded)\n",
    "\n",
    "\n",
    "# Train the KNN model on the scaled, balanced training data\n",
    "knn.fit(X_train_scaled, y_train_balanced)\n",
    "\n",
    "# Make predictions on the scaled, balanced testing set\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the predictions\n",
    "accuracy = accuracy_score(y_test_balanced, y_pred)\n",
    "classification_report_results = classification_report(y_test_balanced, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", classification_report_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. CRISP-DM: Deployment\n",
    "\n",
    "To-Do."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311_pods_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
